[
  {
    "file_path": "pdfs\\10th paper.pdf",
    "total_pages": 8,
    "combined_text": "JOURNAL OF AMRAN UNIVERSITY\nJ. Amr. Uni. 03 (2023) p.259\nCrowd Detection, Monitoring and Management: A literature Review\nHisham Haider Yusef Sa\u2019ad1,2,*, Yahya Al-Ashmoery2,3, Adnan Haider3, Al-Marhabi Zaid3, Kaleed Alwasbi2 , Ruqaih\nHussein Salman2\n1Department of Computer Science, Amran University, Amran, Yemen\n2Department of Information Technology, Al-Razi University\n3Department of Mathematics & Computer, Faculty of Science, Sana\u2019a University, Yemen\n3Department of Cybersecurity and networking, Al-Razi University, Yemen\nAbstract\nThe rapid increase in the global population has led to the emergence of large crowds during public events\nacross various domains such as sports, music festivals, religious gatherings, and political campaigns. If\nthese events are not properly organized and controlled, they have the potential to result in disasters.\nTragically, stampedes occur every year, causing fatalities, disappearances, and injuries for many\nindividuals. Therefore, crowd identification, monitoring, and control are problems that will be addressed\nand discussed in this paper in order to lessen casualties and prevent such catastrophes. The objective of\nthis study is to present a thorough review of technologies and methods relevant to crowd management,\nplanning, behaviour analysis, and counting. Furthermore, it aids researchers' future progress by examining\nrecent technology developments in the field of crowd planning and monitoring.\nKeywords: Crowd Detection, Crowd Monitoring, Crowd Management, Crowd behaviour, Crowd\nsimulation, Crowd density estimation.\n\u0629\u0645\u0627\u0639\u0644\u0627 \u062b\u0627\u062f\u062d\u0644\u0623\u0627 \u0644\u0644\u0627\u062e \u0633\u0627\u0646\u0644\u0627 \u0646\u0645 \u0629\u0631\u064a\u0628\u0643 \u062f\u0648\u0634\u062d \u062a\u0631\u0647\u0638 \u060c\u0643\u0644\u0630\u0644 \u0629\u062c\u064a\u062a\u0646\u0648 .\u0628\u0643\u0648\u0643\u0644\u0627 \u0646\u0627\u0643\u0633 \u062f\u062f\u0639 \u064a\u0641 \u0639\u064a\u0631\u0633\u0644\u0627 \u062f\u064a\u0632\u0627 \u062a\u0644\u0627 \u0628\u0628\u0633\u0628 :\u0635\u062e\u0644\u0645\u0644\u0627\n\u0649\u0644\u0625 \u0627\u0645\u0648 \u0629\u064a\u0633\u0627\u064a\u0633\u0644\u0627 \u062a\u0644\u0627\u0645\u062d\u0644\u0627\u0648 \u0629\u064a\u0646\u064a\u062f\u0644\u0627 \u062a\u0627\u0639\u0645\u062c\u062a\u0644\u0627\u0648 \u0629\u064a\u0642\u064a\u0633\u0648\u0645\u0644\u0627 \u062a\u0627\u0646\u0627\u062c\u0631\u0647\u0645\u0644\u0627\u0648 \u0629\u0636\u0627\u064a\u0631\u0644\u0627 \u0643\u0644\u0630 \u064a\u0641 \u0627\u0645\u0628 \u060c\u0629\u0627\u064a\u062d\u0644\u0627 \u062a\u0644\u0627\u0627\u062c\u0645 \u0646\u0645 \u062f\u064a\u062f\u0639\u0644\u0627 \u064a\u0641\n\u062a\u0627\u064a\u0641\u0648\u0644\u0627 \u0646\u0645 \u0635\u0627\u062e\u0634\u0644\u0623\u0627 \u0646\u0645 \u062f\u064a\u062f\u0639\u0644\u0627 \u064a\u0646\u0627\u0639\u064a .\u062d\u064a\u062d\u0635 \u0644\u0643\u0634\u0628 \u0627\u0647\u062a\u0628\u0642\u0631\u0627\u0645\u0648 \u0627\u0647\u0645\u064a\u0638\u0646\u062a \u0645\u062a\u064a \u0645\u0644 \u0627\u0630\u0625 \u062b\u0631\u0627\u0648\u0643 \u0649\u0644\u0625 \u064a\u062f\u0624\u062a \u062f\u0642 \u064a\u062a\u0644\u0627\u0648 \u060c\u0643\u0644\u0630\n\u064a\u0641 \u0627\u0647\u064a\u0644\u0639 \u0629\u0631\u0637\u064a\u0633\u0644\u0627\u0648 \u0627\u0647\u062a\u0628\u0642\u0631\u0627\u0645\u0648 \u062f\u0648\u0634\u062d\u0644\u0627 \u062f\u064a\u062f\u062d\u062a \u0629\u0644\u0643\u0634\u0645 \u0629\u0634\u0642\u0627\u0646\u0645 \u0645\u062a\u064a\u0633 \u060c\u0643\u0644\u0630\u0644 .\u0645\u0627\u0639 \u0644\u0643 \u0639\u0641\u0627\u062f\u062a\u0644\u0627 \u062b\u062f\u0627\u0648\u062d\u0644 \u0629\u062c\u064a\u062a\u0646 \u062a\u0627\u0628\u0627\u0635\u0644\u0625\u0627\u0648 \u0621\u0627\u0641\u062a\u062e\u0644\u0627\u0627\u0648\n\u0628\u064a\u0644\u0627\u0633\u0644\u0623\u0627\u0648 \u062a\u0627\u064a\u0646\u0642\u062a\u0644\u0644 \u0629\u0644\u0645\u0627\u0634 \u0629\u0639\u062c\u0631\u0627\u0645 \u0645\u064a\u062f\u0642\u062a \u0648\u0647 \u0629\u0633\u0631\u0627\u062f\u0644\u0627 \u0647\u0630\u0647 \u0646\u0645 \u0641\u062f\u0647\u0644\u0627 .\u062b\u0631\u0627\u0648\u0643\u0644\u0627 \u0647\u0630\u0647 \u0644\u062b\u0645 \u0639\u0646\u0645\u0648 \u0631\u0626\u0627\u0633\u062e\u0644\u0627 \u0644\u064a\u0644\u0642\u062a \u0641\u062f\u0647\u0628 \u062b\u062d\u0628\u0644\u0627 \u0627\u0630\u0647\n\u064a\u0641 \u0646\u064a\u062b\u062d\u0627\u0628\u0644\u0627 \u0645\u062f\u0642\u062a \u0649\u0644\u0639 \u062b\u062d\u0628\u0644\u0627 \u0627\u0630\u0647 \u062f\u0639\u0627\u0633\u064a \u060c\u0643\u0644\u0630 \u0649\u0644\u0639 \u0629\u0648\u0644\u0627\u0639 .\u062f\u0631\u0627\u0641\u0644\u0623\u0627 \u062f\u0639\u0648 \u0627\u0647\u0643\u0648\u0644\u0633 \u0644\u064a\u0644\u062d\u062a\u0648 \u0627\u0647\u0637\u064a\u0637\u062e\u062a\u0648 \u062f\u0648\u0634\u062d\u0644\u0627 \u0629\u0631\u0627\u062f\u0625\u0628 \u0629\u0642\u0644\u0639\u062a\u0645\u0644\u0627\n.\u062f\u0648\u0634\u062d\u0644\u0627 \u0629\u0628\u0642\u0631\u0627\u0645\u0648 \u0637\u064a\u0637\u062e\u062a \u0644\u0627\u062c\u0645 \u064a\u0641 \u0629\u062b\u064a\u062f\u062d\u0644\u0627 \u0629\u064a\u062c\u0648\u0644\u0648\u0646\u0643\u062a\u0644\u0627 \u062a\u0631\u0627\u0648\u0637\u062a\u0644\u0627 \u0629\u0633\u0631\u0627\u062f \u0644\u0644\u0627\u062e \u0646\u0645 \u0644\u0628\u0642\u062a\u0633\u0645\u0644\u0627\n1. Introduction\nDuring the Hajj period in 2015, over 2,000 individuals perished and over 850 sustained injuries\n[1]. Additionally, a range of applications can benefit greatly from the knowledge of crowd counting\nand density estimation [2] such as the psychological impacts of individuals congregating in groups\n[3], animal migration [4], and bacterial activity [5]. In general, crowd counting is applicable to a\nwide range of situations where it is essential to comprehend crowd behaviour such as Safety\nmonitoring [6-8], Design of public spaces [9, 10], Evidence\u2010based decision\u2010making [9, 11], Disaster\nmanagement [12, 13], and Virtual environments [14, 15].\nThree recent techniques have been developed for gathering crowd data: wireless/radio frequency\n(RF), vision, and web/social media data extraction. Mobile phones, wireless sensor networks\n(WSN), radio frequency identification (RFID), and near-field communication (NFC)-based systems\nfor gathering crowd data have also been taken into consideration. Two methods have been proposed\nby researchers to gather data on crowds. These strategies are both device-based (everyone carrying\nan RF-based device) and device-free (no participants carrying any RF devices).\nThe size of the crowd population that is in need of tracking has a direct impact on the monitoring\nsystem accuracy and the difficulties encountered. Table I and Fig. 1 illustrate the crowd scale\ndepending on the number of people in each frame. The population density can be divided into three\ncrowd levels identified with the crowd monitoring system (low-scale, normal-scale, and large-\nscale), depending on the number of people in each frame.\n_________________________________________________\n*Corresponding author E-mail: hesham_haider@yahoo.com\n\u00a9Amran University Publishing\nJ. Amr. Uni. 03 (2023) p.259 Hisham H. Y. Sa\u2019ad et al.\nThe rest of the paper is formatted as follows: The crowd detection models are revised in Section 2.\nIn sections 3 and 4, the monitoring and management of crowd are examined, while one case study is\ndiscussed in section 5. Section 6 presents the conclusion of this article.\nTable 1: Crowd scale levels\nCrowd scale level Count\nLow Scale <150\nNormal Scale 150-550\nLarge Scale >550\n(a) Low scale\n(b) Normal Scale\n260\nJ. Amr. Uni. 03 (2023) p.259 Hisham H. Y. Sa\u2019ad et al.\n(c) Large scale\nFig 1: Crowd Scale Level: (a) low-scale, (b) normal-scale, and (c) large-scale [16]\n2. Related Works\nIn this paper, we will divide the related works to three types which are crowd detection, crowd\nmonitoring and crowd management as presented in the coming three sections.\n3. Crowd Detection\nThere are several ways to locate a crowd; in the past, a person would typically report a congestion;\non the other hand, several types of crowd detection systems, including Laser-based, Radio-based\n(wifi), Radio-based (RFID), Radio-based (Bluetooth), video-based, and thermal-based are recently\npresented. In order to manage a crowd (similar to the crowd in Fig. 2), you must first predict when\nit will form. This requires some accurate crowd detection techniques. Two examples of crowds are\nillustrated in Fig 2. The first one is a crowd of Black Lives Matter protest in Australia and the\nsecond demonstrates the pilgrims\u2019 path to the Jamaraat Bridge in Mecca, Saudi Arabia.\na) Black Lives Matter protest in Australia. b) Muslim pilgrims during Hajj\nFig 2. Examples of crowds\nThe authors of [17] present a system that makes advantage of smartphone users to scan nearby\nBluetooth devices and assess population aspects in urban settings. The proposed system does more\nthan just add devices up to more sophisticated capabilities. It examines flow direction and\npopulation density.\n261\nJ. Amr. Uni. 03 (2023) p.259 Hisham H. Y. Sa\u2019ad et al.\nTo examine the system described, a dataset of 200000 findings from 1000 scanning devices was\ncompiled over the course of three days. The outcome demonstrates the effectiveness of Bluetooth\ndevices as a reliable technique for crowd detection and monitoring.\nUtilizing Bluetooth technology that was readily available and installed on the bus ceiling, a wireless\nsystem with innovative and affordable features was created [18] . The Bluetooth device in the bus\nthen does a periodic scan for discoverable devices nearby. In order to identify an origin/destination\nrelationship, the system postprocesses the data and compares it to bus position and ticket\ninformation.\nWi-Fi-based systems for counting crowds face some difficulties. The capacity to distinguish\nbetween those outside the bus and true passengers is one of these difficulties. Numerous articles\nhave researched and debated this issue, including [19] and [20], which find a solution by filtering\nthe probes with a sliding window in order to eliminate the MAC addresses that have not been\ndetected for an extended period of time.\nIn [21], [22], and [23], people counters based on thermal cameras are introduced. Thermal camera-\nbased systems, in contrast to optical cameras like those in Fig. 3, are less susceptible to background\ncolour contrasts or ambient lighting levels, albeit the performance of the systems can be impacted\nby weather and heat sources. Real-time image processing models require a lot of computer power.\nUtilizing picture compression is one technique to cut down on the expensive computation [24].\nFor the detection\u2010based methods, like Figure 1, researchers use a sliding window to detect the\npeople in an image and then use this information to count the number of people 33, 34]. However,\nin the case of extremely crowded scenes, which are difficult to detect (e.g., dense density, severe\nocclusion) for classical methods, the regression\u2010based method comes in handy.\n(a) (b)\nFig 3. Crowd detection (a) The density estimation based on crowd counting (b) The detection based on crowd counting\n[25][37]\n4. Crowd Monitoring\nCrowd monitoring has drawn greater attention recently. The number of research publications has\nsignificantly expanded, as seen in Fig 4 below [26]. The figure shows an increasing especially from\n2010 due to the increasing in the number of the casualties during the huge gathering in all over the\nworld.\n262\nJ. Amr. Uni. 03 (2023) p.259 Hisham H. Y. Sa\u2019ad et al.\nFig 4: Documents published per year on \u201ccrowd monitoring\u201d. [in Scopus]\nMonitoring the crowd is a crucial step in determining the crowd dynamics that will improve event\nmanagement and public safety. As a result, event organizers are advised to keep a watch on the\ncrowd in order to see any dangers early on and take the required precautions [27] . The primary\nobjective of crowd surveillance equipment is to gather important data, such as crowd size and\ndensity. The volume of crowd assembling at a monitored site can be calculated by calculating the\npopulation for precise and effective managing and planning [28].\nClosed-circuit television (CCTV) surveillance is the most common method of crowd monitoring in\ntraditional crowd monitoring systems (CMSs) [29]. A system of video cameras that are used to\nrelay signals from certain locations to a specific set of monitors or displays, typically for\nsurveillance or security purposes, is known as a CCTV system [30]. Typically, CCTV systems have\nrecording hardware that stores video signal for later use. Therefore, CCTV systems can be utilized\nin a variety of settings to keep an eye on and protect people, deter crime, and promote public safety.\nWhen there are obstacles or poor weather conditions at the monitored site, the CCTV camera cannot\neffectively distinguish strange actions or crowd situations.\nFor a number of abnormal scenarios, such as crowding, hail, conflicts, fire, violence attacks, and\ntrampling, a high-efficiency crowd management and evacuation model utilizing communication\ntechnologies and artificial intelligence (AI) is required. Consequently, a multitude of sensors, quick\ndecision transmission, and in-depth data analysis make up the fundamental parts of CMS [31].\nThe proposed system in [32] was constructed using two key parts. The information management\ncomponent also contains a fuzzy logic module and a thermal video analyzer. Recent fuzzy models\n[33] and [34] can be used to improve performance.\n5. Crowd Management\nCrowd management is the examination of people who determine an area's capacity prior to its\nutilization. The systematic planning, coordination, and supervision of the orderly gathering and\nmovement of people is known as crowd management [35]. Crowd management is the supportive,\norganized direction provided for the orderly movement of people. As a crucial component of crowd\nmanagement, actions are done to limit or regulate crowd behaviour. Securing crowd safety may be a\npart of crowd control. Additionally, a set of acts and preparations that are made to use, facilitate,\nand move crowds can be referred to as crowd management.\nIt's crucial to clear up a common misunderstanding about the difference between crowd control and\ncrowd management. Although these two terms are frequently used synonymously, it is important to\nunderstand their differences in order to behave more correctly throughout an event. Crowd\nmanagement encompasses facilitating crowd activities and movement in addition to maintaining\ncrowd safety. On the other hand, crowd control is primarily concerned with the problems that\ndevelop once a crowd starts behaving disorderly or gets out of control [36].\n263\nJ. Amr. Uni. 03 (2023) p.259 Hisham H. Y. Sa\u2019ad et al.\n6. Conclusion\nGlobally, congested conditions are getting worse due to population expansion, global urbanization,\nhigh-speed transportation, and the spread of effective information. The paper discusses a number of\ntechnology developments for seeing, estimating, keeping an eye on, and controlling huge crowds.\nFurthermore, in integrated crowd control frameworks for varied crowd sizes, technologies,\nincluding RF, RFID, WIFI, Bluetooth, optical imaging, and CCTV cameras have been examined\nunder specific conditions. Numerous uses of crowd modelling in the actual world for well-known\ndisasters have been discussed for analysing of crowded situations and in anticipating crowd\nanomalies in real time crowd management systems.\nReferences\n[1] Hoseinpourfard M., et al.. The emergence of Hajj stampedes: lessons for Hajj trauma centers.\nTrauma Monthly, 2017, 22(4).\n[2] Chen, Y., et al., Large group activity security risk assessment and risk early warning based on\nrandom forest algorithm. Pattern Recognition Letters, 2021. 144: p. 1-5.\n[3] Aveni, A.F., The not-so-lonely crowd: Friendship groups in collective behavior. Sociometry,\n1977: p. 96-99.\n[4] Parrish, J.K. and L. Edelstein-Keshet, Complexity, pattern, and evolutionary trade-offs in animal\naggregation. Science, 1999. 284(5411): p. 99-101.\n[5] Zhang, H.-P., et al., Collective motion and density fluctuations in bacterial colonies.\nProceedings of the National Academy of Sciences, 2010. 107(31): p. 13626-13630.\n[6] Yi, S., H. Li, and X. Wang, Pedestrian behavior modeling from stationary crowds with\napplications to intelligent surveillance. IEEE transactions on image processing, 2016. 25(9): p.\n4354-4368.\n[7] Chaker, R., Z. Al Aghbari, and I.N. Junejo, Social network model for crowd anomaly detection\nand localization. Pattern Recognition, 2017. 61: p. 266-281.\n[8] Lee, S., H.G. Kim, and Y.M. Ro. STAN: Spatio-temporal adversarial networks for abnormal\nevent detection. in 2018 IEEE international conference on acoustics, speech and signal processing\n(ICASSP). 2018. IEEE.\n[9] Deng, L., et al., Hospital crowdedness evaluation and in-hospital resource allocation based on\nimage recognition technology. Scientific Reports, 2023. 13(1): p. 299.\n[10] Lu, L., et al., A study of pedestrian group behaviors in crowd evacuation based on an extended\nfloor field cellular automaton model. Transportation research part C: emerging technologies, 2017.\n81: p. 317-329.\n[11] Liu, Z., et al., Decision-Making Framework for GI Layout Considering Site Suitability and\nWeighted Multi-Function Effectiveness: A Case Study in Beijing Sub-Center. Water, 2022. 14(11):\np. 1765.\n[12] Koswatte, S., K. McDougall, and X. Liu, Crowd-Assisted Flood Disaster Management, in\nApplication of Remote Sensing and GIS in Natural Resources and Built Infrastructure Management.\n2023, Springer. p. 39-55.\n[13] Liu, J., Y. Chen, and Y. Chen, Emergency and disaster management-crowd evacuation\nresearch. Journal of Industrial Information Integration, 2021. 21: p. 100191.\n[14] Hu, R., et al., RDC-SAL: Refine distance compensating with quantum scale-aware learning for\ncrowd counting and localization. Applied Intelligence, 2022. 52(12): p. 14336-14348.\n[15] Perez, H., et al., Task-based crowd simulation for heterogeneous architectures, in Innovative\nResearch and Applications in Next-Generation High Performance Computing. 2016, IGI Global. p.\n194-219.\n264\nJ. Amr. Uni. 03 (2023) p.259 Hisham H. Y. Sa\u2019ad et al.\n[16] Jiang, Y., et al., Ultra large-scale crowd monitoring system architecture and design issues.\nIEEE Internet of Things Journal, 2021. 8(13): p. 10356-10366.\n[17] Weppner, J., et al., Participatory bluetooth scans serving as urban crowd probes. IEEE Sensors\nJournal, 2014. 14(12): p. 4196-4206.\n[18] Kostakos, V., T. Camacho, and C. Mantero. Wireless detection of end-to-end passenger trips\non public transport buses. in 13th International IEEE Conference on Intelligent Transportation\nSystems. 2010. IEEE.\n[19] Handte, M., et al. Crowd Density Estimation for Public Transport Vehicles. in EDBT/ICDT\nWorkshops. 2014. Citeseer.\n[20] Handte, M., et al., An internet-of-things enabled connected navigation system for urban bus\nriders. IEEE internet of things journal, 2016. 3(5): p. 735-744.\n[21] Lin, W.-C., W.K. Seah, and W. Li. Exploiting radio irregularity in the Internet of Things for\nautomated people counting. in 2011 IEEE 22nd International Symposium on Personal, Indoor and\nMobile Radio Communications. 2011. IEEE.\n[22] Tikkanen, T., People detection and tracking using a network of low-cost depth cameras. Aalto\nUniversity, 2014.\n[23] Tyndall, A., R. Cardell-Oliver, and A. Keating, Occupancy estimation using a low-pixel count\nthermal imager. IEEE Sensors Journal, 2016. 16(10): p. 3784-3791.\n[24] Saad, A.-M.H., et al., Impact of spatial dynamic search with matching threshold strategy on\nfractal image compression algorithm performance: study. IEEE Access, 2020. 8: p. 52687-52699.\n[25] Li, M., et al. Estimating the number of people in crowded scenes by mid based foreground\nsegmentation and head-shoulder detection. in 2008 19th international conference on pattern\nrecognition. 2008. IEEE.\n[26] Singh, U., et al., Crowd monitoring: State-of-the-art and future directions. IETE Technical\nReview, 2021. 38(6): p. 578-594.\n[27] Li, X., et al., Data fusion for intelligent crowd monitoring and management systems: A survey.\nIEEE Access, 2021. 9: p. 47069-47083.\n[28] Ding, X., et al., Crowd density estimation using fusion of multi-layer features. IEEE\nTransactions on Intelligent Transportation Systems, 2020. 22(8): p. 4776-4787.\n[29] Mallah, J.E., et al. Crowd Monitoring: Critical Situations Prevention Using Smartphones and\nGroup Detection. in Distributed, Ambient, and Pervasive Interactions: Third International\nConference, DAPI 2015, Held as Part of HCI International 2015, Los Angeles, CA, USA, August 2-\n7, 2015, Proceedings 3. 2015. Springer.\n[30] Davies, A.C., J.H. Yin, and S.A. Velastin, Crowd monitoring using image processing.\nElectronics & Communication Engineering Journal, 1995. 7(1): p. 37-47.\n[31] Sirmacek, B. and P. Reinartz. Automatic crowd density and motion analysis in airborne image\nsequences based on a probabilistic framework. in 2011 IEEE International Conference on Computer\nVision Workshops (ICCV Workshops). 2011. IEEE.\n[32] Khozium, M.O., A.G. Abuarafah, and E. AbdRabou, A proposed computer-based system\narchitecture for crowd management of pilgrims using thermography. Life Science Journal, 2012.\n9(2): p. 377-383.\n[33] Sa'ad, H.H.Y., et al., A robust structure identification method for evolving fuzzy system.\nExpert Systems with Applications, 2018. 93: p. 267-282.\n[34] Sa\u2019ad, H.H.Y., N.A.M. Isa, and M.M. Ahmed, A structural evolving approach for fuzzy\nsystems. IEEE Transactions on Fuzzy Systems, 2019. 28(2): p. 273-287.\n265\nJ. Amr. Uni. 03 (2023) p.259 Hisham H. Y. Sa\u2019ad et al.\n[35] Al-Shaery, A. and M. Khozium, Crowd management challenges: Tackling approach for real\ntime crowd monitoring. Int. J. Sci. Eng. Res., 2019.\n[36] Saleh, S.A.M., S.A. Suandi, and H. Ibrahim, Recent survey on crowd density estimation and\ncounting for visual surveillance. Engineering Applications of Artificial Intelligence, 2015. 41: p.\n103-114.\n[37] Deng, Lijia,Zhou, QinghuaWang, ShuihuaG\u00f3rriz, Juan Manuel Zhang, Yudong: Deep learning\nin crowd counting: A survey. 2023\n266",
    "metadata": {
      "Title": "A Reflective Approach to Improve Learning and Teaching of Software Engineering in Large Groups",
      "Author": "eyad",
      "Creator": "Microsoft\u00ae Word 2013",
      "CreationDate": "D:20231026200338+03'00'",
      "ModDate": "D:20231026200338+03'00'",
      "Producer": "Microsoft\u00ae Word 2013"
    },
    "error_message": null,
    "pages": [
      {
        "page_number": 0,
        "text_content": "JOURNAL OF AMRAN UNIVERSITY\nJ. Amr. Uni. 03 (2023) p.259\nCrowd Detection, Monitoring and Management: A literature Review\nHisham Haider Yusef Sa\u2019ad1,2,*, Yahya Al-Ashmoery2,3, Adnan Haider3, Al-Marhabi Zaid3, Kaleed Alwasbi2 , Ruqaih\nHussein Salman2\n1Department of Computer Science, Amran University, Amran, Yemen\n2Department of Information Technology, Al-Razi University\n3Department of Mathematics & Computer, Faculty of Science, Sana\u2019a University, Yemen\n3Department of Cybersecurity and networking, Al-Razi University, Yemen\nAbstract\nThe rapid increase in the global population has led to the emergence of large crowds during public events\nacross various domains such as sports, music festivals, religious gatherings, and political campaigns. If\nthese events are not properly organized and controlled, they have the potential to result in disasters.\nTragically, stampedes occur every year, causing fatalities, disappearances, and injuries for many\nindividuals. Therefore, crowd identification, monitoring, and control are problems that will be addressed\nand discussed in this paper in order to lessen casualties and prevent such catastrophes. The objective of\nthis study is to present a thorough review of technologies and methods relevant to crowd management,\nplanning, behaviour analysis, and counting. Furthermore, it aids researchers' future progress by examining\nrecent technology developments in the field of crowd planning and monitoring.\nKeywords: Crowd Detection, Crowd Monitoring, Crowd Management, Crowd behaviour, Crowd\nsimulation, Crowd density estimation.\n\u0629\u0645\u0627\u0639\u0644\u0627 \u062b\u0627\u062f\u062d\u0644\u0623\u0627 \u0644\u0644\u0627\u062e \u0633\u0627\u0646\u0644\u0627 \u0646\u0645 \u0629\u0631\u064a\u0628\u0643 \u062f\u0648\u0634\u062d \u062a\u0631\u0647\u0638 \u060c\u0643\u0644\u0630\u0644 \u0629\u062c\u064a\u062a\u0646\u0648 .\u0628\u0643\u0648\u0643\u0644\u0627 \u0646\u0627\u0643\u0633 \u062f\u062f\u0639 \u064a\u0641 \u0639\u064a\u0631\u0633\u0644\u0627 \u062f\u064a\u0632\u0627 \u062a\u0644\u0627 \u0628\u0628\u0633\u0628 :\u0635\u062e\u0644\u0645\u0644\u0627\n\u0649\u0644\u0625 \u0627\u0645\u0648 \u0629\u064a\u0633\u0627\u064a\u0633\u0644\u0627 \u062a\u0644\u0627\u0645\u062d\u0644\u0627\u0648 \u0629\u064a\u0646\u064a\u062f\u0644\u0627 \u062a\u0627\u0639\u0645\u062c\u062a\u0644\u0627\u0648 \u0629\u064a\u0642\u064a\u0633\u0648\u0645\u0644\u0627 \u062a\u0627\u0646\u0627\u062c\u0631\u0647\u0645\u0644\u0627\u0648 \u0629\u0636\u0627\u064a\u0631\u0644\u0627 \u0643\u0644\u0630 \u064a\u0641 \u0627\u0645\u0628 \u060c\u0629\u0627\u064a\u062d\u0644\u0627 \u062a\u0644\u0627\u0627\u062c\u0645 \u0646\u0645 \u062f\u064a\u062f\u0639\u0644\u0627 \u064a\u0641\n\u062a\u0627\u064a\u0641\u0648\u0644\u0627 \u0646\u0645 \u0635\u0627\u062e\u0634\u0644\u0623\u0627 \u0646\u0645 \u062f\u064a\u062f\u0639\u0644\u0627 \u064a\u0646\u0627\u0639\u064a .\u062d\u064a\u062d\u0635 \u0644\u0643\u0634\u0628 \u0627\u0647\u062a\u0628\u0642\u0631\u0627\u0645\u0648 \u0627\u0647\u0645\u064a\u0638\u0646\u062a \u0645\u062a\u064a \u0645\u0644 \u0627\u0630\u0625 \u062b\u0631\u0627\u0648\u0643 \u0649\u0644\u0625 \u064a\u062f\u0624\u062a \u062f\u0642 \u064a\u062a\u0644\u0627\u0648 \u060c\u0643\u0644\u0630\n\u064a\u0641 \u0627\u0647\u064a\u0644\u0639 \u0629\u0631\u0637\u064a\u0633\u0644\u0627\u0648 \u0627\u0647\u062a\u0628\u0642\u0631\u0627\u0645\u0648 \u062f\u0648\u0634\u062d\u0644\u0627 \u062f\u064a\u062f\u062d\u062a \u0629\u0644\u0643\u0634\u0645 \u0629\u0634\u0642\u0627\u0646\u0645 \u0645\u062a\u064a\u0633 \u060c\u0643\u0644\u0630\u0644 .\u0645\u0627\u0639 \u0644\u0643 \u0639\u0641\u0627\u062f\u062a\u0644\u0627 \u062b\u062f\u0627\u0648\u062d\u0644 \u0629\u062c\u064a\u062a\u0646 \u062a\u0627\u0628\u0627\u0635\u0644\u0625\u0627\u0648 \u0621\u0627\u0641\u062a\u062e\u0644\u0627\u0627\u0648\n\u0628\u064a\u0644\u0627\u0633\u0644\u0623\u0627\u0648 \u062a\u0627\u064a\u0646\u0642\u062a\u0644\u0644 \u0629\u0644\u0645\u0627\u0634 \u0629\u0639\u062c\u0631\u0627\u0645 \u0645\u064a\u062f\u0642\u062a \u0648\u0647 \u0629\u0633\u0631\u0627\u062f\u0644\u0627 \u0647\u0630\u0647 \u0646\u0645 \u0641\u062f\u0647\u0644\u0627 .\u062b\u0631\u0627\u0648\u0643\u0644\u0627 \u0647\u0630\u0647 \u0644\u062b\u0645 \u0639\u0646\u0645\u0648 \u0631\u0626\u0627\u0633\u062e\u0644\u0627 \u0644\u064a\u0644\u0642\u062a \u0641\u062f\u0647\u0628 \u062b\u062d\u0628\u0644\u0627 \u0627\u0630\u0647\n\u064a\u0641 \u0646\u064a\u062b\u062d\u0627\u0628\u0644\u0627 \u0645\u062f\u0642\u062a \u0649\u0644\u0639 \u062b\u062d\u0628\u0644\u0627 \u0627\u0630\u0647 \u062f\u0639\u0627\u0633\u064a \u060c\u0643\u0644\u0630 \u0649\u0644\u0639 \u0629\u0648\u0644\u0627\u0639 .\u062f\u0631\u0627\u0641\u0644\u0623\u0627 \u062f\u0639\u0648 \u0627\u0647\u0643\u0648\u0644\u0633 \u0644\u064a\u0644\u062d\u062a\u0648 \u0627\u0647\u0637\u064a\u0637\u062e\u062a\u0648 \u062f\u0648\u0634\u062d\u0644\u0627 \u0629\u0631\u0627\u062f\u0625\u0628 \u0629\u0642\u0644\u0639\u062a\u0645\u0644\u0627\n.\u062f\u0648\u0634\u062d\u0644\u0627 \u0629\u0628\u0642\u0631\u0627\u0645\u0648 \u0637\u064a\u0637\u062e\u062a \u0644\u0627\u062c\u0645 \u064a\u0641 \u0629\u062b\u064a\u062f\u062d\u0644\u0627 \u0629\u064a\u062c\u0648\u0644\u0648\u0646\u0643\u062a\u0644\u0627 \u062a\u0631\u0627\u0648\u0637\u062a\u0644\u0627 \u0629\u0633\u0631\u0627\u062f \u0644\u0644\u0627\u062e \u0646\u0645 \u0644\u0628\u0642\u062a\u0633\u0645\u0644\u0627\n1. Introduction\nDuring the Hajj period in 2015, over 2,000 individuals perished and over 850 sustained injuries\n[1]. Additionally, a range of applications can benefit greatly from the knowledge of crowd counting\nand density estimation [2] such as the psychological impacts of individuals congregating in groups\n[3], animal migration [4], and bacterial activity [5]. In general, crowd counting is applicable to a\nwide range of situations where it is essential to comprehend crowd behaviour such as Safety\nmonitoring [6-8], Design of public spaces [9, 10], Evidence\u2010based decision\u2010making [9, 11], Disaster\nmanagement [12, 13], and Virtual environments [14, 15].\nThree recent techniques have been developed for gathering crowd data: wireless/radio frequency\n(RF), vision, and web/social media data extraction. Mobile phones, wireless sensor networks\n(WSN), radio frequency identification (RFID), and near-field communication (NFC)-based systems\nfor gathering crowd data have also been taken into consideration. Two methods have been proposed\nby researchers to gather data on crowds. These strategies are both device-based (everyone carrying\nan RF-based device) and device-free (no participants carrying any RF devices).\nThe size of the crowd population that is in need of tracking has a direct impact on the monitoring\nsystem accuracy and the difficulties encountered. Table I and Fig. 1 illustrate the crowd scale\ndepending on the number of people in each frame. The population density can be divided into three\ncrowd levels identified with the crowd monitoring system (low-scale, normal-scale, and large-\nscale), depending on the number of people in each frame.\n_________________________________________________\n*Corresponding author E-mail: hesham_haider@yahoo.com\n\u00a9Amran University Publishing",
        "tables": [],
        "forms": {},
        "confidence_score": 1.0
      },
      {
        "page_number": 1,
        "text_content": "J. Amr. Uni. 03 (2023) p.259 Hisham H. Y. Sa\u2019ad et al.\nThe rest of the paper is formatted as follows: The crowd detection models are revised in Section 2.\nIn sections 3 and 4, the monitoring and management of crowd are examined, while one case study is\ndiscussed in section 5. Section 6 presents the conclusion of this article.\nTable 1: Crowd scale levels\nCrowd scale level Count\nLow Scale <150\nNormal Scale 150-550\nLarge Scale >550\n(a) Low scale\n(b) Normal Scale\n260",
        "tables": [
          [
            [
              "Crowd scale level",
              "",
              "",
              "Count",
              "",
              ""
            ],
            [
              "",
              "Low Scale",
              "",
              "",
              "<150",
              ""
            ],
            [
              "Normal Scale",
              "",
              "",
              "150-550",
              "",
              ""
            ],
            [
              "",
              "Large Scale",
              "",
              "",
              ">550",
              ""
            ]
          ]
        ],
        "forms": {},
        "confidence_score": 1.0
      },
      {
        "page_number": 2,
        "text_content": "J. Amr. Uni. 03 (2023) p.259 Hisham H. Y. Sa\u2019ad et al.\n(c) Large scale\nFig 1: Crowd Scale Level: (a) low-scale, (b) normal-scale, and (c) large-scale [16]\n2. Related Works\nIn this paper, we will divide the related works to three types which are crowd detection, crowd\nmonitoring and crowd management as presented in the coming three sections.\n3. Crowd Detection\nThere are several ways to locate a crowd; in the past, a person would typically report a congestion;\non the other hand, several types of crowd detection systems, including Laser-based, Radio-based\n(wifi), Radio-based (RFID), Radio-based (Bluetooth), video-based, and thermal-based are recently\npresented. In order to manage a crowd (similar to the crowd in Fig. 2), you must first predict when\nit will form. This requires some accurate crowd detection techniques. Two examples of crowds are\nillustrated in Fig 2. The first one is a crowd of Black Lives Matter protest in Australia and the\nsecond demonstrates the pilgrims\u2019 path to the Jamaraat Bridge in Mecca, Saudi Arabia.\na) Black Lives Matter protest in Australia. b) Muslim pilgrims during Hajj\nFig 2. Examples of crowds\nThe authors of [17] present a system that makes advantage of smartphone users to scan nearby\nBluetooth devices and assess population aspects in urban settings. The proposed system does more\nthan just add devices up to more sophisticated capabilities. It examines flow direction and\npopulation density.\n261",
        "tables": [],
        "forms": {},
        "confidence_score": 1.0
      },
      {
        "page_number": 3,
        "text_content": "J. Amr. Uni. 03 (2023) p.259 Hisham H. Y. Sa\u2019ad et al.\nTo examine the system described, a dataset of 200000 findings from 1000 scanning devices was\ncompiled over the course of three days. The outcome demonstrates the effectiveness of Bluetooth\ndevices as a reliable technique for crowd detection and monitoring.\nUtilizing Bluetooth technology that was readily available and installed on the bus ceiling, a wireless\nsystem with innovative and affordable features was created [18] . The Bluetooth device in the bus\nthen does a periodic scan for discoverable devices nearby. In order to identify an origin/destination\nrelationship, the system postprocesses the data and compares it to bus position and ticket\ninformation.\nWi-Fi-based systems for counting crowds face some difficulties. The capacity to distinguish\nbetween those outside the bus and true passengers is one of these difficulties. Numerous articles\nhave researched and debated this issue, including [19] and [20], which find a solution by filtering\nthe probes with a sliding window in order to eliminate the MAC addresses that have not been\ndetected for an extended period of time.\nIn [21], [22], and [23], people counters based on thermal cameras are introduced. Thermal camera-\nbased systems, in contrast to optical cameras like those in Fig. 3, are less susceptible to background\ncolour contrasts or ambient lighting levels, albeit the performance of the systems can be impacted\nby weather and heat sources. Real-time image processing models require a lot of computer power.\nUtilizing picture compression is one technique to cut down on the expensive computation [24].\nFor the detection\u2010based methods, like Figure 1, researchers use a sliding window to detect the\npeople in an image and then use this information to count the number of people 33, 34]. However,\nin the case of extremely crowded scenes, which are difficult to detect (e.g., dense density, severe\nocclusion) for classical methods, the regression\u2010based method comes in handy.\n(a) (b)\nFig 3. Crowd detection (a) The density estimation based on crowd counting (b) The detection based on crowd counting\n[25][37]\n4. Crowd Monitoring\nCrowd monitoring has drawn greater attention recently. The number of research publications has\nsignificantly expanded, as seen in Fig 4 below [26]. The figure shows an increasing especially from\n2010 due to the increasing in the number of the casualties during the huge gathering in all over the\nworld.\n262",
        "tables": [],
        "forms": {},
        "confidence_score": 1.0
      },
      {
        "page_number": 4,
        "text_content": "J. Amr. Uni. 03 (2023) p.259 Hisham H. Y. Sa\u2019ad et al.\nFig 4: Documents published per year on \u201ccrowd monitoring\u201d. [in Scopus]\nMonitoring the crowd is a crucial step in determining the crowd dynamics that will improve event\nmanagement and public safety. As a result, event organizers are advised to keep a watch on the\ncrowd in order to see any dangers early on and take the required precautions [27] . The primary\nobjective of crowd surveillance equipment is to gather important data, such as crowd size and\ndensity. The volume of crowd assembling at a monitored site can be calculated by calculating the\npopulation for precise and effective managing and planning [28].\nClosed-circuit television (CCTV) surveillance is the most common method of crowd monitoring in\ntraditional crowd monitoring systems (CMSs) [29]. A system of video cameras that are used to\nrelay signals from certain locations to a specific set of monitors or displays, typically for\nsurveillance or security purposes, is known as a CCTV system [30]. Typically, CCTV systems have\nrecording hardware that stores video signal for later use. Therefore, CCTV systems can be utilized\nin a variety of settings to keep an eye on and protect people, deter crime, and promote public safety.\nWhen there are obstacles or poor weather conditions at the monitored site, the CCTV camera cannot\neffectively distinguish strange actions or crowd situations.\nFor a number of abnormal scenarios, such as crowding, hail, conflicts, fire, violence attacks, and\ntrampling, a high-efficiency crowd management and evacuation model utilizing communication\ntechnologies and artificial intelligence (AI) is required. Consequently, a multitude of sensors, quick\ndecision transmission, and in-depth data analysis make up the fundamental parts of CMS [31].\nThe proposed system in [32] was constructed using two key parts. The information management\ncomponent also contains a fuzzy logic module and a thermal video analyzer. Recent fuzzy models\n[33] and [34] can be used to improve performance.\n5. Crowd Management\nCrowd management is the examination of people who determine an area's capacity prior to its\nutilization. The systematic planning, coordination, and supervision of the orderly gathering and\nmovement of people is known as crowd management [35]. Crowd management is the supportive,\norganized direction provided for the orderly movement of people. As a crucial component of crowd\nmanagement, actions are done to limit or regulate crowd behaviour. Securing crowd safety may be a\npart of crowd control. Additionally, a set of acts and preparations that are made to use, facilitate,\nand move crowds can be referred to as crowd management.\nIt's crucial to clear up a common misunderstanding about the difference between crowd control and\ncrowd management. Although these two terms are frequently used synonymously, it is important to\nunderstand their differences in order to behave more correctly throughout an event. Crowd\nmanagement encompasses facilitating crowd activities and movement in addition to maintaining\ncrowd safety. On the other hand, crowd control is primarily concerned with the problems that\ndevelop once a crowd starts behaving disorderly or gets out of control [36].\n263",
        "tables": [],
        "forms": {},
        "confidence_score": 1.0
      },
      {
        "page_number": 5,
        "text_content": "J. Amr. Uni. 03 (2023) p.259 Hisham H. Y. Sa\u2019ad et al.\n6. Conclusion\nGlobally, congested conditions are getting worse due to population expansion, global urbanization,\nhigh-speed transportation, and the spread of effective information. The paper discusses a number of\ntechnology developments for seeing, estimating, keeping an eye on, and controlling huge crowds.\nFurthermore, in integrated crowd control frameworks for varied crowd sizes, technologies,\nincluding RF, RFID, WIFI, Bluetooth, optical imaging, and CCTV cameras have been examined\nunder specific conditions. Numerous uses of crowd modelling in the actual world for well-known\ndisasters have been discussed for analysing of crowded situations and in anticipating crowd\nanomalies in real time crowd management systems.\nReferences\n[1] Hoseinpourfard M., et al.. The emergence of Hajj stampedes: lessons for Hajj trauma centers.\nTrauma Monthly, 2017, 22(4).\n[2] Chen, Y., et al., Large group activity security risk assessment and risk early warning based on\nrandom forest algorithm. Pattern Recognition Letters, 2021. 144: p. 1-5.\n[3] Aveni, A.F., The not-so-lonely crowd: Friendship groups in collective behavior. Sociometry,\n1977: p. 96-99.\n[4] Parrish, J.K. and L. Edelstein-Keshet, Complexity, pattern, and evolutionary trade-offs in animal\naggregation. Science, 1999. 284(5411): p. 99-101.\n[5] Zhang, H.-P., et al., Collective motion and density fluctuations in bacterial colonies.\nProceedings of the National Academy of Sciences, 2010. 107(31): p. 13626-13630.\n[6] Yi, S., H. Li, and X. Wang, Pedestrian behavior modeling from stationary crowds with\napplications to intelligent surveillance. IEEE transactions on image processing, 2016. 25(9): p.\n4354-4368.\n[7] Chaker, R., Z. Al Aghbari, and I.N. Junejo, Social network model for crowd anomaly detection\nand localization. Pattern Recognition, 2017. 61: p. 266-281.\n[8] Lee, S., H.G. Kim, and Y.M. Ro. STAN: Spatio-temporal adversarial networks for abnormal\nevent detection. in 2018 IEEE international conference on acoustics, speech and signal processing\n(ICASSP). 2018. IEEE.\n[9] Deng, L., et al., Hospital crowdedness evaluation and in-hospital resource allocation based on\nimage recognition technology. Scientific Reports, 2023. 13(1): p. 299.\n[10] Lu, L., et al., A study of pedestrian group behaviors in crowd evacuation based on an extended\nfloor field cellular automaton model. Transportation research part C: emerging technologies, 2017.\n81: p. 317-329.\n[11] Liu, Z., et al., Decision-Making Framework for GI Layout Considering Site Suitability and\nWeighted Multi-Function Effectiveness: A Case Study in Beijing Sub-Center. Water, 2022. 14(11):\np. 1765.\n[12] Koswatte, S., K. McDougall, and X. Liu, Crowd-Assisted Flood Disaster Management, in\nApplication of Remote Sensing and GIS in Natural Resources and Built Infrastructure Management.\n2023, Springer. p. 39-55.\n[13] Liu, J., Y. Chen, and Y. Chen, Emergency and disaster management-crowd evacuation\nresearch. Journal of Industrial Information Integration, 2021. 21: p. 100191.\n[14] Hu, R., et al., RDC-SAL: Refine distance compensating with quantum scale-aware learning for\ncrowd counting and localization. Applied Intelligence, 2022. 52(12): p. 14336-14348.\n[15] Perez, H., et al., Task-based crowd simulation for heterogeneous architectures, in Innovative\nResearch and Applications in Next-Generation High Performance Computing. 2016, IGI Global. p.\n194-219.\n264",
        "tables": [],
        "forms": {},
        "confidence_score": 1.0
      },
      {
        "page_number": 6,
        "text_content": "J. Amr. Uni. 03 (2023) p.259 Hisham H. Y. Sa\u2019ad et al.\n[16] Jiang, Y., et al., Ultra large-scale crowd monitoring system architecture and design issues.\nIEEE Internet of Things Journal, 2021. 8(13): p. 10356-10366.\n[17] Weppner, J., et al., Participatory bluetooth scans serving as urban crowd probes. IEEE Sensors\nJournal, 2014. 14(12): p. 4196-4206.\n[18] Kostakos, V., T. Camacho, and C. Mantero. Wireless detection of end-to-end passenger trips\non public transport buses. in 13th International IEEE Conference on Intelligent Transportation\nSystems. 2010. IEEE.\n[19] Handte, M., et al. Crowd Density Estimation for Public Transport Vehicles. in EDBT/ICDT\nWorkshops. 2014. Citeseer.\n[20] Handte, M., et al., An internet-of-things enabled connected navigation system for urban bus\nriders. IEEE internet of things journal, 2016. 3(5): p. 735-744.\n[21] Lin, W.-C., W.K. Seah, and W. Li. Exploiting radio irregularity in the Internet of Things for\nautomated people counting. in 2011 IEEE 22nd International Symposium on Personal, Indoor and\nMobile Radio Communications. 2011. IEEE.\n[22] Tikkanen, T., People detection and tracking using a network of low-cost depth cameras. Aalto\nUniversity, 2014.\n[23] Tyndall, A., R. Cardell-Oliver, and A. Keating, Occupancy estimation using a low-pixel count\nthermal imager. IEEE Sensors Journal, 2016. 16(10): p. 3784-3791.\n[24] Saad, A.-M.H., et al., Impact of spatial dynamic search with matching threshold strategy on\nfractal image compression algorithm performance: study. IEEE Access, 2020. 8: p. 52687-52699.\n[25] Li, M., et al. Estimating the number of people in crowded scenes by mid based foreground\nsegmentation and head-shoulder detection. in 2008 19th international conference on pattern\nrecognition. 2008. IEEE.\n[26] Singh, U., et al., Crowd monitoring: State-of-the-art and future directions. IETE Technical\nReview, 2021. 38(6): p. 578-594.\n[27] Li, X., et al., Data fusion for intelligent crowd monitoring and management systems: A survey.\nIEEE Access, 2021. 9: p. 47069-47083.\n[28] Ding, X., et al., Crowd density estimation using fusion of multi-layer features. IEEE\nTransactions on Intelligent Transportation Systems, 2020. 22(8): p. 4776-4787.\n[29] Mallah, J.E., et al. Crowd Monitoring: Critical Situations Prevention Using Smartphones and\nGroup Detection. in Distributed, Ambient, and Pervasive Interactions: Third International\nConference, DAPI 2015, Held as Part of HCI International 2015, Los Angeles, CA, USA, August 2-\n7, 2015, Proceedings 3. 2015. Springer.\n[30] Davies, A.C., J.H. Yin, and S.A. Velastin, Crowd monitoring using image processing.\nElectronics & Communication Engineering Journal, 1995. 7(1): p. 37-47.\n[31] Sirmacek, B. and P. Reinartz. Automatic crowd density and motion analysis in airborne image\nsequences based on a probabilistic framework. in 2011 IEEE International Conference on Computer\nVision Workshops (ICCV Workshops). 2011. IEEE.\n[32] Khozium, M.O., A.G. Abuarafah, and E. AbdRabou, A proposed computer-based system\narchitecture for crowd management of pilgrims using thermography. Life Science Journal, 2012.\n9(2): p. 377-383.\n[33] Sa'ad, H.H.Y., et al., A robust structure identification method for evolving fuzzy system.\nExpert Systems with Applications, 2018. 93: p. 267-282.\n[34] Sa\u2019ad, H.H.Y., N.A.M. Isa, and M.M. Ahmed, A structural evolving approach for fuzzy\nsystems. IEEE Transactions on Fuzzy Systems, 2019. 28(2): p. 273-287.\n265",
        "tables": [],
        "forms": {},
        "confidence_score": 1.0
      },
      {
        "page_number": 7,
        "text_content": "J. Amr. Uni. 03 (2023) p.259 Hisham H. Y. Sa\u2019ad et al.\n[35] Al-Shaery, A. and M. Khozium, Crowd management challenges: Tackling approach for real\ntime crowd monitoring. Int. J. Sci. Eng. Res., 2019.\n[36] Saleh, S.A.M., S.A. Suandi, and H. Ibrahim, Recent survey on crowd density estimation and\ncounting for visual surveillance. Engineering Applications of Artificial Intelligence, 2015. 41: p.\n103-114.\n[37] Deng, Lijia,Zhou, QinghuaWang, ShuihuaG\u00f3rriz, Juan Manuel Zhang, Yudong: Deep learning\nin crowd counting: A survey. 2023\n266",
        "tables": [],
        "forms": {},
        "confidence_score": 1.0
      }
    ]
  },
  {
    "file_path": "pdfs\\1st paper.pdf",
    "total_pages": 9,
    "combined_text": "Crowd Management and Monitoring\nusing Deep Convolutinal Neural\nNetwork\nPratiksha Singh\nDepartmentofComputerScience Engineering,MadanMohanMalviyaUniver-\nsityofTechnology,Gorakhpur,India\nA K Daniel\nDepartmentofComputerScience Engineering,MadanMohanMalviyaUniver-\nsityofTechnology,Gorakhpur,India\nCorrespondingauthor:PratikshaSingh,Email:pratikshasingh1212@gmail.com\nIndia is counted as one of the most populated countries in the world. A lot of\ncrime is also increasing due to the increasing population, as criminal activities\nare more frequent in a crowded place. Being crowded is also facing a lot of dis-\neases. Therefore, crowd management and monitoring are very important there-\nfore viewed from the security, crowd management, and monitoring plays a very\nimportantroleinidentifyinggroup/individual\u2019sbehaviorinacrowdusingvideo\nand image sequence for counting the person and detection of such misbehavior\nelements, This paper proposed a model for crowd management and monitoring\npersoncountingasobjectdetectiontechniques.ThispaperproposedDeepConvo-\nlutionalNeuralNetwork,andSupportVectorMachine.Thedatasetaretakenfrom\nMall,KumbhMela,andUCFD.Theperformanceofthemodelusingtrainingand\ntestingofdataisimproved.\nKeywords: Crowd management, Crowd monitoring, Crowd detection, Radio fre-\nquencyidentification,Supportvectormachine,Deepconvolutionalneuralnetwork.\n2021.InRajuPal&PraveenK.Shukla(eds.),SCRSConferenceProceedingson\nIntelligent Systems, 171\u2013179. Computing & Intelligent Systems, SCRS, India.\nhttps://doi.org/10.52458/978-93-91842-08-6-15\nPratikshaSingh &AKDaniel\n1 Introduction\nIndia is second in the world in increasing population rate of humans. And the rate of human popu-\nlation has been increasing very rapidly in the last few times and years, that is why crime has also\nincreased due to increasing congestion in the field of development [1], [8]. Basically, the crowd is a\ngroup of organized or different people in a group and is inspired by common goals. Several methods\nof technology are used to count congestion. There are mainly two types of crowd displays, structure\ncrowds and unstructured crowds [7]. In many areas such as markets, towns, college campuses, hos-\npitals, airports, stadiums, shopping malls and cultural and religious places [1], crowds are moni-\ntored by video cameras. Computer vision congestion events have been the subject of a core re-\nsearch. This paper decodes both profitability and cutting-edge methods. It is very essential to un-\nderstand the behavior of the crowd from the research point of view, because based on this we use\ndifferent methods and technologies. It is only through the behavior of the crowd that activities in\nthe social space are shown such as crime activities, Terrorist attack etc. Several methods of algo-\nrithm have been used in computer visions such as crowd monitoring, crowd behavior, detect on the\nperson in the crowd, and find out the crowd [10]. Today many wireless devices and sensors are\navailable such as radio frequency identification is one of them and it has been used for efficient\ncrowd management, Deep Convolutional Neural Network and Support Vector Machine in efficient\ncrowd management and monitoring both.\nMonitoring the crowded area and place monitoring the crowd is a task full of challenges and diffi-\nculties, explaining a wide variety of activities. Identify the behavior of a crowd becomes unpredicta-\nble for some time. The crowd may be involved under the same act or for the same event and due to\nnon-restructuring of this behavior of the crowd, crowd management takes some time apart [17].\nBecause human behavior is different even if it is related to the same purpose or does not recognize\nthat the main objective is dangerous [9]. Humans and traffic cause jams and risks. Corona, swine\nflu, bird flu etc. viruses are also likely to spread as these viruses are caused by skin-to-skin contact\nstatus [5]. It is very important to understand their behavior even to tell riots and terrorist attacks in\na public place due to the mob. Several methods and techniques have been described to secure the\nindividual and the environment and the developed environment. Several computer vision algo-\nrithms exist to describe congestion behavior by tracking, but sometimes this algorithm also fails for\nsome reason [18], [19].\n2 Crowd Model\nThe crowd model has been classified into three categories: crowd management, crowd monitoring\n(behaviour, localisation, and counting), and crowd detection.\nFig. 1. Defined crowd Models\nA) Crowd Management\nCrowd management is a collected and proven plan and a direction towards the gradual progress of\nevents where a lot of people gather. Also, the field of crowd management has improved a lot\nthroughout the year. Real-time has been used to manage and monitor [9]. A large amount of people\ngathers while going to Hajj, and the round robin algorithm has been structured to control the\ncrowds in Hajj. Crowd management is not just for the event but including shopping malls, plazas,\nairports, etc all locations. Practical groups of people can be used as a part of crowd management\n172\n2\nSCRSConferenceProceedingsonIntelligentSystems(2021)\nwith the aim of making them direct and limited, this is called controlling congestion and is essential\nfor social security.\nB) Crowd Monitoring\nCCTV cameras are used to monitor congestion, but CCTV cameras are also unable to cover certain\nareas such as side of the wall. This is the biggest loss of CCTV cameras. Many techniques are used to\nmonitor congestion such as crowd counting, crowd localization, crowd density and crowd behavior\netc.\n\u27a2 Crowd counting entails counting a person present in an area and informing him of the\nnumber of individuals present in that area using a variety of technological approaches. [1].\n\u27a2 The term \"crowd location\" refers to where a person exists and how many persons are in\nthat area, And to indicate which person has a specific place.\n\u27a2 The density of congestion means how many people arepresent in a person and an area\nand what the distance between them is and how much more or less [8].\n\u27a2 Behavior of crowd tells the activities of the crowd what the reason for the gathering of\nthe crowd is and what the main objective is. For example, if a person goes to the airport, it\nmeans that the activity there are a traveling and place, and if a person going to cultural\nplace is mean that cultural events are happening in that place and crowd behavior tech-\nnologies have been used support vector machine and deep learning techniques [11][12].\nC) Crowd Detection\nDetermines a number by identifying individuals located at a location. The model that detects this\nfunction is called the identification model. Some researchers have described detecting crowd at-\ntendees by vision-based and wavelet template techniques in which we call HWD (head wavelet de-\ntection) [8]. This technology has been used for the main function of HWD technology, to tell the\nhead size a specific description of the attribute, and SVM was used primarily to classify the presence\nof the head [21]. And some technology and algorithms have also been analyzed that identify the\narea of the human shoulder and head. The size of a human's shoulder and head area is like an ome-\nga (\u03a9). And to identify humans in the crowd, the ROI is the filter and then the effect is based on\ndifferent approaches in less congestion and successful and more density congestion [8].\n3 Methodology\nThere are various methodologies by which the object can be identified in a group set and the group\nusing various techniques.\nA) Radio Frequency Identification\nRFID (Radio Frequency Identification) has been used to identify crowded individual. It has been\nused in every area for indoor and outdoor. It has been used to count the ruse of real-time. RFID\nperform the people identify through of radio frequency and radio frequency provide unique tag chip\nidentification card and a card is identified through the wireless sensor and identified to detect via of\nGPS tracking device [13][18][15]. RFID chip are available in various sizes, sensor network for read-\ning and sensing chip would have some serious economic considerations in case of irregular event\nlike Kumbha [5]. RFID technology has even been successfully utilized for identifying purposes.\nB) Support Vector Machine\nSupport vector machine (SVM) algorithm for popular supervised machine learning algorithm, the\ncrowd density management introduced a support vector machine technique using higher-order\nsingular value decomposition (HOSVD). [16]. it is also classifying the regression problem for crowd\ncontrol is SVM algorithm the data set display a range for training and testing dataset through su-\npervised learning.\n173\n3\nPratikshaSingh &AKDaniel\nC) Deep Convolutional Neural Network\nMany methods of model work inside of deep learning such as Recurrent Neural Network (RNN),\nAuto encoders (AE) and Artificial Neural Networks (ANN) but Convolutional Neural Networks\n(CNN) very special model. This model is useful for computer vision and image analysis [8]. CNN is\nan assortment for Deep neural network and classifies by identifying of special feature of the image.\nIt provides a comprehensive look for visual objective and in this application the validation of image\nand video such as Computer Vision, Medical Image analysis (MIA), image classification and natural\nlanguage processing (NLP) are composite. CNN display the mathematical function and his calculate\ncongestion. In a process known as Feature Extraction, a convolution tool isolates and identifies the\ndifferent characteristics of a picture for analysis.\nThere are three-layer Pooling layer, Convolutional layer, and Full connected layer [20], and com-\nprised of two additional parameters like dropout and active function and with this help to show its\nresult.\n4 Proposed Model\nThis proposed model consists of data set as input and counting and localization as the techniques\nused for analyses and given objective.\nA) Counting\nHowever, a description of the person present in a video and image and its status cannot be fully\ncommunicated. Nevertheless, we have used a variety of applications to crowd have tried to identify\nsize and person. It breaks the technical and image into a small image and calculates the distance\nand average density between the person present in each image and the detector while many places\nare such where image calculation is a very challenging task [27]. The area that can be captured\ncleanly by image and video. The exact average density of the areas of that is possible. Additionally,\nperson counting, and density is also possible through serial images. This work is displayed by map,\nso and map is used all over the world to calculate ground areas. Calculating the people present in\nthe crowd is also a complex task; MAE and MSE try to make it comfortable.\nAverage absolute error (MAE) determines the absolute average distance between real and approxi-\nmate data, although this high prediction does not punish mistakes. The average distance squared\nbetween actual data and estimated data is measured by the mean square error (MSE). Major mis-\ntakes are highlighted in this section.\n\ud835\udc40\ud835\udc34\ud835\udc38= 1\u2211\ud835\udc41 |\ud835\udc49\ud835\udc57\u2212\ud835\udc49\u2032\ud835\udc57| \u2026\u2026 (1)\n\ud835\udc41 \ud835\udc57=1\n\ud835\udc40\ud835\udc46\ud835\udc38= \u221a1\u2211\ud835\udc41 |\ud835\udc49\ud835\udc57\u2212\ud835\udc49\u2032\ud835\udc57|2..\u2026 (2)\n\ud835\udc41 \ud835\udc57=1\nThe total number of data points N, prediction value from Vjand then and jth truth value from V'j\nand MAE is mean absolute error and MSE is mean squared error value.\nB) Localization\nThis training does not fully claim the crowd's location will give accurately. Because there is a very\nchallenging task in localizing the crowd. It has been tried as an image and video. If viewed realisti-\ncally, the same head can match in multiple heads. The person present at one place is checked with\nevery angle to localize the person, and then try to demonstrate a distance estimate. And the effort\nmade has led to confirmation localization [28].\n\ud835\udc39\u2212\ud835\udc40\ud835\udc52\ud835\udc4e\ud835\udc60\ud835\udc62\ud835\udc5f\ud835\udc52= 2 \ud835\udc4b \ud835\udc5d\ud835\udc5f\ud835\udc52\ud835\udc50\ud835\udc56\ud835\udc60\ud835\udc56\ud835\udc5c\ud835\udc5b \ud835\udc4b \ud835\udc45\ud835\udc52\ud835\udc50\ud835\udc4e\ud835\udc59\ud835\udc59\u2026 (3)\n\ud835\udc43\ud835\udc5f\ud835\udc52\ud835\udc50\ud835\udc56\ud835\udc60\ud835\udc56\ud835\udc5c\ud835\udc5b+\ud835\udc45\ud835\udc52\ud835\udc50\ud835\udc4e\ud835\udc59\ud835\udc59\n\ud835\udc43\ud835\udc5f\ud835\udc52\ud835\udc50\ud835\udc56\ud835\udc60\ud835\udc56\ud835\udc5c\ud835\udc5b= \ud835\udc47\ud835\udc5f\ud835\udc62\ud835\udc52 \ud835\udc43\ud835\udc5c\ud835\udc60\ud835\udc56\ud835\udc61\ud835\udc56\ud835\udc63\ud835\udc52 .... (4)\n\ud835\udc43\ud835\udc5f\ud835\udc52\ud835\udc51\ud835\udc56\ud835\udc50\ud835\udc61\ud835\udc52\ud835\udc51 \ud835\udc63\ud835\udc4e\ud835\udc59\ud835\udc62\ud835\udc52\n174\n4\nSCRSConferenceProceedingsonIntelligentSystems(2021)\n\ud835\udc43\ud835\udc5f\ud835\udc52\ud835\udc51\ud835\udc56\ud835\udc50\ud835\udc61\ud835\udc52\ud835\udc51=\ud835\udc47\ud835\udc5f\ud835\udc62\ud835\udc52 \ud835\udc43\ud835\udc5c\ud835\udc60\ud835\udc56\ud835\udc61\ud835\udc56\ud835\udc63\ud835\udc52+\ud835\udc39\ud835\udc4e\ud835\udc59\ud835\udc60\ud835\udc52 \ud835\udc43\ud835\udc5c\ud835\udc60\ud835\udc56\ud835\udc61\ud835\udc56\ud835\udc63\ud835\udc52\u2026 (5)\n\ud835\udc45\ud835\udc52\ud835\udc50\ud835\udc4e\ud835\udc59\ud835\udc59= \ud835\udc47\ud835\udc5f\ud835\udc62\ud835\udc52 \ud835\udc5d\ud835\udc5c\ud835\udc60\ud835\udc56\ud835\udc61\ud835\udc56\ud835\udc63\ud835\udc52\u2026. (6)\n\ud835\udc34\ud835\udc50\ud835\udc61\ud835\udc62\ud835\udc4e\ud835\udc59 \ud835\udc49\ud835\udc4e\ud835\udc59\ud835\udc62\ud835\udc52\n\ud835\udc34\ud835\udc50\ud835\udc61\ud835\udc62\ud835\udc4e\ud835\udc59 \ud835\udc63\ud835\udc4e\ud835\udc59\ud835\udc62\ud835\udc52=\ud835\udc47\ud835\udc5f\ud835\udc62\ud835\udc52 \ud835\udc5d\ud835\udc5c\ud835\udc60\ud835\udc56\ud835\udc61\ud835\udc56\ud835\udc63\ud835\udc52+\ud835\udc39\ud835\udc4e\ud835\udc59\ud835\udc60\ud835\udc52 \ud835\udc41\ud835\udc52\ud835\udc54\ud835\udc56\ud835\udc61\ud835\udc56\ud835\udc63\ud835\udc52 (7)\nwhere True Positive, False Positive and False Negative. Normally, box level Precision, Recall, and F-\nmeasure are utilized for crowd localization tasks.\nC) Dataset\nFor the result we need some real-world data sets such as crowd videos and crowd pictures. For this\nwe have some data set on the publicly such as the data set of the mall, fair, Kumbh Mela, class-\nrooms, events.\nMall's data are highly used, and they are capture with the help of CCTV camera all around area or\nhave publicly data set This includes 2000 maximum videos and image and the person counted in\nthis 2000 image and video is more the about 62325 more people. But due to the ongoing disease in\ncurrent time the crowd count is reduced [23].\nFig. 2. Mall dataset-based count person\nThe UCSD dataset is the first dataset to counting person and this data set is used by installing the\ncamera and the ends of the person in the camera present in it are made by highly resolution video\nand image in frame or at least 50 frames have been used and it has an overall population of 49885\npeople. It has mostly been used in place like mall, political areas, and stadium [22].\nTable 2. Defined by mean squared error and mean absolute error value UCSD Data set\nMethod MAS MSE\nGaussian process regression 2.23 7.95\nRidge regression 2.24 7.80\nDensity map + MESA 1.71 ---\nCount forest 1.61 4.41\nMulti-column CNN 1.08 1.34\nThe Hindu festival Kumbh Mela to be held in India, which is a crowd data set in Allahabad,\nNashik, Ujjain, and Haridwar is collected by drones and is a mass pilgrimage held every 12 years\nand a huge amount of crowd would gather. Whose management and monitoring are very im-\nportant. According to Allahabad Kumbh mela Data, the mela held in 2013 collects more than 120\nmillion people and then has more than 600 frames in 6 hours and it sets a very large amount of\ndata [25].\nFig. 3. Kumbh Mela based every 6 hours Millen are person count\n175\n5\nPratikshaSingh &AKDaniel\nNWPU data set is a publicly collected data set. It works in deep CNN methods based on small\nscales. NWPU dataset has a dense congestion limit. NWPU crowd data set has about 2133375\nhead count based on 5120 images did has gone [26].\nFig. 4. NWPU dataset\nFig. 5. NWPU person evolution and person count\nShanghai Technology is a very large data set and uses at least 1198 crowd image to count people\nand it divided into two parts of the data set which is first on his 482 image and the second which\nis in part there are 716 images, and the first part is divide two-way as testing and training dataset\nand second images also divide two-way training and testing dataset and it states the best result\n[24].\nFig. 6. ShanghaiTech Part A, Part B Based person count\nThe World Expo\u201910 data set was first displayed by Sam at el. In this data set, 108 consecutive\ncameras were installed for surveillance at the World Expo in Shanghai City in 2010. This camera\ncaptured 1,132 annotated videos and 3,980 frames. The frame had annotation of the cantor of\nindividuals running continuously on 1, 99, 923 [27][28].\nFig. 7. Would expo 10 dataset to count person with help people head\n176\n6\nSCRSConferenceProceedingsonIntelligentSystems(2021)\nTable 3. Defined person counting based on MSE and MAS Would Expo\u201910 data set\nMethod MA MS\nS E\nGaussian process regres- 2.23 7.95\nsion\nRidge regression 2.24 7.80\nDensity map + MESA 1.71 ---\nCount forest 1.61 4.41\nMulti-column CNN 1.08 1.34\nTable 4. Comparison of varies nine real-world public datasets based of frames\nData Areas Purpose No of Person head From Use Device\nImage counting\nMecca madina Counting and --- ---- Images, videos CCTV Surveillance\nbehavior\nMall Counting and 2000 62325 Images, videos CCTV Surveillance\ndetection\nShanghai tech Counting 1198 482,716 Images, videos CCTV Surveillance\npart A, B\nCUHK Counting 1535 ---- Images, videos CCTV and inter-\nnet\nUCSD Counting 2000 49885 Images, videos CCTV Surveillance\nNWPU Counting 5120 21,33,345 Image Camera\nKumbh mela Counting and 6144 120 mil- Images, videos CCTV Surveil-\nlocalization lion lance and Drone\nUCF_CC_50 Crowd man- 50 1279 Images, videos CCTV\nagement\nWould expo\u201910 Person count- 1132 1199,9233 Image videos CCTV\ning\n5 Performance Analysis of Model and Result\nA) Counting\nThe ground truth image is called a sample image. It states the exact position of the person. Support\nvector machines have been used to make this image accurate. In this study we have used the data set\nof the mall. From the point of training, we have selected mall data sets and video sequential images,\nand the conclusion is displayed in the picture.\nFig. 8. Target Image (Mall Data set)\nFig. 9. Support vector machine base crowd counting\n177\n7\nPratikshaSingh &AKDaniel\nSVM Actual\n60 40 45\n50 39 34 37\n40\n33 30\n25\n30\n30\n1 2 0 0 3 4 6 9 6 7 5 8 4\n0\nImg 35 Img30 Img 25 Img 40 Img 30 Img 50 Img 40 Img 35 Img 40\nFig. 10. Actual value and SVM method Comparison based person counting\nTable 5. Define same image different-different area count person SVM techniques with image range\nRange (Image) SVM Actual value\nImg1 (35) 3 33\nImg2 (30) 4 30\nImg3 (25) 6 25\nImg4 (40) 9 40\nImg5 (30) 6 30\nImg6 (50) 7 45\nImg7 (40) 5 39\nImg8 (35) 8 34\nImg9 (40) 4 37\nB) Localization\nIn many applications, as such the technology of high-density crowd tracking has been used. While\nground veracity has been used 1\u20131 to indicate the error of permanent taxation. F-Measure in col-\nlaboration with a Greedy Association, have tried local calculations from Recall and Precision. The\narea of the recall, precision curve is also known as L-AUC. It has been used to display overall per-\nformance [27].\nTable 6. Localization Accuracy on using different date set\nMethod Recall Precision F-measure\nM-CNN 64% 60% 62%\nLSC-CNN 73.55% 75% 74%\nCL-CNN 60% 76% 67%\nD-CNN 79% 82% 81%\n6 Conclusion\nCrowd management, monitoring and counting analysis has gained popularity in recent years for\nidentifying individual\u2019s behavior and misbehavior in video and image sequence The analysis of\nmultiple crowd management and monitoring approach had been discussed. Crowd management\nand monitoring are complicated attributed to reason such as lighting variations in each picture\nscene. The paper including the finding using the Deep convolution neural network and SVM\nbased crowd management and monitoring technique had been applied for more detailed infor-\nmation. The model is implemented for evaluation of Mall, Kumbh Mela, using would expo\u201910,\nShanghai Tech, NWPU and UCFD dataset.\n178\n8\nSCRSConferenceProceedingsonIntelligentSystems(2021)\nReferences\n[1] S. Jeevitha and R. Rajeswari, \u201cA Review of crowd counting techniques\u201d, Int. J. Res. Analytical Reviews, vol.\n5, no. 3, pp. 1343-1348, 2018.\n[2] K. Khan, W. Albattah, R. U. Khan, A. M. Qamar and D. Nayab, \u201cAdvances and trends in real time visual\ncrowd analysis\u201d , Sensors, vol. 20, no. 18, pp. 1-12, 2020.\n[3] A. Rani, \u201cThe soft crowd management-Special reference to Kumbh-Haridwar\u201d, IOSR J. Computer Eng.,\nvol.19, no. 1, pp 99-102, 2017.\n[4] C. Martella, J. Li, C. Conrado and A. Vermeeren, \u201cOn current crowd management practices and the need\nfor increased situation awareness, predication, and intervention\u201d, Safety Sci., vol. 91, pp. 381-393, 2017.\n[5] M. Yamin and Y. Ades, \u201cCrowd management with RFID and Wireless Technologies\u201d in First Int. Conf. on\nNetworks & Communications, pp. 439-442, 2009.\n[6] Wafaa M. Shalash, Aliaa Al. Hazimi, Basme Al Zahrani,\u201d A mobile based crowd management system\u201d, Int.\nJ. Adv. Res. Comput. Commu. Eng., vol. 6, pp. 205-215, 2017.\n[7] A. Khan, J. A. Shah, K. Kadir, W. Albattah and F. Khan, \u201cCrowd monitoring and Localization using Deep\nConvolutional neural network: the review\u201d, Appl. Sci., vol. 10, no. 14, pp. 1-17, 2020.\n[8] M. D. Chaudhari and A. S. Ghotkar, \u201cA study on crowd detection and density analysis for safety control\u201d,\nInt. J. Comput. Sci. Eng., vol. 6, pp. 424-428, 2018.\n[9] A. Mohammed, A. Shaery and M. O. Khozium, \u201cCrowd management challenges: Tackling Approach for real\ntime crowd monitoring\u201d, Int. J. Sci. Eng. Res., vol. 7, no. 1, pp. 84-88, 2019.\n[10] S. Lamba and N. Nain, \u201cCrowd monitoring and classification: A survey\u201d, in Advances in Computer and\nComputational Sciences, S. K. Bhatia et al. Eds. Springer Nature: Singapore, 2017, pp 21-31.\n[11] D. Yimin, C. Fudong, L. Jinping and C. Wei, \u201cAbnormal Behavior Detection Based on Optical Flow Trajec-\ntory of Human Joint Points\u201d, in Chinese Control and Decision Conference, 2019, pp. 653\u2013658.\n[12] T. Wang, M. Qiao, A. Zhu, G. Shan and H. Snoussi, \u201cAbnormal event detection via the analysis of multi-\nframe optical flow Information\u201d, Front. Comput. Sci., vol. 14, pp. 304\u2013313, 2020.\n[13]M. Addlesee, R. Curwen, S. Hodges, J. Newman, P. Steggles, A. Ward and A. Hopper, \u201cImplementing a\nsentient computing system\u201d, in Computer, vol. 34, no. 8, pp. 50\u201356, 2001.\n[14] I. Bokun and K. Zielinski, \u201cActive Badges\u2014the Next Generation Industrial Enterprises\u201d, 2009.\n[15] S. Granneman, \u201cRFID Chips Are Here\u201d, Available\nhttp://www.theregister.co.uk/2003/06/27/rfid_chips_are_here Want, R., Hopper, A., Falco, V. and Gib\nbons, J.: 1992, The active badge location system, ACM Transactions on Information Systems 10(1).\n[16] B. Zhou, F. Zhang and L. Peng, \u201cHigher-order SVD analysis for crowd density estimation\u201d, Comput. Vision\nand Image Understanding, vol. 116, no. 9, pp. 1014\u20131021, 2012.\n[17] V. A. Sindagi and V. M. Patel, \u201cA Survey of Recent Advances in CNN-based Single Image Crowd Counting\nand Density Estimation\u201d, Pattern Recognition Letters, 2017.\n[18] H. Weiming, et al. \u201cA survey on visual surveillance of object motion and behaviors\u201d, IEEE Transactions\non Systems, Man, and Cybernetics, Part C (Applications and Reviews), vol. 34, no. 3, pp. 334-352, 2004.\n[19] B. Tapas, et al. \u201cAn Adaptive Codebook Model for Change Detection with Dynamic Background\u201d, in 11th\nInt. Conf. on Signal-Image Technology & Internet-Based Systems, 2015, pp. 110-116.\n[20] C. Wang, H. Zhang, L. Yang, S. Liu and X. Cao, \u201cDeep People Counting in Extremely Dense Crowds\u201d, in\n23rd ACM Int. Conf. on Multimedia, 2015, pp. 1299-1302.\n[21] S. F. Lin, J. Y. Chen and H. X. Chao, \u201cEstimation of number of people in crowded scenes using perspective\ntransformation\u201d, IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans,\nvol. 31, no. 6, pp. 645-654, 2001.\n[22] A. B. Chan, Z. S. J. Liang, N. Vasconcelos, \u201cPrivacy preserving crowd monitoring: Counting people with-\nout people models or tracking\u201d, in IEEE Conf. on Computer Vision and Pattern Recognition, 2008, pp. 1-7.\n[23] K. Chen, S. Gong, T. Xiang and C. C. Loy, \u201cCumulative attribute space for age and crowd density estima-\ntion\u201d, in IEEE Conf. on Computer Vision and Pattern Recognition, 2013, pp. 2467-2474.\n[24] Y. Zhang, D. Zhou, S. Chen, S. Gao and Y. Ma, \u201cSingle-image crowd counting via multi-column convolu-\ntional neural network\u201d, in IEEE Conf. on Computer Vision and Pattern Recognition, 2016, pp. 589-597.\n[25] A. Pandey, M. Pandey, N. Singh and A. Trivedi, \u201cKUMBH MELA: A case study for dense crowd counting\nand modeling\u201d, Multimed. Tools Appl., vol. 79, pp. 17837\u201317858, 2020.\n[26] Q. Wang, J. Gao, W. Lin and X. Li, \u201cNWPU-crowd: A large-scale benchmark for crowd counting\u201d, IEEE\nTransac. Pattern Analy. Machine Intell., 2020.\n[27] C. Zhang, K. Kang, H. Li, X. Wang, R. Xie and X. Yang, \u201cData-driven crowd understanding: A baseline for\na largescale crowd dataset\u201d, IEEE Transac. Multimed., vol . 8, no. 6, pp. 1048\u20131061, 2016.\n[28] C. Zhang, H. Li, X. Wang and X. Yang, \u201cCross-scene crowd counting via deep convolutional neural net-\nworks\u201d, in IEEE Conf. Comput. Vision and Pattern Recog., 2015, pp. 833-84.\n179",
    "metadata": {
      "CreationDate": "D:20220215184930",
      "Creator": "PDFium",
      "Producer": "PDFium"
    },
    "error_message": null,
    "pages": [
      {
        "page_number": 0,
        "text_content": "Crowd Management and Monitoring\nusing Deep Convolutinal Neural\nNetwork\nPratiksha Singh\nDepartmentofComputerScience Engineering,MadanMohanMalviyaUniver-\nsityofTechnology,Gorakhpur,India\nA K Daniel\nDepartmentofComputerScience Engineering,MadanMohanMalviyaUniver-\nsityofTechnology,Gorakhpur,India\nCorrespondingauthor:PratikshaSingh,Email:pratikshasingh1212@gmail.com\nIndia is counted as one of the most populated countries in the world. A lot of\ncrime is also increasing due to the increasing population, as criminal activities\nare more frequent in a crowded place. Being crowded is also facing a lot of dis-\neases. Therefore, crowd management and monitoring are very important there-\nfore viewed from the security, crowd management, and monitoring plays a very\nimportantroleinidentifyinggroup/individual\u2019sbehaviorinacrowdusingvideo\nand image sequence for counting the person and detection of such misbehavior\nelements, This paper proposed a model for crowd management and monitoring\npersoncountingasobjectdetectiontechniques.ThispaperproposedDeepConvo-\nlutionalNeuralNetwork,andSupportVectorMachine.Thedatasetaretakenfrom\nMall,KumbhMela,andUCFD.Theperformanceofthemodelusingtrainingand\ntestingofdataisimproved.\nKeywords: Crowd management, Crowd monitoring, Crowd detection, Radio fre-\nquencyidentification,Supportvectormachine,Deepconvolutionalneuralnetwork.\n2021.InRajuPal&PraveenK.Shukla(eds.),SCRSConferenceProceedingson\nIntelligent Systems, 171\u2013179. Computing & Intelligent Systems, SCRS, India.\nhttps://doi.org/10.52458/978-93-91842-08-6-15",
        "tables": [],
        "forms": {},
        "confidence_score": 1.0
      },
      {
        "page_number": 1,
        "text_content": "PratikshaSingh &AKDaniel\n1 Introduction\nIndia is second in the world in increasing population rate of humans. And the rate of human popu-\nlation has been increasing very rapidly in the last few times and years, that is why crime has also\nincreased due to increasing congestion in the field of development [1], [8]. Basically, the crowd is a\ngroup of organized or different people in a group and is inspired by common goals. Several methods\nof technology are used to count congestion. There are mainly two types of crowd displays, structure\ncrowds and unstructured crowds [7]. In many areas such as markets, towns, college campuses, hos-\npitals, airports, stadiums, shopping malls and cultural and religious places [1], crowds are moni-\ntored by video cameras. Computer vision congestion events have been the subject of a core re-\nsearch. This paper decodes both profitability and cutting-edge methods. It is very essential to un-\nderstand the behavior of the crowd from the research point of view, because based on this we use\ndifferent methods and technologies. It is only through the behavior of the crowd that activities in\nthe social space are shown such as crime activities, Terrorist attack etc. Several methods of algo-\nrithm have been used in computer visions such as crowd monitoring, crowd behavior, detect on the\nperson in the crowd, and find out the crowd [10]. Today many wireless devices and sensors are\navailable such as radio frequency identification is one of them and it has been used for efficient\ncrowd management, Deep Convolutional Neural Network and Support Vector Machine in efficient\ncrowd management and monitoring both.\nMonitoring the crowded area and place monitoring the crowd is a task full of challenges and diffi-\nculties, explaining a wide variety of activities. Identify the behavior of a crowd becomes unpredicta-\nble for some time. The crowd may be involved under the same act or for the same event and due to\nnon-restructuring of this behavior of the crowd, crowd management takes some time apart [17].\nBecause human behavior is different even if it is related to the same purpose or does not recognize\nthat the main objective is dangerous [9]. Humans and traffic cause jams and risks. Corona, swine\nflu, bird flu etc. viruses are also likely to spread as these viruses are caused by skin-to-skin contact\nstatus [5]. It is very important to understand their behavior even to tell riots and terrorist attacks in\na public place due to the mob. Several methods and techniques have been described to secure the\nindividual and the environment and the developed environment. Several computer vision algo-\nrithms exist to describe congestion behavior by tracking, but sometimes this algorithm also fails for\nsome reason [18], [19].\n2 Crowd Model\nThe crowd model has been classified into three categories: crowd management, crowd monitoring\n(behaviour, localisation, and counting), and crowd detection.\nFig. 1. Defined crowd Models\nA) Crowd Management\nCrowd management is a collected and proven plan and a direction towards the gradual progress of\nevents where a lot of people gather. Also, the field of crowd management has improved a lot\nthroughout the year. Real-time has been used to manage and monitor [9]. A large amount of people\ngathers while going to Hajj, and the round robin algorithm has been structured to control the\ncrowds in Hajj. Crowd management is not just for the event but including shopping malls, plazas,\nairports, etc all locations. Practical groups of people can be used as a part of crowd management\n172",
        "tables": [],
        "forms": {},
        "confidence_score": 1.0
      },
      {
        "page_number": 2,
        "text_content": "2\nSCRSConferenceProceedingsonIntelligentSystems(2021)\nwith the aim of making them direct and limited, this is called controlling congestion and is essential\nfor social security.\nB) Crowd Monitoring\nCCTV cameras are used to monitor congestion, but CCTV cameras are also unable to cover certain\nareas such as side of the wall. This is the biggest loss of CCTV cameras. Many techniques are used to\nmonitor congestion such as crowd counting, crowd localization, crowd density and crowd behavior\netc.\n\u27a2 Crowd counting entails counting a person present in an area and informing him of the\nnumber of individuals present in that area using a variety of technological approaches. [1].\n\u27a2 The term \"crowd location\" refers to where a person exists and how many persons are in\nthat area, And to indicate which person has a specific place.\n\u27a2 The density of congestion means how many people arepresent in a person and an area\nand what the distance between them is and how much more or less [8].\n\u27a2 Behavior of crowd tells the activities of the crowd what the reason for the gathering of\nthe crowd is and what the main objective is. For example, if a person goes to the airport, it\nmeans that the activity there are a traveling and place, and if a person going to cultural\nplace is mean that cultural events are happening in that place and crowd behavior tech-\nnologies have been used support vector machine and deep learning techniques [11][12].\nC) Crowd Detection\nDetermines a number by identifying individuals located at a location. The model that detects this\nfunction is called the identification model. Some researchers have described detecting crowd at-\ntendees by vision-based and wavelet template techniques in which we call HWD (head wavelet de-\ntection) [8]. This technology has been used for the main function of HWD technology, to tell the\nhead size a specific description of the attribute, and SVM was used primarily to classify the presence\nof the head [21]. And some technology and algorithms have also been analyzed that identify the\narea of the human shoulder and head. The size of a human's shoulder and head area is like an ome-\nga (\u03a9). And to identify humans in the crowd, the ROI is the filter and then the effect is based on\ndifferent approaches in less congestion and successful and more density congestion [8].\n3 Methodology\nThere are various methodologies by which the object can be identified in a group set and the group\nusing various techniques.\nA) Radio Frequency Identification\nRFID (Radio Frequency Identification) has been used to identify crowded individual. It has been\nused in every area for indoor and outdoor. It has been used to count the ruse of real-time. RFID\nperform the people identify through of radio frequency and radio frequency provide unique tag chip\nidentification card and a card is identified through the wireless sensor and identified to detect via of\nGPS tracking device [13][18][15]. RFID chip are available in various sizes, sensor network for read-\ning and sensing chip would have some serious economic considerations in case of irregular event\nlike Kumbha [5]. RFID technology has even been successfully utilized for identifying purposes.\nB) Support Vector Machine\nSupport vector machine (SVM) algorithm for popular supervised machine learning algorithm, the\ncrowd density management introduced a support vector machine technique using higher-order\nsingular value decomposition (HOSVD). [16]. it is also classifying the regression problem for crowd\ncontrol is SVM algorithm the data set display a range for training and testing dataset through su-\npervised learning.\n173",
        "tables": [],
        "forms": {},
        "confidence_score": 1.0
      },
      {
        "page_number": 3,
        "text_content": "3\nPratikshaSingh &AKDaniel\nC) Deep Convolutional Neural Network\nMany methods of model work inside of deep learning such as Recurrent Neural Network (RNN),\nAuto encoders (AE) and Artificial Neural Networks (ANN) but Convolutional Neural Networks\n(CNN) very special model. This model is useful for computer vision and image analysis [8]. CNN is\nan assortment for Deep neural network and classifies by identifying of special feature of the image.\nIt provides a comprehensive look for visual objective and in this application the validation of image\nand video such as Computer Vision, Medical Image analysis (MIA), image classification and natural\nlanguage processing (NLP) are composite. CNN display the mathematical function and his calculate\ncongestion. In a process known as Feature Extraction, a convolution tool isolates and identifies the\ndifferent characteristics of a picture for analysis.\nThere are three-layer Pooling layer, Convolutional layer, and Full connected layer [20], and com-\nprised of two additional parameters like dropout and active function and with this help to show its\nresult.\n4 Proposed Model\nThis proposed model consists of data set as input and counting and localization as the techniques\nused for analyses and given objective.\nA) Counting\nHowever, a description of the person present in a video and image and its status cannot be fully\ncommunicated. Nevertheless, we have used a variety of applications to crowd have tried to identify\nsize and person. It breaks the technical and image into a small image and calculates the distance\nand average density between the person present in each image and the detector while many places\nare such where image calculation is a very challenging task [27]. The area that can be captured\ncleanly by image and video. The exact average density of the areas of that is possible. Additionally,\nperson counting, and density is also possible through serial images. This work is displayed by map,\nso and map is used all over the world to calculate ground areas. Calculating the people present in\nthe crowd is also a complex task; MAE and MSE try to make it comfortable.\nAverage absolute error (MAE) determines the absolute average distance between real and approxi-\nmate data, although this high prediction does not punish mistakes. The average distance squared\nbetween actual data and estimated data is measured by the mean square error (MSE). Major mis-\ntakes are highlighted in this section.\n\ud835\udc40\ud835\udc34\ud835\udc38= 1\u2211\ud835\udc41 |\ud835\udc49\ud835\udc57\u2212\ud835\udc49\u2032\ud835\udc57| \u2026\u2026 (1)\n\ud835\udc41 \ud835\udc57=1\n\ud835\udc40\ud835\udc46\ud835\udc38= \u221a1\u2211\ud835\udc41 |\ud835\udc49\ud835\udc57\u2212\ud835\udc49\u2032\ud835\udc57|2..\u2026 (2)\n\ud835\udc41 \ud835\udc57=1\nThe total number of data points N, prediction value from Vjand then and jth truth value from V'j\nand MAE is mean absolute error and MSE is mean squared error value.\nB) Localization\nThis training does not fully claim the crowd's location will give accurately. Because there is a very\nchallenging task in localizing the crowd. It has been tried as an image and video. If viewed realisti-\ncally, the same head can match in multiple heads. The person present at one place is checked with\nevery angle to localize the person, and then try to demonstrate a distance estimate. And the effort\nmade has led to confirmation localization [28].\n\ud835\udc39\u2212\ud835\udc40\ud835\udc52\ud835\udc4e\ud835\udc60\ud835\udc62\ud835\udc5f\ud835\udc52= 2 \ud835\udc4b \ud835\udc5d\ud835\udc5f\ud835\udc52\ud835\udc50\ud835\udc56\ud835\udc60\ud835\udc56\ud835\udc5c\ud835\udc5b \ud835\udc4b \ud835\udc45\ud835\udc52\ud835\udc50\ud835\udc4e\ud835\udc59\ud835\udc59\u2026 (3)\n\ud835\udc43\ud835\udc5f\ud835\udc52\ud835\udc50\ud835\udc56\ud835\udc60\ud835\udc56\ud835\udc5c\ud835\udc5b+\ud835\udc45\ud835\udc52\ud835\udc50\ud835\udc4e\ud835\udc59\ud835\udc59\n\ud835\udc43\ud835\udc5f\ud835\udc52\ud835\udc50\ud835\udc56\ud835\udc60\ud835\udc56\ud835\udc5c\ud835\udc5b= \ud835\udc47\ud835\udc5f\ud835\udc62\ud835\udc52 \ud835\udc43\ud835\udc5c\ud835\udc60\ud835\udc56\ud835\udc61\ud835\udc56\ud835\udc63\ud835\udc52 .... (4)\n\ud835\udc43\ud835\udc5f\ud835\udc52\ud835\udc51\ud835\udc56\ud835\udc50\ud835\udc61\ud835\udc52\ud835\udc51 \ud835\udc63\ud835\udc4e\ud835\udc59\ud835\udc62\ud835\udc52\n174",
        "tables": [],
        "forms": {},
        "confidence_score": 1.0
      },
      {
        "page_number": 4,
        "text_content": "4\nSCRSConferenceProceedingsonIntelligentSystems(2021)\n\ud835\udc43\ud835\udc5f\ud835\udc52\ud835\udc51\ud835\udc56\ud835\udc50\ud835\udc61\ud835\udc52\ud835\udc51=\ud835\udc47\ud835\udc5f\ud835\udc62\ud835\udc52 \ud835\udc43\ud835\udc5c\ud835\udc60\ud835\udc56\ud835\udc61\ud835\udc56\ud835\udc63\ud835\udc52+\ud835\udc39\ud835\udc4e\ud835\udc59\ud835\udc60\ud835\udc52 \ud835\udc43\ud835\udc5c\ud835\udc60\ud835\udc56\ud835\udc61\ud835\udc56\ud835\udc63\ud835\udc52\u2026 (5)\n\ud835\udc45\ud835\udc52\ud835\udc50\ud835\udc4e\ud835\udc59\ud835\udc59= \ud835\udc47\ud835\udc5f\ud835\udc62\ud835\udc52 \ud835\udc5d\ud835\udc5c\ud835\udc60\ud835\udc56\ud835\udc61\ud835\udc56\ud835\udc63\ud835\udc52\u2026. (6)\n\ud835\udc34\ud835\udc50\ud835\udc61\ud835\udc62\ud835\udc4e\ud835\udc59 \ud835\udc49\ud835\udc4e\ud835\udc59\ud835\udc62\ud835\udc52\n\ud835\udc34\ud835\udc50\ud835\udc61\ud835\udc62\ud835\udc4e\ud835\udc59 \ud835\udc63\ud835\udc4e\ud835\udc59\ud835\udc62\ud835\udc52=\ud835\udc47\ud835\udc5f\ud835\udc62\ud835\udc52 \ud835\udc5d\ud835\udc5c\ud835\udc60\ud835\udc56\ud835\udc61\ud835\udc56\ud835\udc63\ud835\udc52+\ud835\udc39\ud835\udc4e\ud835\udc59\ud835\udc60\ud835\udc52 \ud835\udc41\ud835\udc52\ud835\udc54\ud835\udc56\ud835\udc61\ud835\udc56\ud835\udc63\ud835\udc52 (7)\nwhere True Positive, False Positive and False Negative. Normally, box level Precision, Recall, and F-\nmeasure are utilized for crowd localization tasks.\nC) Dataset\nFor the result we need some real-world data sets such as crowd videos and crowd pictures. For this\nwe have some data set on the publicly such as the data set of the mall, fair, Kumbh Mela, class-\nrooms, events.\nMall's data are highly used, and they are capture with the help of CCTV camera all around area or\nhave publicly data set This includes 2000 maximum videos and image and the person counted in\nthis 2000 image and video is more the about 62325 more people. But due to the ongoing disease in\ncurrent time the crowd count is reduced [23].\nFig. 2. Mall dataset-based count person\nThe UCSD dataset is the first dataset to counting person and this data set is used by installing the\ncamera and the ends of the person in the camera present in it are made by highly resolution video\nand image in frame or at least 50 frames have been used and it has an overall population of 49885\npeople. It has mostly been used in place like mall, political areas, and stadium [22].\nTable 2. Defined by mean squared error and mean absolute error value UCSD Data set\nMethod MAS MSE\nGaussian process regression 2.23 7.95\nRidge regression 2.24 7.80\nDensity map + MESA 1.71 ---\nCount forest 1.61 4.41\nMulti-column CNN 1.08 1.34\nThe Hindu festival Kumbh Mela to be held in India, which is a crowd data set in Allahabad,\nNashik, Ujjain, and Haridwar is collected by drones and is a mass pilgrimage held every 12 years\nand a huge amount of crowd would gather. Whose management and monitoring are very im-\nportant. According to Allahabad Kumbh mela Data, the mela held in 2013 collects more than 120\nmillion people and then has more than 600 frames in 6 hours and it sets a very large amount of\ndata [25].\nFig. 3. Kumbh Mela based every 6 hours Millen are person count\n175",
        "tables": [
          [
            [
              "Method",
              "MAS",
              "MSE"
            ],
            [
              "Gaussian process regression",
              "2.23",
              "7.95"
            ],
            [
              "Ridge regression",
              "2.24",
              "7.80"
            ],
            [
              "Density map + MESA",
              "1.71",
              "---"
            ],
            [
              "Count forest",
              "1.61",
              "4.41"
            ],
            [
              "Multi-column CNN",
              "1.08",
              "1.34"
            ]
          ]
        ],
        "forms": {},
        "confidence_score": 1.0
      },
      {
        "page_number": 5,
        "text_content": "5\nPratikshaSingh &AKDaniel\nNWPU data set is a publicly collected data set. It works in deep CNN methods based on small\nscales. NWPU dataset has a dense congestion limit. NWPU crowd data set has about 2133375\nhead count based on 5120 images did has gone [26].\nFig. 4. NWPU dataset\nFig. 5. NWPU person evolution and person count\nShanghai Technology is a very large data set and uses at least 1198 crowd image to count people\nand it divided into two parts of the data set which is first on his 482 image and the second which\nis in part there are 716 images, and the first part is divide two-way as testing and training dataset\nand second images also divide two-way training and testing dataset and it states the best result\n[24].\nFig. 6. ShanghaiTech Part A, Part B Based person count\nThe World Expo\u201910 data set was first displayed by Sam at el. In this data set, 108 consecutive\ncameras were installed for surveillance at the World Expo in Shanghai City in 2010. This camera\ncaptured 1,132 annotated videos and 3,980 frames. The frame had annotation of the cantor of\nindividuals running continuously on 1, 99, 923 [27][28].\nFig. 7. Would expo 10 dataset to count person with help people head\n176",
        "tables": [],
        "forms": {},
        "confidence_score": 1.0
      },
      {
        "page_number": 6,
        "text_content": "6\nSCRSConferenceProceedingsonIntelligentSystems(2021)\nTable 3. Defined person counting based on MSE and MAS Would Expo\u201910 data set\nMethod MA MS\nS E\nGaussian process regres- 2.23 7.95\nsion\nRidge regression 2.24 7.80\nDensity map + MESA 1.71 ---\nCount forest 1.61 4.41\nMulti-column CNN 1.08 1.34\nTable 4. Comparison of varies nine real-world public datasets based of frames\nData Areas Purpose No of Person head From Use Device\nImage counting\nMecca madina Counting and --- ---- Images, videos CCTV Surveillance\nbehavior\nMall Counting and 2000 62325 Images, videos CCTV Surveillance\ndetection\nShanghai tech Counting 1198 482,716 Images, videos CCTV Surveillance\npart A, B\nCUHK Counting 1535 ---- Images, videos CCTV and inter-\nnet\nUCSD Counting 2000 49885 Images, videos CCTV Surveillance\nNWPU Counting 5120 21,33,345 Image Camera\nKumbh mela Counting and 6144 120 mil- Images, videos CCTV Surveil-\nlocalization lion lance and Drone\nUCF_CC_50 Crowd man- 50 1279 Images, videos CCTV\nagement\nWould expo\u201910 Person count- 1132 1199,9233 Image videos CCTV\ning\n5 Performance Analysis of Model and Result\nA) Counting\nThe ground truth image is called a sample image. It states the exact position of the person. Support\nvector machines have been used to make this image accurate. In this study we have used the data set\nof the mall. From the point of training, we have selected mall data sets and video sequential images,\nand the conclusion is displayed in the picture.\nFig. 8. Target Image (Mall Data set)\nFig. 9. Support vector machine base crowd counting\n177",
        "tables": [
          [
            [
              "Method",
              "MA\nS",
              "MS\nE"
            ],
            [
              "Gaussian process regres-\nsion",
              "2.23",
              "7.95"
            ],
            [
              "Ridge regression",
              "2.24",
              "7.80"
            ],
            [
              "Density map + MESA",
              "1.71",
              "---"
            ],
            [
              "Count forest",
              "1.61",
              "4.41"
            ],
            [
              "Multi-column CNN",
              "1.08",
              "1.34"
            ]
          ],
          [
            [
              "Data Areas",
              "Purpose",
              "No of\nImage",
              "Person head\ncounting",
              "From Use",
              "Device"
            ],
            [
              "Mecca madina",
              "Counting and\nbehavior",
              "---",
              "----",
              "Images, videos",
              "CCTV Surveillance"
            ],
            [
              "Mall",
              "Counting and\ndetection",
              "2000",
              "62325",
              "Images, videos",
              "CCTV Surveillance"
            ],
            [
              "Shanghai tech\npart A, B",
              "Counting",
              "1198",
              "482,716",
              "Images, videos",
              "CCTV Surveillance"
            ],
            [
              "CUHK",
              "Counting",
              "1535",
              "----",
              "Images, videos",
              "CCTV and inter-\nnet"
            ],
            [
              "UCSD",
              "Counting",
              "2000",
              "49885",
              "Images, videos",
              "CCTV Surveillance"
            ],
            [
              "NWPU",
              "Counting",
              "5120",
              "21,33,345",
              "Image",
              "Camera"
            ],
            [
              "Kumbh mela",
              "Counting and\nlocalization",
              "6144",
              "120 mil-\nlion",
              "Images, videos",
              "CCTV Surveil-\nlance and Drone"
            ],
            [
              "UCF_CC_50",
              "Crowd man-\nagement",
              "50",
              "1279",
              "Images, videos",
              "CCTV"
            ],
            [
              "Would expo\u201910",
              "Person count-\ning",
              "1132",
              "1199,9233",
              "Image videos",
              "CCTV"
            ]
          ]
        ],
        "forms": {},
        "confidence_score": 1.0
      },
      {
        "page_number": 7,
        "text_content": "7\nPratikshaSingh &AKDaniel\nSVM Actual\n60 40 45\n50 39 34 37\n40\n33 30\n25\n30\n30\n1 2 0 0 3 4 6 9 6 7 5 8 4\n0\nImg 35 Img30 Img 25 Img 40 Img 30 Img 50 Img 40 Img 35 Img 40\nFig. 10. Actual value and SVM method Comparison based person counting\nTable 5. Define same image different-different area count person SVM techniques with image range\nRange (Image) SVM Actual value\nImg1 (35) 3 33\nImg2 (30) 4 30\nImg3 (25) 6 25\nImg4 (40) 9 40\nImg5 (30) 6 30\nImg6 (50) 7 45\nImg7 (40) 5 39\nImg8 (35) 8 34\nImg9 (40) 4 37\nB) Localization\nIn many applications, as such the technology of high-density crowd tracking has been used. While\nground veracity has been used 1\u20131 to indicate the error of permanent taxation. F-Measure in col-\nlaboration with a Greedy Association, have tried local calculations from Recall and Precision. The\narea of the recall, precision curve is also known as L-AUC. It has been used to display overall per-\nformance [27].\nTable 6. Localization Accuracy on using different date set\nMethod Recall Precision F-measure\nM-CNN 64% 60% 62%\nLSC-CNN 73.55% 75% 74%\nCL-CNN 60% 76% 67%\nD-CNN 79% 82% 81%\n6 Conclusion\nCrowd management, monitoring and counting analysis has gained popularity in recent years for\nidentifying individual\u2019s behavior and misbehavior in video and image sequence The analysis of\nmultiple crowd management and monitoring approach had been discussed. Crowd management\nand monitoring are complicated attributed to reason such as lighting variations in each picture\nscene. The paper including the finding using the Deep convolution neural network and SVM\nbased crowd management and monitoring technique had been applied for more detailed infor-\nmation. The model is implemented for evaluation of Mall, Kumbh Mela, using would expo\u201910,\nShanghai Tech, NWPU and UCFD dataset.\n178",
        "tables": [
          [
            [
              "33",
              "",
              "",
              "30",
              "",
              "",
              "",
              "",
              "",
              "40",
              "",
              "",
              "30",
              "",
              "",
              "",
              "",
              "",
              "39",
              "",
              "",
              "34",
              "",
              "37",
              "",
              ""
            ],
            [
              "",
              "",
              "",
              "",
              "",
              "",
              "25",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              ""
            ],
            [
              "",
              "3",
              "",
              "",
              "4",
              "",
              "",
              "6",
              "",
              "9",
              "",
              "",
              "",
              "6",
              "",
              "",
              "7",
              "",
              "",
              "5",
              "",
              "8",
              "",
              "",
              "4",
              ""
            ]
          ],
          [
            [
              "Range (Image)",
              "SVM",
              "Actual value"
            ],
            [
              "Img1 (35)",
              "3",
              "33"
            ],
            [
              "Img2 (30)",
              "4",
              "30"
            ],
            [
              "Img3 (25)",
              "6",
              "25"
            ],
            [
              "Img4 (40)",
              "9",
              "40"
            ],
            [
              "Img5 (30)",
              "6",
              "30"
            ],
            [
              "Img6 (50)",
              "7",
              "45"
            ],
            [
              "Img7 (40)",
              "5",
              "39"
            ],
            [
              "Img8 (35)",
              "8",
              "34"
            ],
            [
              "Img9 (40)",
              "4",
              "37"
            ]
          ],
          [
            [
              "Method",
              "Recall",
              "Precision",
              "F-measure"
            ],
            [
              "M-CNN",
              "64%",
              "60%",
              "62%"
            ],
            [
              "LSC-CNN",
              "73.55%",
              "75%",
              "74%"
            ],
            [
              "CL-CNN",
              "60%",
              "76%",
              "67%"
            ],
            [
              "D-CNN",
              "79%",
              "82%",
              "81%"
            ]
          ]
        ],
        "forms": {},
        "confidence_score": 1.0
      },
      {
        "page_number": 8,
        "text_content": "8\nSCRSConferenceProceedingsonIntelligentSystems(2021)\nReferences\n[1] S. Jeevitha and R. Rajeswari, \u201cA Review of crowd counting techniques\u201d, Int. J. Res. Analytical Reviews, vol.\n5, no. 3, pp. 1343-1348, 2018.\n[2] K. Khan, W. Albattah, R. U. Khan, A. M. Qamar and D. Nayab, \u201cAdvances and trends in real time visual\ncrowd analysis\u201d , Sensors, vol. 20, no. 18, pp. 1-12, 2020.\n[3] A. Rani, \u201cThe soft crowd management-Special reference to Kumbh-Haridwar\u201d, IOSR J. Computer Eng.,\nvol.19, no. 1, pp 99-102, 2017.\n[4] C. Martella, J. Li, C. Conrado and A. Vermeeren, \u201cOn current crowd management practices and the need\nfor increased situation awareness, predication, and intervention\u201d, Safety Sci., vol. 91, pp. 381-393, 2017.\n[5] M. Yamin and Y. Ades, \u201cCrowd management with RFID and Wireless Technologies\u201d in First Int. Conf. on\nNetworks & Communications, pp. 439-442, 2009.\n[6] Wafaa M. Shalash, Aliaa Al. Hazimi, Basme Al Zahrani,\u201d A mobile based crowd management system\u201d, Int.\nJ. Adv. Res. Comput. Commu. Eng., vol. 6, pp. 205-215, 2017.\n[7] A. Khan, J. A. Shah, K. Kadir, W. Albattah and F. Khan, \u201cCrowd monitoring and Localization using Deep\nConvolutional neural network: the review\u201d, Appl. Sci., vol. 10, no. 14, pp. 1-17, 2020.\n[8] M. D. Chaudhari and A. S. Ghotkar, \u201cA study on crowd detection and density analysis for safety control\u201d,\nInt. J. Comput. Sci. Eng., vol. 6, pp. 424-428, 2018.\n[9] A. Mohammed, A. Shaery and M. O. Khozium, \u201cCrowd management challenges: Tackling Approach for real\ntime crowd monitoring\u201d, Int. J. Sci. Eng. Res., vol. 7, no. 1, pp. 84-88, 2019.\n[10] S. Lamba and N. Nain, \u201cCrowd monitoring and classification: A survey\u201d, in Advances in Computer and\nComputational Sciences, S. K. Bhatia et al. Eds. Springer Nature: Singapore, 2017, pp 21-31.\n[11] D. Yimin, C. Fudong, L. Jinping and C. Wei, \u201cAbnormal Behavior Detection Based on Optical Flow Trajec-\ntory of Human Joint Points\u201d, in Chinese Control and Decision Conference, 2019, pp. 653\u2013658.\n[12] T. Wang, M. Qiao, A. Zhu, G. Shan and H. Snoussi, \u201cAbnormal event detection via the analysis of multi-\nframe optical flow Information\u201d, Front. Comput. Sci., vol. 14, pp. 304\u2013313, 2020.\n[13]M. Addlesee, R. Curwen, S. Hodges, J. Newman, P. Steggles, A. Ward and A. Hopper, \u201cImplementing a\nsentient computing system\u201d, in Computer, vol. 34, no. 8, pp. 50\u201356, 2001.\n[14] I. Bokun and K. Zielinski, \u201cActive Badges\u2014the Next Generation Industrial Enterprises\u201d, 2009.\n[15] S. Granneman, \u201cRFID Chips Are Here\u201d, Available\nhttp://www.theregister.co.uk/2003/06/27/rfid_chips_are_here Want, R., Hopper, A., Falco, V. and Gib\nbons, J.: 1992, The active badge location system, ACM Transactions on Information Systems 10(1).\n[16] B. Zhou, F. Zhang and L. Peng, \u201cHigher-order SVD analysis for crowd density estimation\u201d, Comput. Vision\nand Image Understanding, vol. 116, no. 9, pp. 1014\u20131021, 2012.\n[17] V. A. Sindagi and V. M. Patel, \u201cA Survey of Recent Advances in CNN-based Single Image Crowd Counting\nand Density Estimation\u201d, Pattern Recognition Letters, 2017.\n[18] H. Weiming, et al. \u201cA survey on visual surveillance of object motion and behaviors\u201d, IEEE Transactions\non Systems, Man, and Cybernetics, Part C (Applications and Reviews), vol. 34, no. 3, pp. 334-352, 2004.\n[19] B. Tapas, et al. \u201cAn Adaptive Codebook Model for Change Detection with Dynamic Background\u201d, in 11th\nInt. Conf. on Signal-Image Technology & Internet-Based Systems, 2015, pp. 110-116.\n[20] C. Wang, H. Zhang, L. Yang, S. Liu and X. Cao, \u201cDeep People Counting in Extremely Dense Crowds\u201d, in\n23rd ACM Int. Conf. on Multimedia, 2015, pp. 1299-1302.\n[21] S. F. Lin, J. Y. Chen and H. X. Chao, \u201cEstimation of number of people in crowded scenes using perspective\ntransformation\u201d, IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans,\nvol. 31, no. 6, pp. 645-654, 2001.\n[22] A. B. Chan, Z. S. J. Liang, N. Vasconcelos, \u201cPrivacy preserving crowd monitoring: Counting people with-\nout people models or tracking\u201d, in IEEE Conf. on Computer Vision and Pattern Recognition, 2008, pp. 1-7.\n[23] K. Chen, S. Gong, T. Xiang and C. C. Loy, \u201cCumulative attribute space for age and crowd density estima-\ntion\u201d, in IEEE Conf. on Computer Vision and Pattern Recognition, 2013, pp. 2467-2474.\n[24] Y. Zhang, D. Zhou, S. Chen, S. Gao and Y. Ma, \u201cSingle-image crowd counting via multi-column convolu-\ntional neural network\u201d, in IEEE Conf. on Computer Vision and Pattern Recognition, 2016, pp. 589-597.\n[25] A. Pandey, M. Pandey, N. Singh and A. Trivedi, \u201cKUMBH MELA: A case study for dense crowd counting\nand modeling\u201d, Multimed. Tools Appl., vol. 79, pp. 17837\u201317858, 2020.\n[26] Q. Wang, J. Gao, W. Lin and X. Li, \u201cNWPU-crowd: A large-scale benchmark for crowd counting\u201d, IEEE\nTransac. Pattern Analy. Machine Intell., 2020.\n[27] C. Zhang, K. Kang, H. Li, X. Wang, R. Xie and X. Yang, \u201cData-driven crowd understanding: A baseline for\na largescale crowd dataset\u201d, IEEE Transac. Multimed., vol . 8, no. 6, pp. 1048\u20131061, 2016.\n[28] C. Zhang, H. Li, X. Wang and X. Yang, \u201cCross-scene crowd counting via deep convolutional neural net-\nworks\u201d, in IEEE Conf. Comput. Vision and Pattern Recog., 2015, pp. 833-84.\n179",
        "tables": [],
        "forms": {},
        "confidence_score": 1.0
      }
    ]
  },
  {
    "file_path": "pdfs\\2nd paper.pdf",
    "total_pages": 10,
    "combined_text": "Availableonlineatwww.sciencedirect.com\nAvailableonlineatwww.sciencedirect.com\nAvailableonlineatwww.sciencedirect.com\nAvailable online at www.sciencedirect.com\n2 DushyantKumarSingh,SumitParoothi,MayankkumarRusiaetal./ProcediaComputerScience00(2019)000\u2013000\nProc S edi c aC ie om n pu c ter e Sc D ien i c r e e 00 c (2 t 019)000\u2013000 in the society. Currently, video cameras are used for such kind of surveillance and they can be deployed in several\nProcediaComputerScience00(2019)000\u2013000 www.elsevier.com/locate/procedia placeseasilysuchascompanies,railwaystations,shoppingcenters,bankofficesandATMmachinesetc.\nProcPerdoicae dCioamCpoumtepru StecrieSncciee n1c7e10 (020(22001) 93)5000\u201303\u201350900 www.elsevier.com/locate/procedia Somedecadesago,videosurveillancesystemwerenotsomuchprevalentasoftoday.Variouschangesinthisperiod\nwww.elsevier.com/locate/procedia\nhaveresultedindeploymentandgrowthofvideosurveillancesystem.Humansocialdysfunctionischangingandtheir\nThirdInternationalConferenceonComputingandNetworkCommunications(CoCoNet\u201919)\nThirdInternationalConferenceonComputingandNetworkCommunications(CoCoNet\u201919) approach to security is becoming more sensible. In present time, everyone is getting familiar with technological\nThirdInteHrnuatmionaanlCConrfoewrendceDoneCteocmtpiuotningfoanrdCNeittwyoWrkCidomemSuunricvaetioilnlsa(nCcoCeoNet\u201919) platformtohandledifferentsecuritytasks.CCTVusageisgettingmorepopularwiththepriceaffordabilityandless\nHuman Crowd Detection for City Wide Surveillance effortfortheirsetup.Thesesurveillancesystemarefruitlesstoprovidesecurity,untilinadequatecapacityoftrained\nHuman Crowd Detection for City Wide Surveillance\npeoplewiththeirhighattentioncompetencearefulfilledforwatchingtheflicks[1][2].AccordingtoH.Keval[3]and\nDushyant Kumar Singha, , Sumit Paroothib, Mayank Kumar Rusiac, Mohd. Aquib Ansarid\nDushyant Kumar Singha, \u2217 , Sumit Paroothib, Mayank Kumar Rusiac, Mohd. Aquib Ansarid N.Petrovic[4],Monitoringsystemconsistsoflargenumberofcamerasthathelpstomonitordifferentsites.Itisused\nDushyant KumaarDSepianrtgmehn a t , o \u2217 \u2217f,CSomupmuteirtScPieanrceo&otEhngii b n,eeMringa,yMaNnNIkTAKllauhmabaadr,PRrauyasgiraaj, c U,.MP.INoDhIdA. Aquib Ansarid todetectanynoticeableandunwantedorillegalactivity,soastoimmediatelyrespondtothesituationwithveryshort\naDepartmentofCompubteMroSrcgieanncSeta&nlEeny,gBineenegrliunrgu,,MKNaNrnIaTtaAklala,hINabDaIdA,Prayagraj,U.P.INDIA delay.Sometimes,searchingspecificactivityfromthelargeamountofrecordedvideosfilesseemsverydifficultand\nacDDeeppaarrttmmeennttooffCCoommppuubtteeMrroSSrccgiieeannnccSeeta&&nlEEenny,ggBiinneeeneegrrliiunnrggu,,,MMKNNaNNrnIIaTTtaAAkllallaa,hhINaabbDaaIddA,,PPrraayyaaggrraajj,,UU..PP..IINNDDIIAA\ntimeconsumingprocessforexaminingtheoccurredevent.Hence,itrequirescomputervisionsystem.\ndcDDeeppaarrttmmeennttooffCCoommppuu b tteeMrroSSrccgiieeannnccSeeta&&nlEEenny,ggBiinneeeneegrrliiunnrggu,,,MMKNNaNNrnIIaTTtaAAkllallaa,hhINaabbDaaIddA,,PPrraayyaaggrraajj,,UU..PP..IINNDDIIAA\ndcDDeeppaarrttmmeennttooffCCoommppuutteerrSScciieennccee&&EEnnggiinneeeerriinngg,,MMNNNNIITTAAllllaahhaabbaadd,,PPrraayyaaggrraajj,,UU..PP..IINNDDIIAA Thesystemproposedhereaimstoovercomelimitationsoftraditionalsurveillancetechniques.Thissystemworks\ndDepartmentofComputerScience&Engineering,MNNITAllahabad,Prayagraj,U.P.INDIA inrealtimetodetecttheunusualbehaviourofpeoplesinpublic,forexampleviolentcrowdbehaviour,andpedestrian\ndetection in restricted areas. In this paper, object detection approaches are utilized to detect human pedestrians and\nAbstract crowd behavior analysis is modeled using violent flow descriptor with SVM classification. Next part of the system\nAbstract isrealtimeinformationdisseminationtopoliceauthoritiesatdifferentlevels,i.e.localpolicestation&policehead-\nSAubrsvteriallcatnce systems are most commonly used for monitoring/surveillance of almost all public and private places. Real time\nbSeuhrvaveiiloluanrcoefstyhsetseemssysatreemmsoasdtdcoexmtrmaocnalpyaucisteydtofotrhmeosunritvoeriilnlagn/scuervaecitlilvaintyc.eIonfsaulcmhocsatsaelsl,paunbyliscusapnidciporuisvaoter upnlaecveesn. aRcetaivlittyimies quarter.Thiscomponenthelpsearlyresolutionofanysevereimpactofcrowdviolenceinpublic.Systemasawhole\ndbS eeu thr e vacve t iie lod lua b nry coe afn sta yhl sey tss eei m nsg sytsa htree emr m esa o las tdt idm c e oexm vtirm dae o ocnas lpy trae ucais mteyd otof fo thtrhe mec o asun mrit evo rer aii slnlat g hn / r sco ueu rv gae hcitl tilvh aie ntsyc e.e sIy ons f tse aum lcmh. o Acs tatsm ael asl,x p iam unbyu l m iscupsa lpn aic dceis po, rut isv h a eot s eruup rvnla eec ivl eel s an. ncaR eceta aivc litti t vyim itiy es contributestotheintelligent/smartpolicingforeffectivelawenforcement[10].\nidb seeth meaco vtr iee odu mbrya o naf una tah ll, eys is. eien. sgr y etsha t l eem triem saelatd vidmidee e x ovtir fda oeooc t a asp gtra eecao imt f y CotofCtThth Veecias s umm rev orean isl i l t atohn rrc eoe duga bhc y tti s vhe iec tsy ue. rsi I tyn ysts peu emc rs h.oAn cta a smls eas i ,xniams n o yumms e upc slo pan icc tei rso o,u ltshroeo osr muur.vn Iee tivli elsanndci affiectac ivcu itl titvy fiotiys r This paper is outlined in five sections. Despite introduction in first section, the related works corresponding to\naidsn e y tmec hot ureem dma b n yanb aun eai a nll,y gis.t ieo n.grmethaole nitr tie oma rel C tvi C midT eeVovifd coo eoo nttas ingtr uee o aou mfsCl o y fCa tTh nVe d c tia isr m eml e eor s ans s liyttoh wrreo idt u h gbo hyu t tshea ecn suy ersirty eys s t tp. eem Trsh .oe Anre taf mlosr aie xn, imsroou bmm ues p tcn loa enc sstersoa ,ln th droe eosffmu e r.c vIt e itv ilie lsa n nde cisffis eaco cuf tls itv ufi cot h yr differentmonitoringsystemaredelineatedinsection2.Section3consistsofproposedmethodologyinwhichactivity\nsais unrym veho iur l e lma m nacna e nbu dea einl p ,ge i n .te do. s rme o aon lntit etiom crh e nCv oCi l d oTe gVo ic f aco loons t tta ring enue goo tuf hsCloyC faT sne Vdcutiir sriet m yleo psnseli ryt s o owr n eia dtlhs bo, yu(ti s . e eac .nuyh r u irt meysatp.n eTr b she oei nrnea gfl )os ,ri aen n,d sroo hbm eue nstc cne oen psst o r s oae lndr t o heo effm ne. ece Itit dvieo snf dem is ffi so cor u eflst mufca ohn r detection and communication sub system are briefly described. Section four comprises the comparative analysis of\npsaun ory wveeh riullm faona rcn reo b ude nein dpgetnh tdo esm colo onc n ktietocs r uhrnC voC elioT llgV ainccac elo . nsTttrin henu isgo ptuha sl poy efr asn ded icsuc tiru rie stys le eps s sea lryscow onm iatlh pso l,e u(t tie.ea a.n u yhtour nme o sat m .nT obu hes eirns ego f)l o u,r tae ion, ndro thb heu ans t tcn wee ips ll sonsaeo n t dtho een ff lny eec ret e idv d e uon cfe ems t s hoo eref e smffu o car hnt experimentalresults.Conclusionwithanyfuturescopearedeliberatedinofsection5.\nopsu forwi v nee dril iv lfaoi n drc ure aolud sne bdp u ett nhw des icl ollon aclkt s e o csuh inrnvc oer l eiolalgas i encca tel h.e sTtr eheffnise g cpt t haivpo eef nre sde sis csuco ruf isty tshee pse lar a s wcoonme a n lps fl, oer (tc ie. e eam .uh etou nnm toa amu n toh buo esr i i nsty go)lui , ntaion pnd attr hho eal n lti c nwe gipltlo hns e eoptt u hob enlli nyc ere ped ldauo cce fes mtoho fere aemffciot a yrnt.\nTopfo h w iisneds r yivf s oitdr eum raolu iss nbd pur t oth pwe oic sle llo daclt ksoos u uisnr e vcertei h lae laseen x cti ehs .eti T neh gffiseCcp CtaiTvpeV enrei d nsisf s r coa u sfs trtshu eec s tulaarwc e o o mefnp tfh loe ertce cei amty ue. tonV nta oar m iuoto uhu sosrcio styo m luipnt u io tpe narttrv hoi alslt iionw ngil ttl ehc neh o npt iuo qbn ull eiycs r ape rld eauce cem esp toh lfo e yae e ff cdiottyr o t.\ndTo e fhtii esn c dstyi t v hsite deums a u l sisspi bpc uriot opu wos isl ael cda ti l vtsoi o tyuisn fer c o rtehm aes ree exa til hse ttiimneg ff e eCv cCi t d iTv e e oVn s eiens q sfuroaesf ntcrthue e sc.tul A arwenoi e nfn ttr fhi o cer a cct e eimtyc e.o nVm tam ariu uot nuh iso c r aci tot i ymon ipn suytpes art t e rvm oilslii ios nng alts teh ocehdnp eis uqi b gul neic esdaprl tea o ced eme s cpo rle foaya see cdi t t htyoe .\n2. RelatedWorks\nrdT eeh sti pes ocstn ytshs e tee ti msmuse ispoipcf ris ooy pus oste sae mcdtifvt o oirtyu th sfe erop thmo e lire cex eaila stu tii tmn h g oer Cvit Ciide Teso. V Tsi henqi f sura sey snt scr t ueesc m.tuAi r s enc oianf pta trhibce lae ctet i o tycg .oe Vmn ame r r iua ont u eiscaac ntoia molna p rsu mytess rtieg vmn is a iil osi naflt tse hoc e h rde neisf qiogu une nes dda s rta eon edy mescu prs leo pay isc eei d othu toes\nfrd eee lst lpe ooc w tntso her etuismn u u ses pou icfa i lsoya usc sttei a vmc i t t i yfvoi irt n ytht f her e opm roelsirct e rea ic lat tue itd mhsoe irt vieti .ideI est o.aTl s she o qisu gse eyn nsc ete rea sm. te Aissn tch iae npt car o ibc mlaet p etloe c tgo eem dnm eesruac ntrei i p cat ani t o iao nlnarre sml y a s tst eie dgmntoails tih afe ltshe oevrd eee n sfts ioguh nna edp dsp t eao nnd iyn esg cus rse ipa niscf eioot r hum es\nofree flslt poe o xwn tu soe arl tui w mnaue rsn oui fanls g yam sctte eivs m sitayf g o eirns t . htheeproelsictreicatuetdhsoirtiet.ieIst.aTlshoisgseynsetreamteisstchaepcaobmleptloetgeedneesractreipatnioanlarremlatseidgntoaltihfetheevreenftsouhnadpspeanniynsgussipnicfioorums Thisreviewconcentrateonmethodologiesfordetectingdifferentobjectsofinterestlikehuman,vehicleortanketc.\nofeflltoexwtuoarluwnaursnuianlgamcteivssitaygeins.therestrictedsite.Italsogeneratesthecompletedescriptionrelatedtotheeventshappeningsinform [5].Surveillanceisaverydemandingtopicnowadaysbecauseithelpstheobjectthatcanbetrackedoverlongperiods\no c ft 2 e 0 x 2 tu 0 a T l h w e a A rn u in th g o m rs e . s P s u a b g l e i s s . hedbyElsevierB.V. of time under different circumstances. There are many difficulties to detect and track the object within surveillance\n\u00a9 T(cid:30) (cid:30) T P (cid:30) Tc c e h hh e 2i i2 2 i s ssr 0 0 0 - 2i iir 2 2s sse 0 0 0 v a aa T i T Tn nne h h h w o oo e e ep pp A u e ee A An nnn u u u d ta aa t t h e h hc cc o r o oc cc re er r er s s s e s ss . . . s sss P P P p a aa u o u ur rr b n t tb b t i ii l s l lc cc i i i i sl ll s s b e ee h h h i e l u ue e ui d d dn ntny d dd b b be ee y o y yr rr f E E Et tt h hth l l l h se es s ee e e e C C v C v v s i i iC CCc e e e i r r r e B B B B n B BY YYt . . . i V- -V V -fi N NN . . . c C CCc - --o N NNm D DDm l lli i iit c cct e eee n nen s sso e eef ( (( h hhth t tt t tte p pp : ::T / / / / / /h c cci r rrr e eed a aa t ttI i ii v vnv e etee c ccr o oon m mmat m mimo o ono n nan s sls . .. o oCo r rro g ggn / / / l lfl i iie c ccr e eee n nnn s ssc e eee s ss / / /o b bbn y yy - -- n nCn c coc - --m n nn d dpd / /u/ 4 44t . .i. 0 0n0 / /g/ ) ))and Network v S i o d , e t o he d s u e e m to o d d i e ff ls er a e r n e t m ill o u s m tly in u at s i e o d n i c n on th d a i t ti p o l n a , c a e c s ro w s h s e a re sc c a l t e t a e r re d d et a e n c d tio ch n a o n f gi e n n g un b c a i c a k t g e r h o u u m nd a , n ex b i o s d te y n o c r e a o n f y ob o s th cu er rit o y bj e e t c c t .\nP T CPeeh oeei m s rr--m i rrs eeuvva niin eeicwwo a p tuu e io nnnndda seec rr( c C erreoe ssssCpp a ooor Nnnti ssc eiil tbbe \u2019i1illu 9iit nt)yy. d e oor ff thtthh e eeC ss Ccciiee Bnn Yttii- fifiN ccC cc -oo Nmm Dmm l iii ttc tte een ee soo eff ( htthh ttee p :TT // hh cir irr edd a tIi I vnn ettee crr onn maatt miioo onn n aas ll .oCC roo gnn /lff iee c rre een nns cce ees / ob ony n -nCC coo -mm ndpp / uu 4tt .ii 0nn / gg) aanndd NNeettwwoorrkk canbedoneonthebasisoflength,widthandheight[6].Heterogeneoustechniqueshavebeendefendedfortracking,\nC PCe ooe mmr- mmre uuv nnie iiccw aatti uioon nnd sse ( r(CCr o eoC sCp ooo NNn e set it\u2019 b\u201911i 9 l9i ) t). y. of the scientific committee of the Third International Conference on Computing and Network detectionandidentificationofobjectsinvideos[9][22].\nCKeoymwmorudnsi:caItnitoenrnse(tCoofCThoiNngest;\u20191C9o)m.puterVision;AutomatedVideoSurveillance;CCTVmonitoring;Activitydetection;Humandetection;Edge\nAnautomatedsurveillancesystem,knownasknight[1],isasystemthatisusedforvideosurveillanceandmonitor-\nKcoemywpuotrindgs.: InternetofThings;ComputerVision;AutomatedVideoSurveillance;CCTVmonitoring;Activitydetection;Humandetection;Edge\nKcoemywpuotrindgs.: InternetofThings;ComputerVision;AutomatedVideoSurveillance;CCTVmonitoring;Activitydetection;Humandetection;Edge ingthroughdifferentCCTV,whichconsistsself-operativemode.Itcandetectaswellasclassifythetargetsefficiently\ncomputing. bycoherentlytrackingtheobjectthroughdifferentcamerasusingstate-of-artcomputervisiontechniques.Itproduce\ndetailedtextualdescriptivesummaryinformationintheorbitformwithgooglemaptrackingsitelocation.Thissum-\nmary will gives direction to police officer in analysis and quick response decision. The limitations of surveillance\n1. Introduction systemincludetheinabilitytodetectobjectwhichismasked,identifyingobjectsamongcrowd,managingcrowdand\n1. Introduction workinginunfavorableweatherconditionamongtheearlierautomatedmonitoringsystem.Backgroundsubtraction\n1. Introduction\nVideosurveillanceisthefacilitytoobserveandanalyzeanyparticularsiteforidentifyingsuspiciousactivitywith [7][16]canalsobeoneofthesolutionfordifferentproblemsasliketrafficcontrolling,visualinspectionandinter-\nrespVeicdteotossuarfveetiyllaanndceseiscuthrietyfapcuilriptyosteos.obVsiedrevoesaunrdveainllaalnyczeesaynsytepmarctiacmulearinstioteefxoirstiednecnetiffoyrinsgecsuursiptyicaionudscaricmtiveictyonwtritohl actionbetweencomputerandhumanbymovingobjectdetection[19].Intheseapplicationswehavesomeobjectof\nVideosurveillanceisthefacilitytoobserveandanalyzeanyparticularsiteforidentifyingsuspiciousactivitywith\nrespecttosafetyandsecuritypurposes.Videosurveillancesystemcameintoexistenceforsecurityandcrimecontrol interestfordetectionandthatcanbetrackedfortheiractivity.\nrespecttosafetyandsecuritypurposes.Videosurveillancesystemcameintoexistenceforsecurityandcrimecontrol\nOpticalflowtechniqueisusefultoidentifythevelocitiesofmovingpointsinanimage.Opticalflowworkswhen\n\u2217 Correspondingauthor.Tel.:+91-7905514182;fax:+0-000-000-0000. background is static and foreground object is in motion. Therefore, optical flow delivers important information of\n\u2217 CE-omrraeislpaodnddriensgs:aduuthsohry.aTnetl@.:m+9n1n-i7t.9ac0.5in514182;fax:+0-000-000-0000. objectmomentumw.r.t.time[17].Hassneretal.[2]hasproposedawellknownalgorithmforviolencecrowddetec-\n\u2217\nCE-omrraeislpaodnddriensgs:aduuthsohry.aTnetl@.:m+9n1n-i7t.9ac0.5in514182;fax:+0-000-000-0000.\ntion.Here,individualandstraightfullpostureofthebodyarecoveredinthescene.Unimodalbackgroundmodelis\n1877E--0m5a0i9lacdd2r0e2ss0:TdhueshAyuantht@orsm.nPnuibt.laisch.iendbyElsevierB.V.\n(cid:30) responsibletotrackmovingpersoninthescene.\nT PT 1 1 1 T 8 8 8 e h hhe 7 7 7i ii s sr 7 7 7 s- - - -i iri s s 0 0 0 es 5 5v 5a a a 0 0i 0n nen 9 9 9 w o o (cid:30) (cid:30) p p \u00a9 o c cu e ep n n n 2 2 2 ed 0 0 0 n a ae 2 2 2 c cra 0 0 0 c ccr e e T T T ec s sse s s h h h ps e e e a aos r r A A An t ta i is u c c u u ri l l t t t tb e e h h h iic o o ol u uil r r rte n n s s sy d d . . . u e eo P P P n r rf u u u d t t b b bt h hhe l l l e er i i ie s s s C C h h h tshc e e e C Ci d d d ee B Bn b b b Ct Y Y y y yiC fi - - E E E N N cB l l l s C C s sc e e e Yo - - v v v N Nm- i i i N e e e D Dm r r r C B B B i l lt- i it c c . . N . e V V V e eeD . . n n . s so e elfic ( (t h hhe t tne t t p psTe : : / /h / /( c cihr r rd e et a atIp t tn i i v v:t/e e e/r c cnc o oar m mteiao m mtn o oiav n nle s sCc . . o ooo r rmn g gf / /m l le i ir c coe e enn n ncs s s.e e eo s sro / /gn b b/ y yCl - -i n nco c cem - -n n nps d due / /st 4 4i/n . .b 0 0g / /y ) )a-nndc-Nnde/tw4o.0r/k)Communications J.Rehgetal.[13]proposedatechniquetodetectandtrackofmovingpersonwhoiswalkinginprohibitedareaof\n(PT PCeh eoei e srCr-- iros reNeva vieneitew\u2019o1w p9ue )nn u.dn aec drc ere res ss rpe aor snt p isc oilbn eisluiitn byd il eoi r tfy tth he oef Cs cC thiee Bn tY sifi- c Ncie Ccno- t Nmifi Dmc ilti ctc oeeem nsom ef( ith tht tee tp eT: /ho /cifrr de t a hItne ivt ee Trcnh oam itridom no Ian nls tCe .o ror nng af/let i irc oee nnn acsel e soC /nb o yCn -n foe cmr -n epn du/ ct4ien.0 go /)ann dCNometwpuotrikngC oamnmd uNniectawtioonrks kioskbyusingfacedetection,skincolorandstereo.StaufferandGrimson[14]proposedaBackgroundSubtraction\n(P CCe ooe m rC-rom eNv u ien etw\u2019i1ca9u t)n i.o de n r s r ( e C sp o o C n o si N bi e li t t \u2019 y 19 o ) f . thescientificcommitteeoftheThirdInternationalConferenceonComputingandNetworkCommunications modelthathascapabilitytoprovidegoodresultswhilechangeshappensduetoillumination,repeateddisorderfrom\n(1C0o.1C0o1N6/ejt.\u2019p19ro).cs.2020.04.036\n10.1016/j.procs.2020.04.036 1877-0509\nAvailableonlineatwww.sciencedirect.com\nAvailableonlineatwww.sciencedirect.com\nAvailableonlineatwww.sciencedirect.com\nDushyant Kumar Singh et al. / Procedia Computer Science 171 (2020) 350\u2013359 351\n2 DushyantKumarSingh,SumitParoothi,MayankkumarRusiaetal./ProcediaComputerScience00(2019)000\u2013000\nProcediaComputerScience00(2019)000\u2013000 in the society. Currently, video cameras are used for such kind of surveillance and they can be deployed in several\nProcediaComputerScience00(2019)000\u2013000 www.elsevier.com/locate/procedia placeseasilysuchascompanies,railwaystations,shoppingcenters,bankofficesandATMmachinesetc.\nProcediaComputerScience00(2019)000\u2013000 www.elsevier.com/locate/procedia Somedecadesago,videosurveillancesystemwerenotsomuchprevalentasoftoday.Variouschangesinthisperiod\nwww.elsevier.com/locate/procedia\nhaveresultedindeploymentandgrowthofvideosurveillancesystem.Humansocialdysfunctionischangingandtheir\nThirdInternationalConferenceonComputingandNetworkCommunications(CoCoNet\u201919)\nThirdInternationalConferenceonComputingandNetworkCommunications(CoCoNet\u201919) approach to security is becoming more sensible. In present time, everyone is getting familiar with technological\nThirdInteHrnuatmionaanlCConrfoewrendceDoneCteocmtpiuotningfoanrdCNeittwyoWrkCidomemSuunricvaetioilnlsa(nCcoCeoNet\u201919) platformtohandledifferentsecuritytasks.CCTVusageisgettingmorepopularwiththepriceaffordabilityandless\nHuman Crowd Detection for City Wide Surveillance effortfortheirsetup.Thesesurveillancesystemarefruitlesstoprovidesecurity,untilinadequatecapacityoftrained\nHuman Crowd Detection for City Wide Surveillance\npeoplewiththeirhighattentioncompetencearefulfilledforwatchingtheflicks[1][2].AccordingtoH.Keval[3]and\nDushyant Kumar Singha, , Sumit Paroothib, Mayank Kumar Rusiac, Mohd. Aquib Ansarid\nDushyant Kumar Singha, \u2217 , Sumit Paroothib, Mayank Kumar Rusiac, Mohd. Aquib Ansarid N.Petrovic[4],Monitoringsystemconsistsoflargenumberofcamerasthathelpstomonitordifferentsites.Itisused\nDushyant KumaarDSepianrtgmehn a t , o \u2217 \u2217f,CSomupmuteirtScPieanrceo&otEhngii b n,eeMringa,yMaNnNIkTAKllauhmabaadr,PRrauyasgiraaj, c U,.MP.INoDhIdA. Aquib Ansarid todetectanynoticeableandunwantedorillegalactivity,soastoimmediatelyrespondtothesituationwithveryshort\naDepartmentofCompubteMroSrcgieanncSeta&nlEeny,gBineenegrliunrgu,,MKNaNrnIaTtaAklala,hINabDaIdA,Prayagraj,U.P.INDIA delay.Sometimes,searchingspecificactivityfromthelargeamountofrecordedvideosfilesseemsverydifficultand\nacDDeeppaarrttmmeennttooffCCoommppuubtteeMrroSSrccgiieeannnccSeeta&&nlEEenny,ggBiinneeeneegrrliiunnrggu,,,MMKNNaNNrnIIaTTtaAAkllallaa,hhINaabbDaaIddA,,PPrraayyaaggrraajj,,UU..PP..IINNDDIIAA\ntimeconsumingprocessforexaminingtheoccurredevent.Hence,itrequirescomputervisionsystem.\ndcDDeeppaarrttmmeennttooffCCoommppuu b tteeMrroSSrccgiieeannnccSeeta&&nlEEenny,ggBiinneeeneegrrliiunnrggu,,,MMKNNaNNrnIIaTTtaAAkllallaa,hhINaabbDaaIddA,,PPrraayyaaggrraajj,,UU..PP..IINNDDIIAA\ndcDDeeppaarrttmmeennttooffCCoommppuutteerrSScciieennccee&&EEnnggiinneeeerriinngg,,MMNNNNIITTAAllllaahhaabbaadd,,PPrraayyaaggrraajj,,UU..PP..IINNDDIIAA Thesystemproposedhereaimstoovercomelimitationsoftraditionalsurveillancetechniques.Thissystemworks\ndDepartmentofComputerScience&Engineering,MNNITAllahabad,Prayagraj,U.P.INDIA inrealtimetodetecttheunusualbehaviourofpeoplesinpublic,forexampleviolentcrowdbehaviour,andpedestrian\ndetection in restricted areas. In this paper, object detection approaches are utilized to detect human pedestrians and\nAbstract crowd behavior analysis is modeled using violent flow descriptor with SVM classification. Next part of the system\nAbstract isrealtimeinformationdisseminationtopoliceauthoritiesatdifferentlevels,i.e.localpolicestation&policehead-\nSAubrsvteriallcatnce systems are most commonly used for monitoring/surveillance of almost all public and private places. Real time\nbSeuhrvaveiiloluanrcoefstyhsetseemssysatreemmsoasdtdcoexmtrmaocnalpyaucisteydtofotrhmeosunritvoeriilnlagn/scuervaecitlilvaintyc.eIonfsaulcmhocsatsaelsl,paunbyliscusapnidciporuisvaoter upnlaecveesn. aRcetaivlittyimies quarter.Thiscomponenthelpsearlyresolutionofanysevereimpactofcrowdviolenceinpublic.Systemasawhole\ndbS eeu thr e vacve t iie lod lua b nry coe afn sta yhl sey tss eei m nsg sytsa htree emr m esa o las tdt idm c e oexm vtirm dae o ocnas lpy trae ucais mteyd otof fo thtrhe mec o asun mrit evo rer aii slnlat g hn / r sco ueu rv gae hcitl tilvh aie ntsyc e.e sIy ons f tse aum lcmh. o Acs tatsm ael asl,x p iam unbyu l m iscupsa lpn aic dceis po, rut isv h a eot s eruup rvnla eec ivl eel s an. ncaR eceta aivc litti t vyim itiy es contributestotheintelligent/smartpolicingforeffectivelawenforcement[10].\nidb seeth meaco vtr iee odu mbrya o naf una tah ll, eys is. eien. sgr y etsha t l eem triem saelatd vidmidee e x ovtir fda oeooc t a asp gtra eecao imt f y CotofCtThth Veecias s umm rev orean isl i l t atohn rrc eoe duga bhc y tti s vhe iec tsy ue. rsi I tyn ysts peu emc rs h.oAn cta a smls eas i ,xniams n o yumms e upc slo pan icc tei rso o,u ltshroeo osr muur.vn Iee tivli elsanndci affiectac ivcu itl titvy fiotiys r This paper is outlined in five sections. Despite introduction in first section, the related works corresponding to\naidsn e y tmec hot ureem dma b n yanb aun eai a nll,y gis.t ieo n.grmethaole nitr tie oma rel C tvi C midT eeVovifd coo eoo nttas ingtr uee o aou mfsCl o y fCa tTh nVe d c tia isr m eml e eor s ans s liyttoh wrreo idt u h gbo hyu t tshea ecn suy ersirty eys s t tp. eem Trsh .oe Anre taf mlosr aie xn, imsroou bmm ues p tcn loa enc sstersoa ,ln th droe eosffmu e r.c vIt e itv ilie lsa n nde cisffis eaco cuf tls itv ufi cot h yr differentmonitoringsystemaredelineatedinsection2.Section3consistsofproposedmethodologyinwhichactivity\nsais unrym veho iur l e lma m nacna e nbu dea einl p ,ge i n .te do. s rme o aon lntit etiom crh e nCv oCi l d oTe gVo ic f aco loons t tta ring enue goo tuf hsCloyC faT sne Vdcutiir sriet m yleo psnseli ryt s o owr n eia dtlhs bo, yu(ti s . e eac .nuyh r u irt meysatp.n eTr b she oei nrnea gfl )os ,ri aen n,d sroo hbm eue nstc cne oen psst o r s oae lndr t o heo effm ne. ece Itit dvieo snf dem is ffi so cor u eflst mufca ohn r detection and communication sub system are briefly described. Section four comprises the comparative analysis of\npsaun ory wveeh riullm faona rcn reo b ude nein dpgetnh tdo esm colo onc n ktietocs r uhrnC voC elioT llgV ainccac elo . nsTttrin henu isgo ptuha sl poy efr asn ded icsuc tiru rie stys le eps s sea lryscow onm iatlh pso l,e u(t tie.ea a.n u yhtour nme o sat m .nT obu hes eirns ego f)l o u,r tae ion, ndro thb heu ans t tcn wee ips ll sonsaeo n t dtho een ff lny eec ret e idv d e uon cfe ems t s hoo eref e smffu o car hnt experimentalresults.Conclusionwithanyfuturescopearedeliberatedinofsection5.\nopsu forwi v nee dril iv lfaoi n drc ure aolud sne bdp u ett nhw des icl ollon aclkt s e o csuh inrnvc oer l eiolalgas i encca tel h.e sTtr eheffnise g cpt t haivpo eef nre sde sis csuco ruf isty tshee pse lar a s wcoonme a n lps fl, oer (tc ie. e eam .uh etou nnm toa amu n toh buo esr i i nsty go)lui , ntaion pnd attr hho eal n lti c nwe gipltlo hns e eoptt u hob enlli nyc ere ped ldauo cce fes mtoho fere aemffciot a yrnt.\nTopfo h w iisneds r yivf s oitdr eum raolu iss nbd pur t oth pwe oic sle llo daclt ksoos u uisnr e vcertei h lae laseen x cti ehs .eti T neh gffiseCcp CtaiTvpeV enrei d nsisf s r coa u sfs trtshu eec s tulaarwc e o o mefnp tfh loe ertce cei amty ue. tonV nta oar m iuoto uhu sosrcio styo m luipnt u io tpe narttrv hoi alslt iionw ngil ttl ehc neh o npt iuo qbn ull eiycs r ape rld eauce cem esp toh lfo e yae e ff cdiottyr o t.\ndTo e fhtii esn c dstyi t v hsite deums a u l sisspi bpc uriot opu wos isl ael cda ti l vtsoi o tyuisn fer c o rtehm aes ree exa til hse ttiimneg ff e eCv cCi t d iTv e e oVn s eiens q sfuroaesf ntcrthue e sc.tul A arwenoi e nfn ttr fhi o cer a cct e eimtyc e.o nVm tam ariu uot nuh iso c r aci tot i ymon ipn suytpes art t e rvm oilslii ios nng alts teh ocehdnp eis uqi b gul neic esdaprl tea o ced eme s cpo rle foaya see cdi t t htyoe .\n2. RelatedWorks\nrdT eeh sti pes ocstn ytshs e tee ti msmuse ispoipcf ris ooy pus oste sae mcdtifvt o oirtyu th sfe erop thmo e lire cex eaila stu tii tmn h g oer Cvit Ciide Teso. V Tsi henqi f sura sey snt scr t ueesc m.tuAi r s enc oianf pta trhibce lae ctet i o tycg .oe Vmn ame r r iua ont u eiscaac ntoia molna p rsu mytess rtieg vmn is a iil osi naflt tse hoc e h rde neisf qiogu une nes dda s rta eon edy mescu prs leo pay isc eei d othu toes\nfrd eee lst lpe ooc w tntso her etuismn u u ses pou icfa i lsoya usc sttei a vmc i t t i yfvoi irt n ytht f her e opm roelsirct e rea ic lat tue itd mhsoe irt vieti .ideI est o.aTl s she o qisu gse eyn nsc ete rea sm. te Aissn tch iae npt car o ibc mlaet p etloe c tgo eem dnm eesruac ntrei i p cat ani t o iao nlnarre sml y a s tst eie dgmntoails tih afe ltshe oevrd eee n sfts ioguh nna edp dsp t eao nnd iyn esg cus rse ipa niscf eioot r hum es\nofree flslt poe o xwn tu soe arl tui w mnaue rsn oui fanls g yam sctte eivs m sitayf g o eirns t . htheeproelsictreicatuetdhsoirtiet.ieIst.aTlshoisgseynsetreamteisstchaepcaobmleptloetgeedneesractreipatnioanlarremlatseidgntoaltihfetheevreenftsouhnadpspeanniynsgussipnicfioorums Thisreviewconcentrateonmethodologiesfordetectingdifferentobjectsofinterestlikehuman,vehicleortanketc.\nofeflltoexwtuoarluwnaursnuianlgamcteivssitaygeins.therestrictedsite.Italsogeneratesthecompletedescriptionrelatedtotheeventshappeningsinform [5].Surveillanceisaverydemandingtopicnowadaysbecauseithelpstheobjectthatcanbetrackedoverlongperiods\no c ft 2 e 0 x 2 tu 0 a T l h w e a A rn u in th g o m rs e . s P s u a b g l e i s s . hedbyElsevierB.V. of time under different circumstances. There are many difficulties to detect and track the object within surveillance\n(cid:30) Tchi2s0i2s0aTnhoepeAnuathcocerss.sPaurtbilcilsehuedndbeyrEthleseCviCerBBY.-VN.C-NDlicense(http://creativecommons.org/licenses/by-nc-nd/4.0/) videoduetodifferentilluminationcondition,acrossascatteredandchangingbackground,existenceofobscurityetc.\nP (cid:30) T (cid:30) c ehei2sr 0 -ir 2se 0 vai Tne h woepueAnn u dat e hcr ocer r s es.ss P pao urntbis lci ils beh ilue i dntydbeo yrf Etht l hes e eCv s iCc e i r eBn BYt . i-V fiN. cCc-oNmDmliitcteenesoef(hthttep:T//hcirredatIivnetecronmatmioonnasl.oCrogn/lfiecreennscees/obny-nCco-mndp/u4t.i0n/g) and Network So, these models are mostly used in that places where clear detection of enunciate human body or any other object\nCPTeh oei m sr-m irseuva nineicwo a p tue io nnnda secr( c C ereo sssCpa oor Nntisc eil tbe \u20191ilu 9int)y. deorfththeeCsCcieBnYti-fiNcCc-oNmDmliitcteenesoef(hthttep:T // hcirredatIivnetecronmatmioonnasl.oCrogn /lfiecreennscees/ obny-nCco-mndp / u4t.i0n / g) and Network canbedoneonthebasisoflength,widthandheight[6].Heterogeneoustechniqueshavebeendefendedfortracking,\nCPeoemr-mreuvnieicwatuionndser(CreosCpooNnseitb\u20191il9it)y. of the scientific committee of the Third International Conference on Computing and Network\ndetectionandidentificationofobjectsinvideos[9][22].\nKCeoymwmorudnsi:caItnitoenrnse(tCoofCThoiNngest;\u20191C9o)m.puterVision;AutomatedVideoSurveillance;CCTVmonitoring;Activitydetection;Humandetection;Edge\nAnautomatedsurveillancesystem,knownasknight[1],isasystemthatisusedforvideosurveillanceandmonitor-\nKcoemywpuotrindgs.: InternetofThings;ComputerVision;AutomatedVideoSurveillance;CCTVmonitoring;Activitydetection;Humandetection;Edge\nKcoemywpuotrindgs.: InternetofThings;ComputerVision;AutomatedVideoSurveillance;CCTVmonitoring;Activitydetection;Humandetection;Edge ingthroughdifferentCCTV,whichconsistsself-operativemode.Itcandetectaswellasclassifythetargetsefficiently\ncomputing. bycoherentlytrackingtheobjectthroughdifferentcamerasusingstate-of-artcomputervisiontechniques.Itproduce\ndetailedtextualdescriptivesummaryinformationintheorbitformwithgooglemaptrackingsitelocation.Thissum-\nmary will gives direction to police officer in analysis and quick response decision. The limitations of surveillance\n1. Introduction systemincludetheinabilitytodetectobjectwhichismasked,identifyingobjectsamongcrowd,managingcrowdand\n1. Introduction workinginunfavorableweatherconditionamongtheearlierautomatedmonitoringsystem.Backgroundsubtraction\n1. Introduction\nVideosurveillanceisthefacilitytoobserveandanalyzeanyparticularsiteforidentifyingsuspiciousactivitywith [7][16]canalsobeoneofthesolutionfordifferentproblemsasliketrafficcontrolling,visualinspectionandinter-\nrespVeicdteotossuarfveetiyllaanndceseiscuthrietyfapcuilriptyosteos.obVsiedrevoesaunrdveainllaalnyczeesaynsytepmarctiacmulearinstioteefxoirstiednecnetiffoyrinsgecsuursiptyicaionudscaricmtiveictyonwtriothl actionbetweencomputerandhumanbymovingobjectdetection[19].Intheseapplicationswehavesomeobjectof\nVideosurveillanceisthefacilitytoobserveandanalyzeanyparticularsiteforidentifyingsuspiciousactivitywith\nrespecttosafetyandsecuritypurposes.Videosurveillancesystemcameintoexistenceforsecurityandcrimecontrol interestfordetectionandthatcanbetrackedfortheiractivity.\nrespecttosafetyandsecuritypurposes.Videosurveillancesystemcameintoexistenceforsecurityandcrimecontrol\nOpticalflowtechniqueisusefultoidentifythevelocitiesofmovingpointsinanimage.Opticalflowworkswhen\n\u2217 Correspondingauthor.Tel.:+91-7905514182;fax:+0-000-000-0000. background is static and foreground object is in motion. Therefore, optical flow delivers important information of\n\u2217 EC-omrraeislpaodnddriensgs:aduuthsohry.aTnetl@.:m+9n1n-i7t.9ac0.5in514182;fax:+0-000-000-0000. objectmomentumw.r.t.time[17].Hassneretal.[2]hasproposedawellknownalgorithmforviolencecrowddetec-\n\u2217\nEC-omrraeislpaodnddriensgs:aduuthsohry.aTnetl@.:m+9n1n-i7t.9ac0.5in514182;fax:+0-000-000-0000.\ntion.Here,individualandstraightfullpostureofthebodyarecoveredinthescene.Unimodalbackgroundmodelis\n1877E--0m5a0i9lacdd2r0e2ss0:TdhueshAyuantht@orsm.nPnuibt.laisch.iendbyElsevierB.V.\n(cid:30) responsibletotrackmovingpersoninthescene.\nT18h7is7-is05a0n9opcen20a2c0ceTsshearAticultehournsd.ePrutbhleisCheCdBbYy-ENlCse-vNieDrBlic.Ve.nse(http://creativecommons.org/licenses/by-nc-nd/4.0/)\nPT18ehe7isr7--irs0e5va0ine9wo (cid:30) pcuenn2d0ae2cr0creTessshpeaorAntiscuiltbehioluirtnsyd.eoPrfutbthhleiesChsceCideBnbtYyifi-ENclCsceo-vNmieDmrBiltitc.eVee.nsoef(thhtetpT:/h/cirrdeaItnivteercnoamtiomnoanlsC.oorngf/leirceenncseeso/nbyC-nocm-npdu/t4in.0g/)andNetworkCommunications J.Rehgetal.[13]proposedatechniquetodetectandtrackofmovingpersonwhoiswalkinginprohibitedareaof\n(cid:30)\n(PTCehoeisrC-iroseNvaienetw\u2019o1p9ue)nn.daecrcreessspaorntiscilbeiluitnydeorftthheeCscCieBntYifi-NcCco-NmDmiltitceeensoef(thhtetpT:/h/cirrdeaItnivteercnoamtiomnoanlsC.oorngf/leirceenncseeso/nbyC-nocm-npdu/t4in.0g/)andNetworkCommunications kioskbyusingfacedetection,skincolorandstereo.StaufferandGrimson[14]proposedaBackgroundSubtraction\n(PCeoerC-roeNvieetw\u201919u)n.derresponsibilityofthescientificcommitteeoftheThirdInternationalConferenceonComputingandNetworkCommunications\nmodelthathascapabilitytoprovidegoodresultswhilechangeshappensduetoillumination,repeateddisorderfrom\n(CoCoNet\u201919).\n352 Dushyant Kumar Singh et al. / Procedia Computer Science 171 (2020) 350\u2013359\nDushyantKumarSingh,SumitParoothi,MayankkumarRusiaetal./ProcediaComputerScience00(2019)000\u2013000 3 4 DushyantKumarSingh,SumitParoothi,MayankkumarRusiaetal./ProcediaComputerScience00(2019)000\u2013000\nbackgroundmotion,andlongscenechangingstatus.Theyobservedundesiredorsuspiciousactivitiesthroughanalysis Background Subtraction Technique [7] [17] is mostly used in real time scenario, as HOG could not work in\nof statistics rules and specified amount of time learning mechanism of common patterns of activities. Ricquebourg realtimeefficientllyduetoitshighrequirementofprocessingspeed.Therefore,insteadofprocessingeachframewe\nandBouthemy[15]usesspatiotemporalmechanismtotrackanddetectthepersonbyfindingthetemporaldifferences processtheframesinwhichthemotionisdetected.Formotiondetection,backgroundsubtractionalgorithmisused,\namongthreesuccessiveframesandafterthatcomparisonistakeplacebetweencurrentframetobackgroundreference inwhichthepixelpositionbetweentwoimagescanshowthetruedifferenceinintensitywithrespecttodisplacement.\nframewithconsideringintensitychange. If,weremovethebackgroundthenthisisassumedthatbackgroundpixelsarestaticandforegroundimagepixelsare\nAbdelkaderetal.[18]suggestedaframeworkfordetectingthegestureandactionrecognitionbasedonsilhouette in motion. When movement is detected by background subtraction algorithm, HOG + SVM Classifier is triggered\nmechanism.ThisrecognitionwasdoneviamodellingtrajectoriesonRiemannianshapemanifoldsmethodology.Kel- for human detection. Now, any threshold value can be taken accordingly to improve the accuracy of approach. If,\nlokumpuetal.[20]usedthedynamictexturebasedmethodstorecognizethehumanactivitiesinspatiotemporalway. movementinimageisfastthenhighervalueofthresholdisneeded.\nThe LBP-TOP is used for extracting the features in spatiotemporal space to identify human volumes as well as its HistogramofGradient(HOG)[11][21]isawell-knowntechniqueofthecomputervision.Itisbasicallyaglobal\nmovements.Asper[8]and[9],CCTVtechnologyhasraisedthedemandofCCTVdeploymentincommercialaswell imagedescriptorusedforobjectdetectionandfeatureextractionwithveryhighsuccessrate.Themainobjectiveof\naspublicandprivatesectorstofulfiltheneedofsecuritytask.Nowaday\u2019snumerousCCTVusershavingversatile HOGdescriptoristocounttheexistenceofgradientsorientationswithintheimagelocally.TheHOGusesnumberof\nskills,managementandtroubleshootingexperience.Forexample,USgovernmentallowswebuserstoseeliveCCTV stepsforextractingthefeaturesfromtheimage.Firststepistosplittheimageintotheblocks.Further,theseblocks\nvideofootagethroughinternettomonitorillegalcrossingandconveysuchtypeofactivitytorespectiveauthorities. areechelonedintothecells.Then,thehorizontalandverticalgradientsareevaluatedforeachpixelwithinthecell.\nResidentsofUKuseslivedigitalCCTVimagestorecognizeanysuspiciousorunpredictableactivitybysubscribing TheSobeloperatorcanbeusedforevaluatingthesegradientsasshowninequation1and2.\ncommunitysafetychannelsandrespondimmediatelytopoliceifsomethingfoundillegal.\nS (y,x)=Y(y,x+1) Y(y,x 1) (1)\nx\n\u2212 \u2212\nS (y,x)=Y(y+1,x) Y(y 1,x) (2)\n3. Methodology y \u2212 \u2212\nWhere the pixel intensity with co-ordinates value (x, y) is represented by Y(y, x). The horizontal and vertical\nThesystemwouldmakeuseoftheexistingCCTVinfrastructureinpublicplaceslikeparks,railwaystation,roads,\ngradientsarerepresentedbyS (y,x)andS (y,x)respectively.Thegradientsmagnitude(S)anditsorientation(\u03b8)can\nshoppingcomplexesetc.anddetectsuspiciousactivitiesusingsophisticatedcomputervisiontechniques.Thesystem x y\nbedrawnbyequation3andequation4.\nwould alert the main police control room (or Headquarter) and the nearest police station with relevant information\nin case any suspicious/unwanted activity is detected. The system consists of two distinct components i.e. activity\nS = S2+S2 (3)\ndetectionmoduleandcommunicationmodule. x y\n(cid:31)\nS\ny\n3.1. ActivityDetectionModule \u03b8=arctan( ) (4)\nS\nx\nThiscomponentdetectsthesuspicious/unwantedactivity[12][13][15]inrealtimefromthevideofeedandinforms Theninenumberofhistogrambinsareusedforstoringeachorientationofgradients.Later,thisprocessisrepeated\nto communication component. This system aims to detect pedestrian in the prohibited area and to identify violent foreachcell.Thecellhistogramsarecombinedforeachblock.Innextstep,blocknormalizationprocessisperformed\nbehaviorofcrowdforcitywidesurveillance. fornormalizingtheeachvalueofbins.Theblocknormalizationplaysanimportantroletomakethisdescriptorfree\nfromlightningvariations.Atlast,allthehistogramvectorsareconcatenatedintoonevector.Thisresultantvectorcan\nalsobereferencedasfeaturevectorwhichcanbeusedforobjectdetection.Further,thesefeaturevectorsareusedto\n3.1.1. PedestriandetectioninProhibitedArea\nbuildtheSVMclassifierwhichhelpstodecidewhethertheresultingobjectishumanornot.\nPedestrian detection in prohibited area is aimed to detect presence of people in a prohibited area and capture a\nSupport Vector Machine (SVM) is very effective and useful machine learning technique to classify different\npictureasevidence.Theobjectiveisachievedbytakingadvantageofbackgroundsubtractiontechniqueandhistogram\nclasses. SVM classifier technique basically used to maximize the marginal difference between two distinct classes.\noforientedgradients(HOG).Theworkflowofpedestriandetectionframeworkisillustratedinfigure1.\nPerformanceofSVMisbestinhighdimensionalspaceespeciallywhenavailabledatasetisless.SVMworkswellfor\nlinearhyperplaneaswellasmultidimensionalhyperplanebutitcanalsobegoodifdataisnotinlinearform.Sofor\nnon-linearclassificationSVMusesdifferentkernelfunctionforvariousdecisionfunction.Thisiscalledapproximation\nfunction.Twomostcommonlyusedfunctionsarelossfunctionandobjectivefunction.LossFunctiondescribeswhat\nto minimize to achieve best results. Hinge Loss is used for training classifier and maximize margin classification is\nusedforsupportvectormachine.\nC(x,y, f(x))=(1 y f(x)) (5)\n+\n\u2212 \u2217\nHere, C, x, y and f(x) are loss, sample value, true level and predicted level respectively. If y f(x) 1 then\n\u2217 \u2265\nC(x,y, f(x))=0otherwiseC(x,y, f(x))=1 y f(x).\n\u2212 \u2217\nObjectivefunctionofSVMconcernwithregularizerandloss.Wewanttofindthedecisionsurfacethatismaximum\nfarawayfromanydatapoints.Regularizercontrolsthetradeoff,sothattrainingandtestingerrorscanbeminimizedto\nimprovetheperformanceofclassifierforunseendata.Ifsamplesareclassifiedcorrectlythenweightwcanbeupdated\nbythegradientusingequation6.\nFig.1.WorkFlowofPedestrianDetectionFramework w=w+n( 2\u03bdw) (6)\n\u2212\nDushyant Kumar Singh et al. / Procedia Computer Science 171 (2020) 350\u2013359 353\nDushyantKumarSingh,SumitParoothi,MayankkumarRusiaetal./ProcediaComputerScience00(2019)000\u2013000 3 4 DushyantKumarSingh,SumitParoothi,MayankkumarRusiaetal./ProcediaComputerScience00(2019)000\u2013000\nbackgroundmotion,andlongscenechangingstatus.Theyobservedundesiredorsuspiciousactivitiesthroughanalysis Background Subtraction Technique [7] [17] is mostly used in real time scenario, as HOG could not work in\nof statistics rules and specified amount of time learning mechanism of common patterns of activities. Ricquebourg realtimeefficientllyduetoitshighrequirementofprocessingspeed.Therefore,insteadofprocessingeachframewe\nandBouthemy[15]usesspatiotemporalmechanismtotrackanddetectthepersonbyfindingthetemporaldifferences processtheframesinwhichthemotionisdetected.Formotiondetection,backgroundsubtractionalgorithmisused,\namongthreesuccessiveframesandafterthatcomparisonistakeplacebetweencurrentframetobackgroundreference inwhichthepixelpositionbetweentwoimagescanshowthetruedifferenceinintensitywithrespecttodisplacement.\nframewithconsideringintensitychange. If,weremovethebackgroundthenthisisassumedthatbackgroundpixelsarestaticandforegroundimagepixelsare\nAbdelkaderetal.[18]suggestedaframeworkfordetectingthegestureandactionrecognitionbasedonsilhouette in motion. When movement is detected by background subtraction algorithm, HOG + SVM Classifier is triggered\nmechanism.ThisrecognitionwasdoneviamodellingtrajectoriesonRiemannianshapemanifoldsmethodology.Kel- for human detection. Now, any threshold value can be taken accordingly to improve the accuracy of approach. If,\nlokumpuetal.[20]usedthedynamictexturebasedmethodstorecognizethehumanactivitiesinspatiotemporalway. movementinimageisfastthenhighervalueofthresholdisneeded.\nThe LBP-TOP is used for extracting the features in spatiotemporal space to identify human volumes as well as its HistogramofGradient(HOG)[11][21]isawell-knowntechniqueofthecomputervision.Itisbasicallyaglobal\nmovements.Asper[8]and[9],CCTVtechnologyhasraisedthedemandofCCTVdeploymentincommercialaswell imagedescriptorusedforobjectdetectionandfeatureextractionwithveryhighsuccessrate.Themainobjectiveof\naspublicandprivatesectorstofulfiltheneedofsecuritytask.Nowaday\u2019snumerousCCTVusershavingversatile HOGdescriptoristocounttheexistenceofgradientsorientationswithintheimagelocally.TheHOGusesnumberof\nskills,managementandtroubleshootingexperience.Forexample,USgovernmentallowswebuserstoseeliveCCTV stepsforextractingthefeaturesfromtheimage.Firststepistosplittheimageintotheblocks.Further,theseblocks\nvideofootagethroughinternettomonitorillegalcrossingandconveysuchtypeofactivitytorespectiveauthorities. areechelonedintothecells.Then,thehorizontalandverticalgradientsareevaluatedforeachpixelwithinthecell.\nResidentsofUKuseslivedigitalCCTVimagestorecognizeanysuspiciousorunpredictableactivitybysubscribing TheSobeloperatorcanbeusedforevaluatingthesegradientsasshowninequation1and2.\ncommunitysafetychannelsandrespondimmediatelytopoliceifsomethingfoundillegal.\nS (y,x)=Y(y,x+1) Y(y,x 1) (1)\nx\n\u2212 \u2212\nS (y,x)=Y(y+1,x) Y(y 1,x) (2)\n3. Methodology y \u2212 \u2212\nWhere the pixel intensity with co-ordinates value (x, y) is represented by Y(y, x). The horizontal and vertical\nThesystemwouldmakeuseoftheexistingCCTVinfrastructureinpublicplaceslikeparks,railwaystation,roads,\ngradientsarerepresentedbyS (y,x)andS (y,x)respectively.Thegradientsmagnitude(S)anditsorientation(\u03b8)can\nshoppingcomplexesetc.anddetectsuspiciousactivitiesusingsophisticatedcomputervisiontechniques.Thesystem x y\nbedrawnbyequation3andequation4.\nwould alert the main police control room (or Headquarter) and the nearest police station with relevant information\nin case any suspicious/unwanted activity is detected. The system consists of two distinct components i.e. activity\nS = S2+S2 (3)\ndetectionmoduleandcommunicationmodule. x y\n(cid:31)\nS\ny\n3.1. ActivityDetectionModule \u03b8=arctan( ) (4)\nS\nx\nThiscomponentdetectsthesuspicious/unwantedactivity[12][13][15]inrealtimefromthevideofeedandinforms Theninenumberofhistogrambinsareusedforstoringeachorientationofgradients.Later,thisprocessisrepeated\nto communication component. This system aims to detect pedestrian in the prohibited area and to identify violent foreachcell.Thecellhistogramsarecombinedforeachblock.Innextstep,blocknormalizationprocessisperformed\nbehaviorofcrowdforcitywidesurveillance. fornormalizingtheeachvalueofbins.Theblocknormalizationplaysanimportantroletomakethisdescriptorfree\nfromlightningvariations.Atlast,allthehistogramvectorsareconcatenatedintoonevector.Thisresultantvectorcan\nalsobereferencedasfeaturevectorwhichcanbeusedforobjectdetection.Further,thesefeaturevectorsareusedto\n3.1.1. PedestriandetectioninProhibitedArea\nbuildtheSVMclassifierwhichhelpstodecidewhethertheresultingobjectishumanornot.\nPedestrian detection in prohibited area is aimed to detect presence of people in a prohibited area and capture a\nSupport Vector Machine (SVM) is very effective and useful machine learning technique to classify different\npictureasevidence.Theobjectiveisachievedbytakingadvantageofbackgroundsubtractiontechniqueandhistogram\nclasses. SVM classifier technique basically used to maximize the marginal difference between two distinct classes.\noforientedgradients(HOG).Theworkflowofpedestriandetectionframeworkisillustratedinfigure1.\nPerformanceofSVMisbestinhighdimensionalspaceespeciallywhenavailabledatasetisless.SVMworkswellfor\nlinearhyperplaneaswellasmultidimensionalhyperplanebutitcanalsobegoodifdataisnotinlinearform.Sofor\nnon-linearclassificationSVMusesdifferentkernelfunctionforvariousdecisionfunction.Thisiscalledapproximation\nfunction.Twomostcommonlyusedfunctionsarelossfunctionandobjectivefunction.LossFunctiondescribeswhat\nto minimize to achieve best results. Hinge Loss is used for training classifier and maximize margin classification is\nusedforsupportvectormachine.\nC(x,y, f(x))=(1 y f(x)) (5)\n+\n\u2212 \u2217\nHere, C, x, y and f(x) are loss, sample value, true level and predicted level respectively. If y f(x) 1 then\n\u2217 \u2265\nC(x,y, f(x))=0otherwiseC(x,y, f(x))=1 y f(x).\n\u2212 \u2217\nObjectivefunctionofSVMconcernwithregularizerandloss.Wewanttofindthedecisionsurfacethatismaximum\nfarawayfromanydatapoints.Regularizercontrolsthetradeoff,sothattrainingandtestingerrorscanbeminimizedto\nimprovetheperformanceofclassifierforunseendata.Ifsamplesareclassifiedcorrectlythenweightwcanbeupdated\nbythegradientusingequation6.\nFig.1.WorkFlowofPedestrianDetectionFramework w=w+n( 2\u03bdw) (6)\n\u2212\n354 Dushyant Kumar Singh et al. / Procedia Computer Science 171 (2020) 350\u2013359\n6 DushyantKumarSingh,SumitParoothi,MayankkumarRusiaetal./ProcediaComputerScience00(2019)000\u2013000\nDushyantKumarSingh,SumitParoothi,MayankkumarRusiaetal./ProcediaComputerScience00(2019)000\u2013000 5\nFig.2.WorkFlowofViolentCrowdDetectionFramework\nIfsamplesaremisclassifiedthangradientofbothtermscanbeupdatedwithrespecttoweightvectorasshownin\neqation7.\nw=w+n(yx 2\u03bdw) (7)\ni i\n\u2212\nWherenisthelearningrateand\u03bdisregularizer.Asaregularizingparameterwehavetochoose1/epochs(epoch=\nNumberofiterationtotrainthemachine).\n3.1.2. ViolentCrowdBehavior\nFig.3.FlowofCommunicationModule\nViolentCrowdBehaviorfocusesonmonitoringthecrowdedeventsforoutbreaksofviolence.Efficientanalysisin\nrealtimeoperatingenvironmentcanbeachievedbyobservingdensityonaspecificpointandmotionofthecrowdin\nspecificdirection.Movementofthecrowdtowardscertainplacecanbeoneofthecausetoviolence.Forthepurpose,\n3.1.3. CommunicationModule\nwe have to find out high shape and intensive processing without compromising processing speed with respect to\nThis component is responsible for transferring of information from the site under surveillance to the pertinent\nchanges observed in vector magnitudes through time. The Violent Flows (ViF) descriptor is used here to extract\npolice control room. There are 3 types of nodes in communication module architecture i.e. main server, client A,\nthe motion information for sequence of frames. Further, these vectors are used svm classifier [18] [22] is used for\nclientB.ThebriefarchitectureofcommunicationmoduleforproposedsystemisshowninFigure3.\nclassifying the violent or non-violent behaviour. The complete process for violent crowd detection is presented in\nMainserverisresponsibleforallkindsofcommunicationsinthesystem.Asimplewebserverhasbeenutilized\nFigure2.\nforthispurpose.ARESTfulwebserviceismadetorunontheserverwhichrespondstotherequestofallotherclients.\nViF representation framework [23] [24] is used to notice the violence in the real time video sequences. It is\nCurrentlythebuilt-inserverofflaskhasbeenutilized.Italsomaintainsadatabasewhichstores:\ncalculated into various steps. First step is to evaluate the optical flow vectors between the couples of successive\nvideosequences.Theflowvectorsareprovidedforeachframepixels.ThepixelisrepresentedbyP andtheflow\nx,y,t\nConfigurationsettingsforeachsurveillancesite(discussedindetaillater).\nvectorsaredenotedbyu x,y,t andv x,y,t .Where,tisrepresentedbyindexofframe.Themagnitudeofthevectorscanbe \u2022\nIDs for each surveillance site, police control room along with description address, contact and other relevant\nrepresentedbyequation8. \u2022\ninfo.\nM = u2 +v2 (8) Messagesreceivedfromsurveillancesites.\nx,y,t x,y,t x,y,t \u2022\n(cid:31)\nNext,thebinaryindicator B x,y,t isevaluatedforeachframepixels.Itisevaluatedbyif | M x,y,t \u2212 M x,y,t \u2212 1| \u2264 \u03b8 then TheparticularsofthedatabaseschemacanbeprocuredfromFigure4.\nB = 1otherwise B = 0.Itshowstheimportanceofmagnitudesadjustmentamongvideoframes.Where,\u03b8 is\nx,y,t x,y,t\nthethresholdvaluethatisadaptivelyfixedtoaveragerateof M M .Here,theaveragemodificationchange\n| x,y,t \u2212 x,y,t \u2212 1|\nmapisevaluatedforindividualpixelsoverentiresequencesorframesbyaverageofthesebinaryvalues(asshownin\nequation9).\n1\nb = \u03a3 b (9)\nx,y x,y x,y,t\nT\nFurther,thereisneedtobeformedtheViF-descriptor.Itisformedbyapportioningthebintonon-overlappingcells\nofsizeMxN.ItisalsobeneededtocollectthemagnitudeadjustmentfrequencythatisalsohelpfultobuildtheViF-\ndescriptionforeachseparatedcell.Here,afixedsizeofhistogramisusedforrepresentingthechangeinmagnitude\ndistributionofeachcell.Finally,theseevaluatedhistogramsarefusedintoadistinctvector.Thisevaluatedvectoris\nrepresentedasViF-descriptorfeaturevector.Thenthesefeaturevectorcanbeusedtobuildanyoftheclassifier[6].\nIn this paper, the ViF-descriptor is used to extract the information from the video sequences. The information from\nFig.4.DatabaseDesign\nfeaturevectorisusedbytheSVMclassifiertoclassifytheviolentandnon-violentscenes.\nDushyant Kumar Singh et al. / Procedia Computer Science 171 (2020) 350\u2013359 355\n6 DushyantKumarSingh,SumitParoothi,MayankkumarRusiaetal./ProcediaComputerScience00(2019)000\u2013000\nDushyantKumarSingh,SumitParoothi,MayankkumarRusiaetal./ProcediaComputerScience00(2019)000\u2013000 5\nFig.2.WorkFlowofViolentCrowdDetectionFramework\nIfsamplesaremisclassifiedthangradientofbothtermscanbeupdatedwithrespecttoweightvectorasshownin\neqation7.\nw=w+n(yx 2\u03bdw) (7)\ni i\n\u2212\nWherenisthelearningrateand\u03bdisregularizer.Asaregularizingparameterwehavetochoose1/epochs(epoch=\nNumberofiterationtotrainthemachine).\n3.1.2. ViolentCrowdBehavior\nFig.3.FlowofCommunicationModule\nViolentCrowdBehaviorfocusesonmonitoringthecrowdedeventsforoutbreaksofviolence.Efficientanalysisin\nrealtimeoperatingenvironmentcanbeachievedbyobservingdensityonaspecificpointandmotionofthecrowdin\nspecificdirection.Movementofthecrowdtowardscertainplacecanbeoneofthecausetoviolence.Forthepurpose,\n3.1.3. CommunicationModule\nwe have to find out high shape and intensive processing without compromising processing speed with respect to\nThis component is responsible for transferring of information from the site under surveillance to the pertinent\nchanges observed in vector magnitudes through time. The Violent Flows (ViF) descriptor is used here to extract\npolice control room. There are 3 types of nodes in communication module architecture i.e. main server, client A,\nthe motion information for sequence of frames. Further, these vectors are used svm classifier [18] [22] is used for\nclientB.ThebriefarchitectureofcommunicationmoduleforproposedsystemisshowninFigure3.\nclassifying the violent or non-violent behaviour. The complete process for violent crowd detection is presented in\nMainserverisresponsibleforallkindsofcommunicationsinthesystem.Asimplewebserverhasbeenutilized\nFigure2.\nforthispurpose.ARESTfulwebserviceismadetorunontheserverwhichrespondstotherequestofallotherclients.\nViF representation framework [23] [24] is used to notice the violence in the real time video sequences. It is\nCurrentlythebuilt-inserverofflaskhasbeenutilized.Italsomaintainsadatabasewhichstores:\ncalculated into various steps. First step is to evaluate the optical flow vectors between the couples of successive\nvideosequences.Theflowvectorsareprovidedforeachframepixels.ThepixelisrepresentedbyP andtheflow\nx,y,t\nConfigurationsettingsforeachsurveillancesite(discussedindetaillater).\nvectorsaredenotedbyu x,y,t andv x,y,t .Where,tisrepresentedbyindexofframe.Themagnitudeofthevectorscanbe \u2022\nIDs for each surveillance site, police control room along with description address, contact and other relevant\nrepresentedbyequation8. \u2022\ninfo.\nM = u2 +v2 (8) Messagesreceivedfromsurveillancesites.\nx,y,t x,y,t x,y,t \u2022\n(cid:31)\nNext,thebinaryindicator B x,y,t isevaluatedforeachframepixels.Itisevaluatedbyif | M x,y,t \u2212 M x,y,t \u2212 1| \u2264 \u03b8 then TheparticularsofthedatabaseschemacanbeprocuredfromFigure4.\nB = 1otherwise B = 0.Itshowstheimportanceofmagnitudesadjustmentamongvideoframes.Where,\u03b8 is\nx,y,t x,y,t\nthethresholdvaluethatisadaptivelyfixedtoaveragerateof M M .Here,theaveragemodificationchange\n| x,y,t \u2212 x,y,t \u2212 1|\nmapisevaluatedforindividualpixelsoverentiresequencesorframesbyaverageofthesebinaryvalues(asshownin\nequation9).\n1\nb = \u03a3 b (9)\nx,y x,y x,y,t\nT\nFurther,thereisneedtobeformedtheViF-descriptor.Itisformedbyapportioningthebintonon-overlappingcells\nofsizeMxN.ItisalsobeneededtocollectthemagnitudeadjustmentfrequencythatisalsohelpfultobuildtheViF-\ndescriptionforeachseparatedcell.Here,afixedsizeofhistogramisusedforrepresentingthechangeinmagnitude\ndistributionofeachcell.Finally,theseevaluatedhistogramsarefusedintoadistinctvector.Thisevaluatedvectoris\nrepresentedasViF-descriptorfeaturevector.Thenthesefeaturevectorcanbeusedtobuildanyoftheclassifier[6].\nIn this paper, the ViF-descriptor is used to extract the information from the video sequences. The information from\nFig.4.DatabaseDesign\nfeaturevectorisusedbytheSVMclassifiertoclassifytheviolentandnon-violentscenes.\n356 Dushyant Kumar Singh et al. / Procedia Computer Science 171 (2020) 350\u2013359\nDushyantKumarSingh,SumitParoothi,MayankkumarRusiaetal./ProcediaComputerScience00(2019)000\u2013000 7 8 DushyantKumarSingh,SumitParoothi,MayankkumarRusiaetal./ProcediaComputerScience00(2019)000\u2013000\nClient A is placed in each site under surveillance as a node. It will receive video feed from all CCTVs on this\nsite. It will process the feed and send a thorough account of any activity to the main server. Each site will have its\nownconfigurationsettings.Itwillretrievethesesettingsperiodicallyfromthemainserver.Themessagesenttomain\nserverwillcontainthesefields:SiteID,CCTVID,ActivityRecognized,TimeStampandImageSequence.\nConfigurationsettingsgovernthefunctioningofthisnodeandalsokeepdetailsaboutthenode.Thesesettingswill\nbestoredonthelocalfilesystemitself.EachclientAnodeperiodically(15seconds)requeststhemainserverforits\nconfigurationsettingsandworksaccordingly.\nClient B is situated in each police control room as a node. It will periodically request the main server (at small\nintervalof1-2seconds)tocheckifithasanynewmessagewhichisdestinedforthiscontrolRoom.ClientB2receives\nalerts from all surveillance sites. While, other control rooms as client B1 receive only if they are the nearest site. It\nis provided with a GUI which shows which site has reported an alert. The GUI makes it easier and intuitive to get\nFig.6.DetectionResults\ninformation about any site. Configuration Settings enable the server to have fine control over each surveillance site\nwithrespecttoitsfunctioningandself-information.\nConfigurationSettingsincludethesefields:SiteID,NearestPoliceControlRoomID,MainPoliceControlRoom 4.1. ActivityDetectionModule\nID,Description(Address,Contactetc.)andisAreaProhibited(True/False).\nGraphicaluserinterface-googlemaps(GUI)enablesthepersonnelatpoliceheadquarterstomonitorthecomplete Currentlythesystemsupportsdetectionoftwoactivities.Eachactivitywastestedunderdifferentbenchmarksas\ncity.Allthesurveillancesitesaremarkedonthemap.TheCCTVcamerasareusedheretomonitortheactivitiesof applicable.\npeople.Ifanysuspiciousactivityisrecorded,thesystemsalertsthepolicepersonnelandalsohighlightsthelocation HumandetectioninprohibitedareaisdonewithHOGdescriptor[15][23][24].Thresholdingareaparameterused\noftheactivity.Thepersonnelcanalsoseekinformationofanysurveillancesitegooglemapsjavascript.APIisused inbackgroundsubtractionalsoimpactsprocessingtime.Iffalsepositiveincreasesthanprocessingtimeincreasesand\nto embed map in the GUI. google maps geocoding API is used for conversion between addresses and Geolocation. viceversa.TheresultscanbeseeninFigure6.\nHTMLandCSShavebeenusedtocustomizealertwindowsinmap.QWebViewisaWebKitwidgetfromthePyQt Theperformance(intermofprocessingtime)oftheproposedalgorithmundervariousparametersareillustratedin\nlibrary. It enables us to use java script code from within a python Qt application. A button is also displayed on the Table1andTable2.BothtablesshowtheparametertuninginHOGdescriptoraccordingtoWinStrideandScaleFactor.\ncustomized info window on google maps. The person can view the video or image (evidence) as applicable to the Where, WinStride is the stepsize in x and y directions of sliding window and the ScaleFactor is used to control the\nactivity(showninfigure5). imagepyramid.\nTable1.ParameterResultstuninginHOGdescriptorasperWinStride.\n4. EXPERIMENTALRESULTS\nS.No. WinStride(x,y) ProcessingTime\nWeusedRaspberrypi3modelBrunningonraspbianstretchatsurveillancesite.Theanalysisofvideofeedhappens 1 4,4 0.47sec\nontheedgeitselfaswithedgecomputing.Theexperimentalresultsandanalysisforeachofthecomponentisshown 2 8,8 0.10sec\n3 16,16 0.071sec\ninfollowingsections:\nTable2.ParameterResultstuninginHOGdescriptorasperScaleFactor.\nS.No. ScaleFactor ProcessingTime\n1 1.01 0.50sec\n2 1.06 0.10sec\n3 1.3 0.03sec\n4 1.5 0.029sec\nViolentcrowdbehaviordetectionalgorithmisimplementedinpython.Here,1inevery3framesareprocessedto\nachieverealtimeperformanceforaccuratetemporaldetection.Eachsequenceof15frames(aftersampling)isclassi-\nfiedasviolentbehaviorornon-violentbehaviorusingSVM.Violentflowsdatabase[25]isanexpansionbenchmark\nforcrowdviolencedetection.Weusedtotal246videosastraining(outofwhich123areforviolenceandremaining\n123non-violencebehavioridentification).Videosthatwehaveusedaredownloadedfromthewebwithaverage3.60\nseconds and are under uncontrolled, in-the-wild conditions. We adopted five-fold cross validation method which is\nmostcommonformanyproblems.The246videosarealmostequallydistributedin5parts,eachpartcontainingequal\nnumberofviolentbehaviorandnon-violentbehaviorvideowhicharerandomlyselected.Thefirstfourpartsofvideo\naretakenfortrainingandfifthpartistakenfortestingpurposes.Thetrainingsetcontains46videosofwhich23depict\nFig.5.(A)CityView,(B)SuspiciousActivityDetected,(C)SafeSiteInfo,(D)SuspiciousActivityInfo\nviolenceand23depictnon-violence.\nDushyant Kumar Singh et al. / Procedia Computer Science 171 (2020) 350\u2013359 357\nDushyantKumarSingh,SumitParoothi,MayankkumarRusiaetal./ProcediaComputerScience00(2019)000\u2013000 7 8 DushyantKumarSingh,SumitParoothi,MayankkumarRusiaetal./ProcediaComputerScience00(2019)000\u2013000\nClient A is placed in each site under surveillance as a node. It will receive video feed from all CCTVs on this\nsite. It will process the feed and send a thorough account of any activity to the main server. Each site will have its\nownconfigurationsettings.Itwillretrievethesesettingsperiodicallyfromthemainserver.Themessagesenttomain\nserverwillcontainthesefields:SiteID,CCTVID,ActivityRecognized,TimeStampandImageSequence.\nConfigurationsettingsgovernthefunctioningofthisnodeandalsokeepdetailsaboutthenode.Thesesettingswill\nbestoredonthelocalfilesystemitself.EachclientAnodeperiodically(15seconds)requeststhemainserverforits\nconfigurationsettingsandworksaccordingly.\nClient B is situated in each police control room as a node. It will periodically request the main server (at small\nintervalof1-2seconds)tocheckifithasanynewmessagewhichisdestinedforthiscontrolRoom.ClientB2receives\nalerts from all surveillance sites. While, other control rooms as client B1 receive only if they are the nearest site. It\nis provided with a GUI which shows which site has reported an alert. The GUI makes it easier and intuitive to get\nFig.6.DetectionResults\ninformation about any site. Configuration Settings enable the server to have fine control over each surveillance site\nwithrespecttoitsfunctioningandself-information.\nConfigurationSettingsincludethesefields:SiteID,NearestPoliceControlRoomID,MainPoliceControlRoom 4.1. ActivityDetectionModule\nID,Description(Address,Contactetc.)andisAreaProhibited(True/False).\nGraphicaluserinterface-googlemaps(GUI)enablesthepersonnelatpoliceheadquarterstomonitorthecomplete Currentlythesystemsupportsdetectionoftwoactivities.Eachactivitywastestedunderdifferentbenchmarksas\ncity.Allthesurveillancesitesaremarkedonthemap.TheCCTVcamerasareusedheretomonitortheactivitiesof applicable.\npeople.Ifanysuspiciousactivityisrecorded,thesystemsalertsthepolicepersonnelandalsohighlightsthelocation HumandetectioninprohibitedareaisdonewithHOGdescriptor[15][23][24].Thresholdingareaparameterused\noftheactivity.Thepersonnelcanalsoseekinformationofanysurveillancesitegooglemapsjavascript.APIisused inbackgroundsubtractionalsoimpactsprocessingtime.Iffalsepositiveincreasesthanprocessingtimeincreasesand\nto embed map in the GUI. google maps geocoding API is used for conversion between addresses and Geolocation. viceversa.TheresultscanbeseeninFigure6.\nHTMLandCSShavebeenusedtocustomizealertwindowsinmap.QWebViewisaWebKitwidgetfromthePyQt Theperformance(intermofprocessingtime)oftheproposedalgorithmundervariousparametersareillustratedin\nlibrary. It enables us to use java script code from within a python Qt application. A button is also displayed on the Table1andTable2.BothtablesshowtheparametertuninginHOGdescriptoraccordingtoWinStrideandScaleFactor.\ncustomized info window on google maps. The person can view the video or image (evidence) as applicable to the Where, WinStride is the stepsize in x and y directions of sliding window and the ScaleFactor is used to control the\nactivity(showninfigure5). imagepyramid.\nTable1.ParameterResultstuninginHOGdescriptorasperWinStride.\n4. EXPERIMENTALRESULTS\nS.No. WinStride(x,y) ProcessingTime\nWeusedRaspberrypi3modelBrunningonraspbianstretchatsurveillancesite.Theanalysisofvideofeedhappens 1 4,4 0.47sec\nontheedgeitselfaswithedgecomputing.Theexperimentalresultsandanalysisforeachofthecomponentisshown 2 8,8 0.10sec\n3 16,16 0.071sec\ninfollowingsections:\nTable2.ParameterResultstuninginHOGdescriptorasperScaleFactor.\nS.No. ScaleFactor ProcessingTime\n1 1.01 0.50sec\n2 1.06 0.10sec\n3 1.3 0.03sec\n4 1.5 0.029sec\nViolentcrowdbehaviordetectionalgorithmisimplementedinpython.Here,1inevery3framesareprocessedto\nachieverealtimeperformanceforaccuratetemporaldetection.Eachsequenceof15frames(aftersampling)isclassi-\nfiedasviolentbehaviorornon-violentbehaviorusingSVM.Violentflowsdatabase[25]isanexpansionbenchmark\nforcrowdviolencedetection.Weusedtotal246videosastraining(outofwhich123areforviolenceandremaining\n123non-violencebehavioridentification).Videosthatwehaveusedaredownloadedfromthewebwithaverage3.60\nseconds and are under uncontrolled, in-the-wild conditions. We adopted five-fold cross validation method which is\nmostcommonformanyproblems.The246videosarealmostequallydistributedin5parts,eachpartcontainingequal\nnumberofviolentbehaviorandnon-violentbehaviorvideowhicharerandomlyselected.Thefirstfourpartsofvideo\naretakenfortrainingandfifthpartistakenfortestingpurposes.Thetrainingsetcontains46videosofwhich23depict\nFig.5.(A)CityView,(B)SuspiciousActivityDetected,(C)SafeSiteInfo,(D)SuspiciousActivityInfo\nviolenceand23depictnon-violence.\n358 Dushyant Kumar Singh et al. / Procedia Computer Science 171 (2020) 350\u2013359\nDushyantKumarSingh,SumitParoothi,MayankkumarRusiaetal./ProcediaComputerScience00(2019)000\u2013000 9 10 DushyantKumarSingh,SumitParoothi,MayankkumarRusiaetal./ProcediaComputerScience00(2019)000\u2013000\ncomparedforpedestriandetectionandviolencecrowddetection.Formeasurementofprocessingtime,theRaspberry\nPi device and personal computer (PC) are used as processing unit. The system offers a host of bright prospects for\nextending the work to make it even more effective. It can be used in other various applications or detecting other\nunwantedactivitieslikeabnormalcrowddensity,multiplepeoplerunning(theftetc.),personfallingetc.fromCCTV\nfootageinrealtime.\nReferences\nFig.7.ConfusionMatrixforCrowdViolencedetectionmodel\n[1] PoliceDepartmentInformationSystemsTechnologyEnhancementProject(ISTEP).\u201dReportontheReviewoftheQueenslandPoliceService,\nBrisbane.\u201dDepartmentofJustice,OfficeofCommunityOrientedPolicingServices,Washington,DC.Bingham,M.,2014\n[2] T.Hassner,Y.Itcher,andO.Kliper-Gross.\u201dViolentFlows:Real-TimeDetectionofViolentCrowdBehavior.\u201d3rdIEEEInternationalWorkshop\nThe results can be visualized from the confusion matrix [24] [26] in Figure 7. We process selective short time onSociallyIntelligentSurveillanceandMonitoring(SISM)attheIEEEConf.onComputerVisionandPatternRecognition(CVPR),Rhode\nof delay in frame sequences separately, classifying them by labelling as either violent or nonviolent, if violent sub- Island,June2012.\nsequence of frames are detected then action take place. Acceptable results were obtained when we used PC for [3] H.Keval.\u201dEffective,design,configuration,anduseofdigitalCCTV.\u201dPhDthesis,UniversityCollegeLondon,2009\n[4] N.Petrovic,N.Jojic,andT.Huang.\u201dAdaptivevideofastforward.\u201dMultimediaToolsandApplications,26(3):327\u2013344,2005.\nprocessing. Extraction of feature vector (ViF descriptor) for a sequence of 15 frames took on an average 5 seconds\n[5] Y.Pritch,S.Ratovitch,A.Hendel,andS.Peleg.\u201dClusteredsynopsisofsurveillancevideo.\u201dInAdvancedVideoandSignalBasedSurveillance,\ntobeprocessed.Hence,appropriatesamplinghadtobedonewithoutcompromisingondelayinalerts.Experiments\npages195\u2013200,2009\nshowedthatnotmorethan15continuousframesarerequiredforclassification. [6] M.Shah,O.Javed,K.Shafique.\u201dAutomatedVisualSurveillanceinRealisticScenarios.\u201dIEEEMultiMedia,Vol.14,Issue:1,2007\nWhere,TP,FP,FNandTNareTruePositive,FalsePositive,FalseNegativeandTrueNegativerespectively.The [7] O.Javed,K.Shafique,andM.Shah.\u201dAHierarchicalApproachtoRobustBackgroundSubtractionUsingColorandGradientInformation.\u201d\nmathematicalevaluationofaccuracyandsensitivityareshowninequation10and11. Proc.IEEEWorkshoponMotionandVideoComputing,IEEECSPress,2002,pp.22-27\n[8] O.Javed.\u201dTrackingacrossMultipleCameraswithDisjointViews.\u201dProc.Proc.9thIEEEInt\u2019lConf.ComputerVision,pp.343-3572003\nTP+TN [9] R.Collins,A.Lipton,T.Kanade\u201dIntroductiontotheSpecialSectiononVideoSurveillance.\u201dIEEETrans.PatternAnalysisandMachine\nAccuracy= =81.25% (10)\nTP+FP+TN+FN Intelligence,vol.22,no.8,pp.745,2000\n[10] ManoharKarki,SaikatBasu,RobertDiBiano,SupratikMukhopadhyay,JerryWeltman,MalcolmStagg.\u201dAsymbolicframeworkforrecogniz-\nTP ingactivitiesinfullmotionsurveillancevideos.\u201dComputationalIntelligence(SSCI),pp.1-7,2016.\nSensitivity= =87.5% (11) [11] N.Dalal,B.Triggs.\u201dHistogramsoforientedgradientsforhumandetection.\u201dComputerVisionandPatternRecognition,2005.CVPR2005.\nTP+FN\nIEEE\n[12] Wrenetal.\u201dPfinder,Real-TimeTrackingoftheHumanBody.\u201dIEEETrans.PatternAnalysisandMachineIntelligence,vol.19,no.7,pp.\n4.2. CommunicationComponent\n780-785,1997.\n[13] J.Rehg,M.Loughlin,andK.Waters.\u201dVisionforaSmartKiosk.\u201dComputerVisionandPatternRecognition,IEEEPress,1997,pp.690-696\n3clientAsystemsand3clientBsystemsweremadetorunsimultaneouslywithinputfromlocallystoredvideos.A [14] StaufferandW.E.L.Grimson.\u201dLearningPatternsofActivityUsingReal-TimeTracking.\u201dIEEETrans.PatternAnalysisandMachineIntelli-\nprivatenetworkwasneededforcommunication.Thesystemwastestedusingbothmobilehotspotandcollegenetwork. gence,vol.22,no.8,2000,pp.747-757\n[15] Y.RicquebourgandP.Bouthemy.\u201dReal-TimeTrackingofMovingPersonsbyExploitingSpatiotemporalImageSlices.\u201dIEEETrans.Pattern\nNofaults/datalossincommunicationsystemwereobservedduringtesting.Therewerenoerrorsoratypicalbehavior\nAnalysisandMachineIntelligence,vol.22,no.8,2000,pp.797-808\ndetected in the communication component. The table 3 illustrates the average delay between time of occurence of\n[16] PushkarP.Goswami,DiwakarPaswan,DushyantKumarSingh.\u201dDetectingmovingobjectsintrafficsurveillancevideo.\u201dInternationalJournal\nactivity and raising of alert at client B. There was a maximum lag of 3 seconds between detection of activity at ofControlTheoryandapplications,Vol9.17,pp8423-8430(2016)\nsurveillancesiteandmessagealertatcontrolroom. [17] PushkarProtikGoswami,DushyantKumarSingh.\u201dAhybridapproachforreal-timeobjectdetectionandtrackingtocoverbackgroundturbu-\nlenceproblem.\u201dIndianJournalofScienceandTechnology,Vol9.45(2016)\n[18] Abdelkaderetal.\u201dSilhouette-basedgestureandactionrecognitionviamodelingtrajectoriesonRiemannianshapemanifolds.\u201dComputerVision\nTable3.Averagedelay(inseconds) andImageUnderstanding,115.3(2011):439-455\n[19] O.Kliper-Gross,T.Hassner,andL.Wolf.\u201dPfinder,Real-TimeTrackingoftheHumanBody.\u201dIEEETrans.PatternAnalysisandMachine\nDetection RaspberryPi PC\nIntelligence,pages31\u201345,2011\nPedestrianDetection 4.1sec 2.6sec [20] V.Kellokumpu,G.Zhao,andM.Pietikainen.\u201dHumanactivityrecognitionusingadynamictexturebasedmethod.\u201dInBMVC,pages1\u201310,\nViolentCrowdDetection 5.1sec 4.3sec 2008\n[21] O.Kliper-Gross,T.Hassner,andLiu.Beyond.\u201dPixels:ExploringNewRepresentationsandApplicationsforMotionAnalysis.\u201dPhDthesis,\nMassachusettsInstituteofTechnology,May2009\n[22] NikhilSingh,ShambhuShankarBharti,RupalSingh,DushyantKumarSingh.\u201dRemotelycontrolledhomeautomationsystem.\u201dInternational\nConferenceonAdvancesinEngineering&TechnologyResearch(ICAETR-2014),IEEE2014\n[23] Agarwal,Anshuman,ShivamGupta,andDushyantKumarSingh.\u201dReviewofopticalflowtechniqueformovingobjectdetection.\u201d2ndInter-\n5. CONCLUSIONANDFUTUREVISION\nnationalConferenceonContemporaryComputingandInformatics(IC3I),pages1\u201310,2016\n[24] KaelonLloydetal.\u201dDetectingViolentandAbnormalCrowdactivityusingTemporalAnalysisofGreyLevelCo-occurrenceMatrix(GLCM)\nDespiterecentadvancementintechnologyofcomputervisionandrelatedareas,therearestillsomemajorissues BasedTextureMeasures.\u201dComputerVisionandPatternRecognition,2017\nthatneedstobeovercomeformakingcompleterealtimeoperatingandreliableautomatedsurveillancesystem.These [25] Violent-Flows-CrowdViolence Non-violenceDatabaseandbenchmark-2017,www.cslab.openu.ac.il/download\nissuesincludetechnicalaspectasphysicalplacementtofullcoverage,installation,maintenanceofcamera,network [26] M.A.AnsariandM.Dixit.\u201dAnenhancedCBIRusingHSVquantization,discretewavelettransformandedgehistogramdescriptor.\u201d2017\nInternationalConferenceonComputing,CommunicationandAutomation(ICCCA),GreaterNoida,2017,pp.1136-1141.\nbandwidthrequiredtosupportcameraandingeneralaspectasrobustnessofcameraintypicalweatherandlightening\nconditions,installationcost,privacyconcernetc.However,demandofautomatedsurveillancesystemisbeingmade\nmore frequently specially in public safety and home security with respect to quality control parameters. Military\nintelligencecanalsobeoneoftheapplicationareaastechnologicalgrowthisalsoreachingtoheights.Thispaperis\nprovidingagoodresultforcrowdviolencedetectionintermsofaccuracyandsensitivity.Theprocessingtimeisalso\nDushyant Kumar Singh et al. / Procedia Computer Science 171 (2020) 350\u2013359 359\nDushyantKumarSingh,SumitParoothi,MayankkumarRusiaetal./ProcediaComputerScience00(2019)000\u2013000 9 10 DushyantKumarSingh,SumitParoothi,MayankkumarRusiaetal./ProcediaComputerScience00(2019)000\u2013000\ncomparedforpedestriandetectionandviolencecrowddetection.Formeasurementofprocessingtime,theRaspberry\nPi device and personal computer (PC) are used as processing unit. The system offers a host of bright prospects for\nextending the work to make it even more effective. It can be used in other various applications or detecting other\nunwantedactivitieslikeabnormalcrowddensity,multiplepeoplerunning(theftetc.),personfallingetc.fromCCTV\nfootageinrealtime.\nReferences\nFig.7.ConfusionMatrixforCrowdViolencedetectionmodel\n[1] PoliceDepartmentInformationSystemsTechnologyEnhancementProject(ISTEP).\u201dReportontheReviewoftheQueenslandPoliceService,\nBrisbane.\u201dDepartmentofJustice,OfficeofCommunityOrientedPolicingServices,Washington,DC.Bingham,M.,2014\n[2] T.Hassner,Y.Itcher,andO.Kliper-Gross.\u201dViolentFlows:Real-TimeDetectionofViolentCrowdBehavior.\u201d3rdIEEEInternationalWorkshop\nThe results can be visualized from the confusion matrix [24] [26] in Figure 7. We process selective short time onSociallyIntelligentSurveillanceandMonitoring(SISM)attheIEEEConf.onComputerVisionandPatternRecognition(CVPR),Rhode\nof delay in frame sequences separately, classifying them by labelling as either violent or nonviolent, if violent sub- Island,June2012.\nsequence of frames are detected then action take place. Acceptable results were obtained when we used PC for [3] H.Keval.\u201dEffective,design,configuration,anduseofdigitalCCTV.\u201dPhDthesis,UniversityCollegeLondon,2009\n[4] N.Petrovic,N.Jojic,andT.Huang.\u201dAdaptivevideofastforward.\u201dMultimediaToolsandApplications,26(3):327\u2013344,2005.\nprocessing. Extraction of feature vector (ViF descriptor) for a sequence of 15 frames took on an average 5 seconds\n[5] Y.Pritch,S.Ratovitch,A.Hendel,andS.Peleg.\u201dClusteredsynopsisofsurveillancevideo.\u201dInAdvancedVideoandSignalBasedSurveillance,\ntobeprocessed.Hence,appropriatesamplinghadtobedonewithoutcompromisingondelayinalerts.Experiments\npages195\u2013200,2009\nshowedthatnotmorethan15continuousframesarerequiredforclassification. [6] M.Shah,O.Javed,K.Shafique.\u201dAutomatedVisualSurveillanceinRealisticScenarios.\u201dIEEEMultiMedia,Vol.14,Issue:1,2007\nWhere,TP,FP,FNandTNareTruePositive,FalsePositive,FalseNegativeandTrueNegativerespectively.The [7] O.Javed,K.Shafique,andM.Shah.\u201dAHierarchicalApproachtoRobustBackgroundSubtractionUsingColorandGradientInformation.\u201d\nmathematicalevaluationofaccuracyandsensitivityareshowninequation10and11. Proc.IEEEWorkshoponMotionandVideoComputing,IEEECSPress,2002,pp.22-27\n[8] O.Javed.\u201dTrackingacrossMultipleCameraswithDisjointViews.\u201dProc.Proc.9thIEEEInt\u2019lConf.ComputerVision,pp.343-3572003\nTP+TN [9] R.Collins,A.Lipton,T.Kanade\u201dIntroductiontotheSpecialSectiononVideoSurveillance.\u201dIEEETrans.PatternAnalysisandMachine\nAccuracy= =81.25% (10)\nTP+FP+TN+FN Intelligence,vol.22,no.8,pp.745,2000\n[10] ManoharKarki,SaikatBasu,RobertDiBiano,SupratikMukhopadhyay,JerryWeltman,MalcolmStagg.\u201dAsymbolicframeworkforrecogniz-\nTP ingactivitiesinfullmotionsurveillancevideos.\u201dComputationalIntelligence(SSCI),pp.1-7,2016.\nSensitivity= =87.5% (11) [11] N.Dalal,B.Triggs.\u201dHistogramsoforientedgradientsforhumandetection.\u201dComputerVisionandPatternRecognition,2005.CVPR2005.\nTP+FN\nIEEE\n[12] Wrenetal.\u201dPfinder,Real-TimeTrackingoftheHumanBody.\u201dIEEETrans.PatternAnalysisandMachineIntelligence,vol.19,no.7,pp.\n4.2. CommunicationComponent\n780-785,1997.\n[13] J.Rehg,M.Loughlin,andK.Waters.\u201dVisionforaSmartKiosk.\u201dComputerVisionandPatternRecognition,IEEEPress,1997,pp.690-696\n3clientAsystemsand3clientBsystemsweremadetorunsimultaneouslywithinputfromlocallystoredvideos.A [14] StaufferandW.E.L.Grimson.\u201dLearningPatternsofActivityUsingReal-TimeTracking.\u201dIEEETrans.PatternAnalysisandMachineIntelli-\nprivatenetworkwasneededforcommunication.Thesystemwastestedusingbothmobilehotspotandcollegenetwork. gence,vol.22,no.8,2000,pp.747-757\n[15] Y.RicquebourgandP.Bouthemy.\u201dReal-TimeTrackingofMovingPersonsbyExploitingSpatiotemporalImageSlices.\u201dIEEETrans.Pattern\nNofaults/datalossincommunicationsystemwereobservedduringtesting.Therewerenoerrorsoratypicalbehavior\nAnalysisandMachineIntelligence,vol.22,no.8,2000,pp.797-808\ndetected in the communication component. The table 3 illustrates the average delay between time of occurence of\n[16] PushkarP.Goswami,DiwakarPaswan,DushyantKumarSingh.\u201dDetectingmovingobjectsintrafficsurveillancevideo.\u201dInternationalJournal\nactivity and raising of alert at client B. There was a maximum lag of 3 seconds between detection of activity at ofControlTheoryandapplications,Vol9.17,pp8423-8430(2016)\nsurveillancesiteandmessagealertatcontrolroom. [17] PushkarProtikGoswami,DushyantKumarSingh.\u201dAhybridapproachforreal-timeobjectdetectionandtrackingtocoverbackgroundturbu-\nlenceproblem.\u201dIndianJournalofScienceandTechnology,Vol9.45(2016)\n[18] Abdelkaderetal.\u201dSilhouette-basedgestureandactionrecognitionviamodelingtrajectoriesonRiemannianshapemanifolds.\u201dComputerVision\nTable3.Averagedelay(inseconds) andImageUnderstanding,115.3(2011):439-455\n[19] O.Kliper-Gross,T.Hassner,andL.Wolf.\u201dPfinder,Real-TimeTrackingoftheHumanBody.\u201dIEEETrans.PatternAnalysisandMachine\nDetection RaspberryPi PC\nIntelligence,pages31\u201345,2011\nPedestrianDetection 4.1sec 2.6sec [20] V.Kellokumpu,G.Zhao,andM.Pietikainen.\u201dHumanactivityrecognitionusingadynamictexturebasedmethod.\u201dInBMVC,pages1\u201310,\nViolentCrowdDetection 5.1sec 4.3sec 2008\n[21] O.Kliper-Gross,T.Hassner,andLiu.Beyond.\u201dPixels:ExploringNewRepresentationsandApplicationsforMotionAnalysis.\u201dPhDthesis,\nMassachusettsInstituteofTechnology,May2009\n[22] NikhilSingh,ShambhuShankarBharti,RupalSingh,DushyantKumarSingh.\u201dRemotelycontrolledhomeautomationsystem.\u201dInternational\nConferenceonAdvancesinEngineering&TechnologyResearch(ICAETR-2014),IEEE2014\n[23] Agarwal,Anshuman,ShivamGupta,andDushyantKumarSingh.\u201dReviewofopticalflowtechniqueformovingobjectdetection.\u201d2ndInter-\n5. CONCLUSIONANDFUTUREVISION\nnationalConferenceonContemporaryComputingandInformatics(IC3I),pages1\u201310,2016\n[24] KaelonLloydetal.\u201dDetectingViolentandAbnormalCrowdactivityusingTemporalAnalysisofGreyLevelCo-occurrenceMatrix(GLCM)\nDespiterecentadvancementintechnologyofcomputervisionandrelatedareas,therearestillsomemajorissues BasedTextureMeasures.\u201dComputerVisionandPatternRecognition,2017\nthatneedstobeovercomeformakingcompleterealtimeoperatingandreliableautomatedsurveillancesystem.These [25] Violent-Flows-CrowdViolence Non-violenceDatabaseandbenchmark-2017,www.cslab.openu.ac.il/download\nissuesincludetechnicalaspectasphysicalplacementtofullcoverage,installation,maintenanceofcamera,network [26] M.A.AnsariandM.Dixit.\u201dAnenhancedCBIRusingHSVquantization,discretewavelettransformandedgehistogramdescriptor.\u201d2017\nInternationalConferenceonComputing,CommunicationandAutomation(ICCCA),GreaterNoida,2017,pp.1136-1141.\nbandwidthrequiredtosupportcameraandingeneralaspectasrobustnessofcameraintypicalweatherandlightening\nconditions,installationcost,privacyconcernetc.However,demandofautomatedsurveillancesystemisbeingmade\nmore frequently specially in public safety and home security with respect to quality control parameters. Military\nintelligencecanalsobeoneoftheapplicationareaastechnologicalgrowthisalsoreachingtoheights.Thispaperis\nprovidingagoodresultforcrowdviolencedetectionintermsofaccuracyandsensitivity.Theprocessingtimeisalso",
    "metadata": {
      "AuthoritativeDomain[1]": "sciencedirect.com",
      "AuthoritativeDomain[2]": "elsevier.com",
      "CreationDate": "D:20200525160524+05'30'",
      "CrossMarkDomains[1]": "sciencedirect.com",
      "CrossMarkDomains[2]": "elsevier.com",
      "CrossmarkDomainExclusive": "2010-04-23",
      "CrossmarkMajorVersionDate": "2010-04-23",
      "ElsevierWebPDFSpecifications": "6.5",
      "ModDate": "D:20200530100214+05'30'",
      "Producer": "Adobe PDF Library 15.0",
      "doi": "10.1016/j.procs.2020.04.036",
      "robots": "noindex"
    },
    "error_message": null,
    "pages": [
      {
        "page_number": 0,
        "text_content": "Availableonlineatwww.sciencedirect.com\nAvailableonlineatwww.sciencedirect.com\nAvailableonlineatwww.sciencedirect.com\nAvailable online at www.sciencedirect.com\n2 DushyantKumarSingh,SumitParoothi,MayankkumarRusiaetal./ProcediaComputerScience00(2019)000\u2013000\nProc S edi c aC ie om n pu c ter e Sc D ien i c r e e 00 c (2 t 019)000\u2013000 in the society. Currently, video cameras are used for such kind of surveillance and they can be deployed in several\nProcediaComputerScience00(2019)000\u2013000 www.elsevier.com/locate/procedia placeseasilysuchascompanies,railwaystations,shoppingcenters,bankofficesandATMmachinesetc.\nProcPerdoicae dCioamCpoumtepru StecrieSncciee n1c7e10 (020(22001) 93)5000\u201303\u201350900 www.elsevier.com/locate/procedia Somedecadesago,videosurveillancesystemwerenotsomuchprevalentasoftoday.Variouschangesinthisperiod\nwww.elsevier.com/locate/procedia\nhaveresultedindeploymentandgrowthofvideosurveillancesystem.Humansocialdysfunctionischangingandtheir\nThirdInternationalConferenceonComputingandNetworkCommunications(CoCoNet\u201919)\nThirdInternationalConferenceonComputingandNetworkCommunications(CoCoNet\u201919) approach to security is becoming more sensible. In present time, everyone is getting familiar with technological\nThirdInteHrnuatmionaanlCConrfoewrendceDoneCteocmtpiuotningfoanrdCNeittwyoWrkCidomemSuunricvaetioilnlsa(nCcoCeoNet\u201919) platformtohandledifferentsecuritytasks.CCTVusageisgettingmorepopularwiththepriceaffordabilityandless\nHuman Crowd Detection for City Wide Surveillance effortfortheirsetup.Thesesurveillancesystemarefruitlesstoprovidesecurity,untilinadequatecapacityoftrained\nHuman Crowd Detection for City Wide Surveillance\npeoplewiththeirhighattentioncompetencearefulfilledforwatchingtheflicks[1][2].AccordingtoH.Keval[3]and\nDushyant Kumar Singha, , Sumit Paroothib, Mayank Kumar Rusiac, Mohd. Aquib Ansarid\nDushyant Kumar Singha, \u2217 , Sumit Paroothib, Mayank Kumar Rusiac, Mohd. Aquib Ansarid N.Petrovic[4],Monitoringsystemconsistsoflargenumberofcamerasthathelpstomonitordifferentsites.Itisused\nDushyant KumaarDSepianrtgmehn a t , o \u2217 \u2217f,CSomupmuteirtScPieanrceo&otEhngii b n,eeMringa,yMaNnNIkTAKllauhmabaadr,PRrauyasgiraaj, c U,.MP.INoDhIdA. Aquib Ansarid todetectanynoticeableandunwantedorillegalactivity,soastoimmediatelyrespondtothesituationwithveryshort\naDepartmentofCompubteMroSrcgieanncSeta&nlEeny,gBineenegrliunrgu,,MKNaNrnIaTtaAklala,hINabDaIdA,Prayagraj,U.P.INDIA delay.Sometimes,searchingspecificactivityfromthelargeamountofrecordedvideosfilesseemsverydifficultand\nacDDeeppaarrttmmeennttooffCCoommppuubtteeMrroSSrccgiieeannnccSeeta&&nlEEenny,ggBiinneeeneegrrliiunnrggu,,,MMKNNaNNrnIIaTTtaAAkllallaa,hhINaabbDaaIddA,,PPrraayyaaggrraajj,,UU..PP..IINNDDIIAA\ntimeconsumingprocessforexaminingtheoccurredevent.Hence,itrequirescomputervisionsystem.\ndcDDeeppaarrttmmeennttooffCCoommppuu b tteeMrroSSrccgiieeannnccSeeta&&nlEEenny,ggBiinneeeneegrrliiunnrggu,,,MMKNNaNNrnIIaTTtaAAkllallaa,hhINaabbDaaIddA,,PPrraayyaaggrraajj,,UU..PP..IINNDDIIAA\ndcDDeeppaarrttmmeennttooffCCoommppuutteerrSScciieennccee&&EEnnggiinneeeerriinngg,,MMNNNNIITTAAllllaahhaabbaadd,,PPrraayyaaggrraajj,,UU..PP..IINNDDIIAA Thesystemproposedhereaimstoovercomelimitationsoftraditionalsurveillancetechniques.Thissystemworks\ndDepartmentofComputerScience&Engineering,MNNITAllahabad,Prayagraj,U.P.INDIA inrealtimetodetecttheunusualbehaviourofpeoplesinpublic,forexampleviolentcrowdbehaviour,andpedestrian\ndetection in restricted areas. In this paper, object detection approaches are utilized to detect human pedestrians and\nAbstract crowd behavior analysis is modeled using violent flow descriptor with SVM classification. Next part of the system\nAbstract isrealtimeinformationdisseminationtopoliceauthoritiesatdifferentlevels,i.e.localpolicestation&policehead-\nSAubrsvteriallcatnce systems are most commonly used for monitoring/surveillance of almost all public and private places. Real time\nbSeuhrvaveiiloluanrcoefstyhsetseemssysatreemmsoasdtdcoexmtrmaocnalpyaucisteydtofotrhmeosunritvoeriilnlagn/scuervaecitlilvaintyc.eIonfsaulcmhocsatsaelsl,paunbyliscusapnidciporuisvaoter upnlaecveesn. aRcetaivlittyimies quarter.Thiscomponenthelpsearlyresolutionofanysevereimpactofcrowdviolenceinpublic.Systemasawhole\ndbS eeu thr e vacve t iie lod lua b nry coe afn sta yhl sey tss eei m nsg sytsa htree emr m esa o las tdt idm c e oexm vtirm dae o ocnas lpy trae ucais mteyd otof fo thtrhe mec o asun mrit evo rer aii slnlat g hn / r sco ueu rv gae hcitl tilvh aie ntsyc e.e sIy ons f tse aum lcmh. o Acs tatsm ael asl,x p iam unbyu l m iscupsa lpn aic dceis po, rut isv h a eot s eruup rvnla eec ivl eel s an. ncaR eceta aivc litti t vyim itiy es contributestotheintelligent/smartpolicingforeffectivelawenforcement[10].\nidb seeth meaco vtr iee odu mbrya o naf una tah ll, eys is. eien. sgr y etsha t l eem triem saelatd vidmidee e x ovtir fda oeooc t a asp gtra eecao imt f y CotofCtThth Veecias s umm rev orean isl i l t atohn rrc eoe duga bhc y tti s vhe iec tsy ue. rsi I tyn ysts peu emc rs h.oAn cta a smls eas i ,xniams n o yumms e upc slo pan icc tei rso o,u ltshroeo osr muur.vn Iee tivli elsanndci affiectac ivcu itl titvy fiotiys r This paper is outlined in five sections. Despite introduction in first section, the related works corresponding to\naidsn e y tmec hot ureem dma b n yanb aun eai a nll,y gis.t ieo n.grmethaole nitr tie oma rel C tvi C midT eeVovifd coo eoo nttas ingtr uee o aou mfsCl o y fCa tTh nVe d c tia isr m eml e eor s ans s liyttoh wrreo idt u h gbo hyu t tshea ecn suy ersirty eys s t tp. eem Trsh .oe Anre taf mlosr aie xn, imsroou bmm ues p tcn loa enc sstersoa ,ln th droe eosffmu e r.c vIt e itv ilie lsa n nde cisffis eaco cuf tls itv ufi cot h yr differentmonitoringsystemaredelineatedinsection2.Section3consistsofproposedmethodologyinwhichactivity\nsais unrym veho iur l e lma m nacna e nbu dea einl p ,ge i n .te do. s rme o aon lntit etiom crh e nCv oCi l d oTe gVo ic f aco loons t tta ring enue goo tuf hsCloyC faT sne Vdcutiir sriet m yleo psnseli ryt s o owr n eia dtlhs bo, yu(ti s . e eac .nuyh r u irt meysatp.n eTr b she oei nrnea gfl )os ,ri aen n,d sroo hbm eue nstc cne oen psst o r s oae lndr t o heo effm ne. ece Itit dvieo snf dem is ffi so cor u eflst mufca ohn r detection and communication sub system are briefly described. Section four comprises the comparative analysis of\npsaun ory wveeh riullm faona rcn reo b ude nein dpgetnh tdo esm colo onc n ktietocs r uhrnC voC elioT llgV ainccac elo . nsTttrin henu isgo ptuha sl poy efr asn ded icsuc tiru rie stys le eps s sea lryscow onm iatlh pso l,e u(t tie.ea a.n u yhtour nme o sat m .nT obu hes eirns ego f)l o u,r tae ion, ndro thb heu ans t tcn wee ips ll sonsaeo n t dtho een ff lny eec ret e idv d e uon cfe ems t s hoo eref e smffu o car hnt experimentalresults.Conclusionwithanyfuturescopearedeliberatedinofsection5.\nopsu forwi v nee dril iv lfaoi n drc ure aolud sne bdp u ett nhw des icl ollon aclkt s e o csuh inrnvc oer l eiolalgas i encca tel h.e sTtr eheffnise g cpt t haivpo eef nre sde sis csuco ruf isty tshee pse lar a s wcoonme a n lps fl, oer (tc ie. e eam .uh etou nnm toa amu n toh buo esr i i nsty go)lui , ntaion pnd attr hho eal n lti c nwe gipltlo hns e eoptt u hob enlli nyc ere ped ldauo cce fes mtoho fere aemffciot a yrnt.\nTopfo h w iisneds r yivf s oitdr eum raolu iss nbd pur t oth pwe oic sle llo daclt ksoos u uisnr e vcertei h lae laseen x cti ehs .eti T neh gffiseCcp CtaiTvpeV enrei d nsisf s r coa u sfs trtshu eec s tulaarwc e o o mefnp tfh loe ertce cei amty ue. tonV nta oar m iuoto uhu sosrcio styo m luipnt u io tpe narttrv hoi alslt iionw ngil ttl ehc neh o npt iuo qbn ull eiycs r ape rld eauce cem esp toh lfo e yae e ff cdiottyr o t.\ndTo e fhtii esn c dstyi t v hsite deums a u l sisspi bpc uriot opu wos isl ael cda ti l vtsoi o tyuisn fer c o rtehm aes ree exa til hse ttiimneg ff e eCv cCi t d iTv e e oVn s eiens q sfuroaesf ntcrthue e sc.tul A arwenoi e nfn ttr fhi o cer a cct e eimtyc e.o nVm tam ariu uot nuh iso c r aci tot i ymon ipn suytpes art t e rvm oilslii ios nng alts teh ocehdnp eis uqi b gul neic esdaprl tea o ced eme s cpo rle foaya see cdi t t htyoe .\n2. RelatedWorks\nrdT eeh sti pes ocstn ytshs e tee ti msmuse ispoipcf ris ooy pus oste sae mcdtifvt o oirtyu th sfe erop thmo e lire cex eaila stu tii tmn h g oer Cvit Ciide Teso. V Tsi henqi f sura sey snt scr t ueesc m.tuAi r s enc oianf pta trhibce lae ctet i o tycg .oe Vmn ame r r iua ont u eiscaac ntoia molna p rsu mytess rtieg vmn is a iil osi naflt tse hoc e h rde neisf qiogu une nes dda s rta eon edy mescu prs leo pay isc eei d othu toes\nfrd eee lst lpe ooc w tntso her etuismn u u ses pou icfa i lsoya usc sttei a vmc i t t i yfvoi irt n ytht f her e opm roelsirct e rea ic lat tue itd mhsoe irt vieti .ideI est o.aTl s she o qisu gse eyn nsc ete rea sm. te Aissn tch iae npt car o ibc mlaet p etloe c tgo eem dnm eesruac ntrei i p cat ani t o iao nlnarre sml y a s tst eie dgmntoails tih afe ltshe oevrd eee n sfts ioguh nna edp dsp t eao nnd iyn esg cus rse ipa niscf eioot r hum es\nofree flslt poe o xwn tu soe arl tui w mnaue rsn oui fanls g yam sctte eivs m sitayf g o eirns t . htheeproelsictreicatuetdhsoirtiet.ieIst.aTlshoisgseynsetreamteisstchaepcaobmleptloetgeedneesractreipatnioanlarremlatseidgntoaltihfetheevreenftsouhnadpspeanniynsgussipnicfioorums Thisreviewconcentrateonmethodologiesfordetectingdifferentobjectsofinterestlikehuman,vehicleortanketc.\nofeflltoexwtuoarluwnaursnuianlgamcteivssitaygeins.therestrictedsite.Italsogeneratesthecompletedescriptionrelatedtotheeventshappeningsinform [5].Surveillanceisaverydemandingtopicnowadaysbecauseithelpstheobjectthatcanbetrackedoverlongperiods\no c ft 2 e 0 x 2 tu 0 a T l h w e a A rn u in th g o m rs e . s P s u a b g l e i s s . hedbyElsevierB.V. of time under different circumstances. There are many difficulties to detect and track the object within surveillance\n\u00a9 T(cid:30) (cid:30) T P (cid:30) Tc c e h hh e 2i i2 2 i s ssr 0 0 0 - 2i iir 2 2s sse 0 0 0 v a aa T i T Tn nne h h h w o oo e e ep pp A u e ee A An nnn u u u d ta aa t t h e h hc cc o r o oc cc re er r er s s s e s ss . . . s sss P P P p a aa u o u ur rr b n t tb b t i ii l s l lc cc i i i i sl ll s s b e ee h h h i e l u ue e ui d d dn ntny d dd b b be ee y o y yr rr f E E Et tt h hth l l l h se es s ee e e e C C v C v v s i i iC CCc e e e i r r r e B B B B n B BY YYt . . . i V- -V V -fi N NN . . . c C CCc - --o N NNm D DDm l lli i iit c cct e eee n nen s sso e eef ( (( h hhth t tt t tte p pp : ::T / / / / / /h c cci r rrr e eed a aa t ttI i ii v vnv e etee c ccr o oon m mmat m mimo o ono n nan s sls . .. o oCo r rro g ggn / / / l lfl i iie c ccr e eee n nnn s ssc e eee s ss / / /o b bbn y yy - -- n nCn c coc - --m n nn d dpd / /u/ 4 44t . .i. 0 0n0 / /g/ ) ))and Network v S i o d , e t o he d s u e e m to o d d i e ff ls er a e r n e t m ill o u s m tly in u at s i e o d n i c n on th d a i t ti p o l n a , c a e c s ro w s h s e a re sc c a l t e t a e r re d d et a e n c d tio ch n a o n f gi e n n g un b c a i c a k t g e r h o u u m nd a , n ex b i o s d te y n o c r e a o n f y ob o s th cu er rit o y bj e e t c c t .\nP T CPeeh oeei m s rr--m i rrs eeuvva niin eeicwwo a p tuu e io nnnndda seec rr( c C erreoe ssssCpp a ooor Nnnti ssc eiil tbbe \u2019i1illu 9iit nt)yy. d e oor ff thtthh e eeC ss Ccciiee Bnn Yttii- fifiN ccC cc -oo Nmm Dmm l iii ttc tte een ee soo eff ( htthh ttee p :TT // hh cir irr edd a tIi I vnn ettee crr onn maatt miioo onn n aas ll .oCC roo gnn /lff iee c rre een nns cce ees / ob ony n -nCC coo -mm ndpp / uu 4tt .ii 0nn / gg) aanndd NNeettwwoorrkk canbedoneonthebasisoflength,widthandheight[6].Heterogeneoustechniqueshavebeendefendedfortracking,\nC PCe ooe mmr- mmre uuv nnie iiccw aatti uioon nnd sse ( r(CCr o eoC sCp ooo NNn e set it\u2019 b\u201911i 9 l9i ) t). y. of the scientific committee of the Third International Conference on Computing and Network detectionandidentificationofobjectsinvideos[9][22].\nCKeoymwmorudnsi:caItnitoenrnse(tCoofCThoiNngest;\u20191C9o)m.puterVision;AutomatedVideoSurveillance;CCTVmonitoring;Activitydetection;Humandetection;Edge\nAnautomatedsurveillancesystem,knownasknight[1],isasystemthatisusedforvideosurveillanceandmonitor-\nKcoemywpuotrindgs.: InternetofThings;ComputerVision;AutomatedVideoSurveillance;CCTVmonitoring;Activitydetection;Humandetection;Edge\nKcoemywpuotrindgs.: InternetofThings;ComputerVision;AutomatedVideoSurveillance;CCTVmonitoring;Activitydetection;Humandetection;Edge ingthroughdifferentCCTV,whichconsistsself-operativemode.Itcandetectaswellasclassifythetargetsefficiently\ncomputing. bycoherentlytrackingtheobjectthroughdifferentcamerasusingstate-of-artcomputervisiontechniques.Itproduce\ndetailedtextualdescriptivesummaryinformationintheorbitformwithgooglemaptrackingsitelocation.Thissum-\nmary will gives direction to police officer in analysis and quick response decision. The limitations of surveillance\n1. Introduction systemincludetheinabilitytodetectobjectwhichismasked,identifyingobjectsamongcrowd,managingcrowdand\n1. Introduction workinginunfavorableweatherconditionamongtheearlierautomatedmonitoringsystem.Backgroundsubtraction\n1. Introduction\nVideosurveillanceisthefacilitytoobserveandanalyzeanyparticularsiteforidentifyingsuspiciousactivitywith [7][16]canalsobeoneofthesolutionfordifferentproblemsasliketrafficcontrolling,visualinspectionandinter-\nrespVeicdteotossuarfveetiyllaanndceseiscuthrietyfapcuilriptyosteos.obVsiedrevoesaunrdveainllaalnyczeesaynsytepmarctiacmulearinstioteefxoirstiednecnetiffoyrinsgecsuursiptyicaionudscaricmtiveictyonwtritohl actionbetweencomputerandhumanbymovingobjectdetection[19].Intheseapplicationswehavesomeobjectof\nVideosurveillanceisthefacilitytoobserveandanalyzeanyparticularsiteforidentifyingsuspiciousactivitywith\nrespecttosafetyandsecuritypurposes.Videosurveillancesystemcameintoexistenceforsecurityandcrimecontrol interestfordetectionandthatcanbetrackedfortheiractivity.\nrespecttosafetyandsecuritypurposes.Videosurveillancesystemcameintoexistenceforsecurityandcrimecontrol\nOpticalflowtechniqueisusefultoidentifythevelocitiesofmovingpointsinanimage.Opticalflowworkswhen\n\u2217 Correspondingauthor.Tel.:+91-7905514182;fax:+0-000-000-0000. background is static and foreground object is in motion. Therefore, optical flow delivers important information of\n\u2217 CE-omrraeislpaodnddriensgs:aduuthsohry.aTnetl@.:m+9n1n-i7t.9ac0.5in514182;fax:+0-000-000-0000. objectmomentumw.r.t.time[17].Hassneretal.[2]hasproposedawellknownalgorithmforviolencecrowddetec-\n\u2217\nCE-omrraeislpaodnddriensgs:aduuthsohry.aTnetl@.:m+9n1n-i7t.9ac0.5in514182;fax:+0-000-000-0000.\ntion.Here,individualandstraightfullpostureofthebodyarecoveredinthescene.Unimodalbackgroundmodelis\n1877E--0m5a0i9lacdd2r0e2ss0:TdhueshAyuantht@orsm.nPnuibt.laisch.iendbyElsevierB.V.\n(cid:30) responsibletotrackmovingpersoninthescene.\nT PT 1 1 1 T 8 8 8 e h hhe 7 7 7i ii s sr 7 7 7 s- - - -i iri s s 0 0 0 es 5 5v 5a a a 0 0i 0n nen 9 9 9 w o o (cid:30) (cid:30) p p \u00a9 o c cu e ep n n n 2 2 2 ed 0 0 0 n a ae 2 2 2 c cra 0 0 0 c ccr e e T T T ec s sse s s h h h ps e e e a aos r r A A An t ta i is u c c u u ri l l t t t tb e e h h h iic o o ol u uil r r rte n n s s sy d d . . . u e eo P P P n r rf u u u d t t b b bt h hhe l l l e er i i ie s s s C C h h h tshc e e e C Ci d d d ee B Bn b b b Ct Y Y y y yiC fi - - E E E N N cB l l l s C C s sc e e e Yo - - v v v N Nm- i i i N e e e D Dm r r r C B B B i l lt- i it c c . . N . e V V V e eeD . . n n . s so e elfic ( (t h hhe t tne t t p psTe : : / /h / /( c cihr r rd e et a atIp t tn i i v v:t/e e e/r c cnc o oar m mteiao m mtn o oiav n nle s sCc . . o ooo r rmn g gf / /m l le i ir c coe e enn n ncs s s.e e eo s sro / /gn b b/ y yCl - -i n nco c cem - -n n nps d due / /st 4 4i/n . .b 0 0g / /y ) )a-nndc-Nnde/tw4o.0r/k)Communications J.Rehgetal.[13]proposedatechniquetodetectandtrackofmovingpersonwhoiswalkinginprohibitedareaof\n(PT PCeh eoei e srCr-- iros reNeva vieneitew\u2019o1w p9ue )nn u.dn aec drc ere res ss rpe aor snt p isc oilbn eisluiitn byd il eoi r tfy tth he oef Cs cC thiee Bn tY sifi- c Ncie Ccno- t Nmifi Dmc ilti ctc oeeem nsom ef( ith tht tee tp eT: /ho /cifrr de t a hItne ivt ee Trcnh oam itridom no Ian nls tCe .o ror nng af/let i irc oee nnn acsel e soC /nb o yCn -n foe cmr -n epn du/ ct4ien.0 go /)ann dCNometwpuotrikngC oamnmd uNniectawtioonrks kioskbyusingfacedetection,skincolorandstereo.StaufferandGrimson[14]proposedaBackgroundSubtraction\n(P CCe ooe m rC-rom eNv u ien etw\u2019i1ca9u t)n i.o de n r s r ( e C sp o o C n o si N bi e li t t \u2019 y 19 o ) f . thescientificcommitteeoftheThirdInternationalConferenceonComputingandNetworkCommunications modelthathascapabilitytoprovidegoodresultswhilechangeshappensduetoillumination,repeateddisorderfrom\n(1C0o.1C0o1N6/ejt.\u2019p19ro).cs.2020.04.036\n10.1016/j.procs.2020.04.036 1877-0509",
        "tables": [],
        "forms": {},
        "confidence_score": 1.0
      },
      {
        "page_number": 1,
        "text_content": "Availableonlineatwww.sciencedirect.com\nAvailableonlineatwww.sciencedirect.com\nAvailableonlineatwww.sciencedirect.com\nDushyant Kumar Singh et al. / Procedia Computer Science 171 (2020) 350\u2013359 351\n2 DushyantKumarSingh,SumitParoothi,MayankkumarRusiaetal./ProcediaComputerScience00(2019)000\u2013000\nProcediaComputerScience00(2019)000\u2013000 in the society. Currently, video cameras are used for such kind of surveillance and they can be deployed in several\nProcediaComputerScience00(2019)000\u2013000 www.elsevier.com/locate/procedia placeseasilysuchascompanies,railwaystations,shoppingcenters,bankofficesandATMmachinesetc.\nProcediaComputerScience00(2019)000\u2013000 www.elsevier.com/locate/procedia Somedecadesago,videosurveillancesystemwerenotsomuchprevalentasoftoday.Variouschangesinthisperiod\nwww.elsevier.com/locate/procedia\nhaveresultedindeploymentandgrowthofvideosurveillancesystem.Humansocialdysfunctionischangingandtheir\nThirdInternationalConferenceonComputingandNetworkCommunications(CoCoNet\u201919)\nThirdInternationalConferenceonComputingandNetworkCommunications(CoCoNet\u201919) approach to security is becoming more sensible. In present time, everyone is getting familiar with technological\nThirdInteHrnuatmionaanlCConrfoewrendceDoneCteocmtpiuotningfoanrdCNeittwyoWrkCidomemSuunricvaetioilnlsa(nCcoCeoNet\u201919) platformtohandledifferentsecuritytasks.CCTVusageisgettingmorepopularwiththepriceaffordabilityandless\nHuman Crowd Detection for City Wide Surveillance effortfortheirsetup.Thesesurveillancesystemarefruitlesstoprovidesecurity,untilinadequatecapacityoftrained\nHuman Crowd Detection for City Wide Surveillance\npeoplewiththeirhighattentioncompetencearefulfilledforwatchingtheflicks[1][2].AccordingtoH.Keval[3]and\nDushyant Kumar Singha, , Sumit Paroothib, Mayank Kumar Rusiac, Mohd. Aquib Ansarid\nDushyant Kumar Singha, \u2217 , Sumit Paroothib, Mayank Kumar Rusiac, Mohd. Aquib Ansarid N.Petrovic[4],Monitoringsystemconsistsoflargenumberofcamerasthathelpstomonitordifferentsites.Itisused\nDushyant KumaarDSepianrtgmehn a t , o \u2217 \u2217f,CSomupmuteirtScPieanrceo&otEhngii b n,eeMringa,yMaNnNIkTAKllauhmabaadr,PRrauyasgiraaj, c U,.MP.INoDhIdA. Aquib Ansarid todetectanynoticeableandunwantedorillegalactivity,soastoimmediatelyrespondtothesituationwithveryshort\naDepartmentofCompubteMroSrcgieanncSeta&nlEeny,gBineenegrliunrgu,,MKNaNrnIaTtaAklala,hINabDaIdA,Prayagraj,U.P.INDIA delay.Sometimes,searchingspecificactivityfromthelargeamountofrecordedvideosfilesseemsverydifficultand\nacDDeeppaarrttmmeennttooffCCoommppuubtteeMrroSSrccgiieeannnccSeeta&&nlEEenny,ggBiinneeeneegrrliiunnrggu,,,MMKNNaNNrnIIaTTtaAAkllallaa,hhINaabbDaaIddA,,PPrraayyaaggrraajj,,UU..PP..IINNDDIIAA\ntimeconsumingprocessforexaminingtheoccurredevent.Hence,itrequirescomputervisionsystem.\ndcDDeeppaarrttmmeennttooffCCoommppuu b tteeMrroSSrccgiieeannnccSeeta&&nlEEenny,ggBiinneeeneegrrliiunnrggu,,,MMKNNaNNrnIIaTTtaAAkllallaa,hhINaabbDaaIddA,,PPrraayyaaggrraajj,,UU..PP..IINNDDIIAA\ndcDDeeppaarrttmmeennttooffCCoommppuutteerrSScciieennccee&&EEnnggiinneeeerriinngg,,MMNNNNIITTAAllllaahhaabbaadd,,PPrraayyaaggrraajj,,UU..PP..IINNDDIIAA Thesystemproposedhereaimstoovercomelimitationsoftraditionalsurveillancetechniques.Thissystemworks\ndDepartmentofComputerScience&Engineering,MNNITAllahabad,Prayagraj,U.P.INDIA inrealtimetodetecttheunusualbehaviourofpeoplesinpublic,forexampleviolentcrowdbehaviour,andpedestrian\ndetection in restricted areas. In this paper, object detection approaches are utilized to detect human pedestrians and\nAbstract crowd behavior analysis is modeled using violent flow descriptor with SVM classification. Next part of the system\nAbstract isrealtimeinformationdisseminationtopoliceauthoritiesatdifferentlevels,i.e.localpolicestation&policehead-\nSAubrsvteriallcatnce systems are most commonly used for monitoring/surveillance of almost all public and private places. Real time\nbSeuhrvaveiiloluanrcoefstyhsetseemssysatreemmsoasdtdcoexmtrmaocnalpyaucisteydtofotrhmeosunritvoeriilnlagn/scuervaecitlilvaintyc.eIonfsaulcmhocsatsaelsl,paunbyliscusapnidciporuisvaoter upnlaecveesn. aRcetaivlittyimies quarter.Thiscomponenthelpsearlyresolutionofanysevereimpactofcrowdviolenceinpublic.Systemasawhole\ndbS eeu thr e vacve t iie lod lua b nry coe afn sta yhl sey tss eei m nsg sytsa htree emr m esa o las tdt idm c e oexm vtirm dae o ocnas lpy trae ucais mteyd otof fo thtrhe mec o asun mrit evo rer aii slnlat g hn / r sco ueu rv gae hcitl tilvh aie ntsyc e.e sIy ons f tse aum lcmh. o Acs tatsm ael asl,x p iam unbyu l m iscupsa lpn aic dceis po, rut isv h a eot s eruup rvnla eec ivl eel s an. ncaR eceta aivc litti t vyim itiy es contributestotheintelligent/smartpolicingforeffectivelawenforcement[10].\nidb seeth meaco vtr iee odu mbrya o naf una tah ll, eys is. eien. sgr y etsha t l eem triem saelatd vidmidee e x ovtir fda oeooc t a asp gtra eecao imt f y CotofCtThth Veecias s umm rev orean isl i l t atohn rrc eoe duga bhc y tti s vhe iec tsy ue. rsi I tyn ysts peu emc rs h.oAn cta a smls eas i ,xniams n o yumms e upc slo pan icc tei rso o,u ltshroeo osr muur.vn Iee tivli elsanndci affiectac ivcu itl titvy fiotiys r This paper is outlined in five sections. Despite introduction in first section, the related works corresponding to\naidsn e y tmec hot ureem dma b n yanb aun eai a nll,y gis.t ieo n.grmethaole nitr tie oma rel C tvi C midT eeVovifd coo eoo nttas ingtr uee o aou mfsCl o y fCa tTh nVe d c tia isr m eml e eor s ans s liyttoh wrreo idt u h gbo hyu t tshea ecn suy ersirty eys s t tp. eem Trsh .oe Anre taf mlosr aie xn, imsroou bmm ues p tcn loa enc sstersoa ,ln th droe eosffmu e r.c vIt e itv ilie lsa n nde cisffis eaco cuf tls itv ufi cot h yr differentmonitoringsystemaredelineatedinsection2.Section3consistsofproposedmethodologyinwhichactivity\nsais unrym veho iur l e lma m nacna e nbu dea einl p ,ge i n .te do. s rme o aon lntit etiom crh e nCv oCi l d oTe gVo ic f aco loons t tta ring enue goo tuf hsCloyC faT sne Vdcutiir sriet m yleo psnseli ryt s o owr n eia dtlhs bo, yu(ti s . e eac .nuyh r u irt meysatp.n eTr b she oei nrnea gfl )os ,ri aen n,d sroo hbm eue nstc cne oen psst o r s oae lndr t o heo effm ne. ece Itit dvieo snf dem is ffi so cor u eflst mufca ohn r detection and communication sub system are briefly described. Section four comprises the comparative analysis of\npsaun ory wveeh riullm faona rcn reo b ude nein dpgetnh tdo esm colo onc n ktietocs r uhrnC voC elioT llgV ainccac elo . nsTttrin henu isgo ptuha sl poy efr asn ded icsuc tiru rie stys le eps s sea lryscow onm iatlh pso l,e u(t tie.ea a.n u yhtour nme o sat m .nT obu hes eirns ego f)l o u,r tae ion, ndro thb heu ans t tcn wee ips ll sonsaeo n t dtho een ff lny eec ret e idv d e uon cfe ems t s hoo eref e smffu o car hnt experimentalresults.Conclusionwithanyfuturescopearedeliberatedinofsection5.\nopsu forwi v nee dril iv lfaoi n drc ure aolud sne bdp u ett nhw des icl ollon aclkt s e o csuh inrnvc oer l eiolalgas i encca tel h.e sTtr eheffnise g cpt t haivpo eef nre sde sis csuco ruf isty tshee pse lar a s wcoonme a n lps fl, oer (tc ie. e eam .uh etou nnm toa amu n toh buo esr i i nsty go)lui , ntaion pnd attr hho eal n lti c nwe gipltlo hns e eoptt u hob enlli nyc ere ped ldauo cce fes mtoho fere aemffciot a yrnt.\nTopfo h w iisneds r yivf s oitdr eum raolu iss nbd pur t oth pwe oic sle llo daclt ksoos u uisnr e vcertei h lae laseen x cti ehs .eti T neh gffiseCcp CtaiTvpeV enrei d nsisf s r coa u sfs trtshu eec s tulaarwc e o o mefnp tfh loe ertce cei amty ue. tonV nta oar m iuoto uhu sosrcio styo m luipnt u io tpe narttrv hoi alslt iionw ngil ttl ehc neh o npt iuo qbn ull eiycs r ape rld eauce cem esp toh lfo e yae e ff cdiottyr o t.\ndTo e fhtii esn c dstyi t v hsite deums a u l sisspi bpc uriot opu wos isl ael cda ti l vtsoi o tyuisn fer c o rtehm aes ree exa til hse ttiimneg ff e eCv cCi t d iTv e e oVn s eiens q sfuroaesf ntcrthue e sc.tul A arwenoi e nfn ttr fhi o cer a cct e eimtyc e.o nVm tam ariu uot nuh iso c r aci tot i ymon ipn suytpes art t e rvm oilslii ios nng alts teh ocehdnp eis uqi b gul neic esdaprl tea o ced eme s cpo rle foaya see cdi t t htyoe .\n2. RelatedWorks\nrdT eeh sti pes ocstn ytshs e tee ti msmuse ispoipcf ris ooy pus oste sae mcdtifvt o oirtyu th sfe erop thmo e lire cex eaila stu tii tmn h g oer Cvit Ciide Teso. V Tsi henqi f sura sey snt scr t ueesc m.tuAi r s enc oianf pta trhibce lae ctet i o tycg .oe Vmn ame r r iua ont u eiscaac ntoia molna p rsu mytess rtieg vmn is a iil osi naflt tse hoc e h rde neisf qiogu une nes dda s rta eon edy mescu prs leo pay isc eei d othu toes\nfrd eee lst lpe ooc w tntso her etuismn u u ses pou icfa i lsoya usc sttei a vmc i t t i yfvoi irt n ytht f her e opm roelsirct e rea ic lat tue itd mhsoe irt vieti .ideI est o.aTl s she o qisu gse eyn nsc ete rea sm. te Aissn tch iae npt car o ibc mlaet p etloe c tgo eem dnm eesruac ntrei i p cat ani t o iao nlnarre sml y a s tst eie dgmntoails tih afe ltshe oevrd eee n sfts ioguh nna edp dsp t eao nnd iyn esg cus rse ipa niscf eioot r hum es\nofree flslt poe o xwn tu soe arl tui w mnaue rsn oui fanls g yam sctte eivs m sitayf g o eirns t . htheeproelsictreicatuetdhsoirtiet.ieIst.aTlshoisgseynsetreamteisstchaepcaobmleptloetgeedneesractreipatnioanlarremlatseidgntoaltihfetheevreenftsouhnadpspeanniynsgussipnicfioorums Thisreviewconcentrateonmethodologiesfordetectingdifferentobjectsofinterestlikehuman,vehicleortanketc.\nofeflltoexwtuoarluwnaursnuianlgamcteivssitaygeins.therestrictedsite.Italsogeneratesthecompletedescriptionrelatedtotheeventshappeningsinform [5].Surveillanceisaverydemandingtopicnowadaysbecauseithelpstheobjectthatcanbetrackedoverlongperiods\no c ft 2 e 0 x 2 tu 0 a T l h w e a A rn u in th g o m rs e . s P s u a b g l e i s s . hedbyElsevierB.V. of time under different circumstances. There are many difficulties to detect and track the object within surveillance\n(cid:30) Tchi2s0i2s0aTnhoepeAnuathcocerss.sPaurtbilcilsehuedndbeyrEthleseCviCerBBY.-VN.C-NDlicense(http://creativecommons.org/licenses/by-nc-nd/4.0/) videoduetodifferentilluminationcondition,acrossascatteredandchangingbackground,existenceofobscurityetc.\nP (cid:30) T (cid:30) c ehei2sr 0 -ir 2se 0 vai Tne h woepueAnn u dat e hcr ocer r s es.ss P pao urntbis lci ils beh ilue i dntydbeo yrf Etht l hes e eCv s iCc e i r eBn BYt . i-V fiN. cCc-oNmDmliitcteenesoef(hthttep:T//hcirredatIivnetecronmatmioonnasl.oCrogn/lfiecreennscees/obny-nCco-mndp/u4t.i0n/g) and Network So, these models are mostly used in that places where clear detection of enunciate human body or any other object\nCPTeh oei m sr-m irseuva nineicwo a p tue io nnnda secr( c C ereo sssCpa oor Nntisc eil tbe \u20191ilu 9int)y. deorfththeeCsCcieBnYti-fiNcCc-oNmDmliitcteenesoef(hthttep:T // hcirredatIivnetecronmatmioonnasl.oCrogn /lfiecreennscees/ obny-nCco-mndp / u4t.i0n / g) and Network canbedoneonthebasisoflength,widthandheight[6].Heterogeneoustechniqueshavebeendefendedfortracking,\nCPeoemr-mreuvnieicwatuionndser(CreosCpooNnseitb\u20191il9it)y. of the scientific committee of the Third International Conference on Computing and Network\ndetectionandidentificationofobjectsinvideos[9][22].\nKCeoymwmorudnsi:caItnitoenrnse(tCoofCThoiNngest;\u20191C9o)m.puterVision;AutomatedVideoSurveillance;CCTVmonitoring;Activitydetection;Humandetection;Edge\nAnautomatedsurveillancesystem,knownasknight[1],isasystemthatisusedforvideosurveillanceandmonitor-\nKcoemywpuotrindgs.: InternetofThings;ComputerVision;AutomatedVideoSurveillance;CCTVmonitoring;Activitydetection;Humandetection;Edge\nKcoemywpuotrindgs.: InternetofThings;ComputerVision;AutomatedVideoSurveillance;CCTVmonitoring;Activitydetection;Humandetection;Edge ingthroughdifferentCCTV,whichconsistsself-operativemode.Itcandetectaswellasclassifythetargetsefficiently\ncomputing. bycoherentlytrackingtheobjectthroughdifferentcamerasusingstate-of-artcomputervisiontechniques.Itproduce\ndetailedtextualdescriptivesummaryinformationintheorbitformwithgooglemaptrackingsitelocation.Thissum-\nmary will gives direction to police officer in analysis and quick response decision. The limitations of surveillance\n1. Introduction systemincludetheinabilitytodetectobjectwhichismasked,identifyingobjectsamongcrowd,managingcrowdand\n1. Introduction workinginunfavorableweatherconditionamongtheearlierautomatedmonitoringsystem.Backgroundsubtraction\n1. Introduction\nVideosurveillanceisthefacilitytoobserveandanalyzeanyparticularsiteforidentifyingsuspiciousactivitywith [7][16]canalsobeoneofthesolutionfordifferentproblemsasliketrafficcontrolling,visualinspectionandinter-\nrespVeicdteotossuarfveetiyllaanndceseiscuthrietyfapcuilriptyosteos.obVsiedrevoesaunrdveainllaalnyczeesaynsytepmarctiacmulearinstioteefxoirstiednecnetiffoyrinsgecsuursiptyicaionudscaricmtiveictyonwtriothl actionbetweencomputerandhumanbymovingobjectdetection[19].Intheseapplicationswehavesomeobjectof\nVideosurveillanceisthefacilitytoobserveandanalyzeanyparticularsiteforidentifyingsuspiciousactivitywith\nrespecttosafetyandsecuritypurposes.Videosurveillancesystemcameintoexistenceforsecurityandcrimecontrol interestfordetectionandthatcanbetrackedfortheiractivity.\nrespecttosafetyandsecuritypurposes.Videosurveillancesystemcameintoexistenceforsecurityandcrimecontrol\nOpticalflowtechniqueisusefultoidentifythevelocitiesofmovingpointsinanimage.Opticalflowworkswhen\n\u2217 Correspondingauthor.Tel.:+91-7905514182;fax:+0-000-000-0000. background is static and foreground object is in motion. Therefore, optical flow delivers important information of\n\u2217 EC-omrraeislpaodnddriensgs:aduuthsohry.aTnetl@.:m+9n1n-i7t.9ac0.5in514182;fax:+0-000-000-0000. objectmomentumw.r.t.time[17].Hassneretal.[2]hasproposedawellknownalgorithmforviolencecrowddetec-\n\u2217\nEC-omrraeislpaodnddriensgs:aduuthsohry.aTnetl@.:m+9n1n-i7t.9ac0.5in514182;fax:+0-000-000-0000.\ntion.Here,individualandstraightfullpostureofthebodyarecoveredinthescene.Unimodalbackgroundmodelis\n1877E--0m5a0i9lacdd2r0e2ss0:TdhueshAyuantht@orsm.nPnuibt.laisch.iendbyElsevierB.V.\n(cid:30) responsibletotrackmovingpersoninthescene.\nT18h7is7-is05a0n9opcen20a2c0ceTsshearAticultehournsd.ePrutbhleisCheCdBbYy-ENlCse-vNieDrBlic.Ve.nse(http://creativecommons.org/licenses/by-nc-nd/4.0/)\nPT18ehe7isr7--irs0e5va0ine9wo (cid:30) pcuenn2d0ae2cr0creTessshpeaorAntiscuiltbehioluirtnsyd.eoPrfutbthhleiesChsceCideBnbtYyifi-ENclCsceo-vNmieDmrBiltitc.eVee.nsoef(thhtetpT:/h/cirrdeaItnivteercnoamtiomnoanlsC.oorngf/leirceenncseeso/nbyC-nocm-npdu/t4in.0g/)andNetworkCommunications J.Rehgetal.[13]proposedatechniquetodetectandtrackofmovingpersonwhoiswalkinginprohibitedareaof\n(cid:30)\n(PTCehoeisrC-iroseNvaienetw\u2019o1p9ue)nn.daecrcreessspaorntiscilbeiluitnydeorftthheeCscCieBntYifi-NcCco-NmDmiltitceeensoef(thhtetpT:/h/cirrdeaItnivteercnoamtiomnoanlsC.oorngf/leirceenncseeso/nbyC-nocm-npdu/t4in.0g/)andNetworkCommunications kioskbyusingfacedetection,skincolorandstereo.StaufferandGrimson[14]proposedaBackgroundSubtraction\n(PCeoerC-roeNvieetw\u201919u)n.derresponsibilityofthescientificcommitteeoftheThirdInternationalConferenceonComputingandNetworkCommunications\nmodelthathascapabilitytoprovidegoodresultswhilechangeshappensduetoillumination,repeateddisorderfrom\n(CoCoNet\u201919).",
        "tables": [],
        "forms": {},
        "confidence_score": 1.0
      },
      {
        "page_number": 2,
        "text_content": "352 Dushyant Kumar Singh et al. / Procedia Computer Science 171 (2020) 350\u2013359\nDushyantKumarSingh,SumitParoothi,MayankkumarRusiaetal./ProcediaComputerScience00(2019)000\u2013000 3 4 DushyantKumarSingh,SumitParoothi,MayankkumarRusiaetal./ProcediaComputerScience00(2019)000\u2013000\nbackgroundmotion,andlongscenechangingstatus.Theyobservedundesiredorsuspiciousactivitiesthroughanalysis Background Subtraction Technique [7] [17] is mostly used in real time scenario, as HOG could not work in\nof statistics rules and specified amount of time learning mechanism of common patterns of activities. Ricquebourg realtimeefficientllyduetoitshighrequirementofprocessingspeed.Therefore,insteadofprocessingeachframewe\nandBouthemy[15]usesspatiotemporalmechanismtotrackanddetectthepersonbyfindingthetemporaldifferences processtheframesinwhichthemotionisdetected.Formotiondetection,backgroundsubtractionalgorithmisused,\namongthreesuccessiveframesandafterthatcomparisonistakeplacebetweencurrentframetobackgroundreference inwhichthepixelpositionbetweentwoimagescanshowthetruedifferenceinintensitywithrespecttodisplacement.\nframewithconsideringintensitychange. If,weremovethebackgroundthenthisisassumedthatbackgroundpixelsarestaticandforegroundimagepixelsare\nAbdelkaderetal.[18]suggestedaframeworkfordetectingthegestureandactionrecognitionbasedonsilhouette in motion. When movement is detected by background subtraction algorithm, HOG + SVM Classifier is triggered\nmechanism.ThisrecognitionwasdoneviamodellingtrajectoriesonRiemannianshapemanifoldsmethodology.Kel- for human detection. Now, any threshold value can be taken accordingly to improve the accuracy of approach. If,\nlokumpuetal.[20]usedthedynamictexturebasedmethodstorecognizethehumanactivitiesinspatiotemporalway. movementinimageisfastthenhighervalueofthresholdisneeded.\nThe LBP-TOP is used for extracting the features in spatiotemporal space to identify human volumes as well as its HistogramofGradient(HOG)[11][21]isawell-knowntechniqueofthecomputervision.Itisbasicallyaglobal\nmovements.Asper[8]and[9],CCTVtechnologyhasraisedthedemandofCCTVdeploymentincommercialaswell imagedescriptorusedforobjectdetectionandfeatureextractionwithveryhighsuccessrate.Themainobjectiveof\naspublicandprivatesectorstofulfiltheneedofsecuritytask.Nowaday\u2019snumerousCCTVusershavingversatile HOGdescriptoristocounttheexistenceofgradientsorientationswithintheimagelocally.TheHOGusesnumberof\nskills,managementandtroubleshootingexperience.Forexample,USgovernmentallowswebuserstoseeliveCCTV stepsforextractingthefeaturesfromtheimage.Firststepistosplittheimageintotheblocks.Further,theseblocks\nvideofootagethroughinternettomonitorillegalcrossingandconveysuchtypeofactivitytorespectiveauthorities. areechelonedintothecells.Then,thehorizontalandverticalgradientsareevaluatedforeachpixelwithinthecell.\nResidentsofUKuseslivedigitalCCTVimagestorecognizeanysuspiciousorunpredictableactivitybysubscribing TheSobeloperatorcanbeusedforevaluatingthesegradientsasshowninequation1and2.\ncommunitysafetychannelsandrespondimmediatelytopoliceifsomethingfoundillegal.\nS (y,x)=Y(y,x+1) Y(y,x 1) (1)\nx\n\u2212 \u2212\nS (y,x)=Y(y+1,x) Y(y 1,x) (2)\n3. Methodology y \u2212 \u2212\nWhere the pixel intensity with co-ordinates value (x, y) is represented by Y(y, x). The horizontal and vertical\nThesystemwouldmakeuseoftheexistingCCTVinfrastructureinpublicplaceslikeparks,railwaystation,roads,\ngradientsarerepresentedbyS (y,x)andS (y,x)respectively.Thegradientsmagnitude(S)anditsorientation(\u03b8)can\nshoppingcomplexesetc.anddetectsuspiciousactivitiesusingsophisticatedcomputervisiontechniques.Thesystem x y\nbedrawnbyequation3andequation4.\nwould alert the main police control room (or Headquarter) and the nearest police station with relevant information\nin case any suspicious/unwanted activity is detected. The system consists of two distinct components i.e. activity\nS = S2+S2 (3)\ndetectionmoduleandcommunicationmodule. x y\n(cid:31)\nS\ny\n3.1. ActivityDetectionModule \u03b8=arctan( ) (4)\nS\nx\nThiscomponentdetectsthesuspicious/unwantedactivity[12][13][15]inrealtimefromthevideofeedandinforms Theninenumberofhistogrambinsareusedforstoringeachorientationofgradients.Later,thisprocessisrepeated\nto communication component. This system aims to detect pedestrian in the prohibited area and to identify violent foreachcell.Thecellhistogramsarecombinedforeachblock.Innextstep,blocknormalizationprocessisperformed\nbehaviorofcrowdforcitywidesurveillance. fornormalizingtheeachvalueofbins.Theblocknormalizationplaysanimportantroletomakethisdescriptorfree\nfromlightningvariations.Atlast,allthehistogramvectorsareconcatenatedintoonevector.Thisresultantvectorcan\nalsobereferencedasfeaturevectorwhichcanbeusedforobjectdetection.Further,thesefeaturevectorsareusedto\n3.1.1. PedestriandetectioninProhibitedArea\nbuildtheSVMclassifierwhichhelpstodecidewhethertheresultingobjectishumanornot.\nPedestrian detection in prohibited area is aimed to detect presence of people in a prohibited area and capture a\nSupport Vector Machine (SVM) is very effective and useful machine learning technique to classify different\npictureasevidence.Theobjectiveisachievedbytakingadvantageofbackgroundsubtractiontechniqueandhistogram\nclasses. SVM classifier technique basically used to maximize the marginal difference between two distinct classes.\noforientedgradients(HOG).Theworkflowofpedestriandetectionframeworkisillustratedinfigure1.\nPerformanceofSVMisbestinhighdimensionalspaceespeciallywhenavailabledatasetisless.SVMworkswellfor\nlinearhyperplaneaswellasmultidimensionalhyperplanebutitcanalsobegoodifdataisnotinlinearform.Sofor\nnon-linearclassificationSVMusesdifferentkernelfunctionforvariousdecisionfunction.Thisiscalledapproximation\nfunction.Twomostcommonlyusedfunctionsarelossfunctionandobjectivefunction.LossFunctiondescribeswhat\nto minimize to achieve best results. Hinge Loss is used for training classifier and maximize margin classification is\nusedforsupportvectormachine.\nC(x,y, f(x))=(1 y f(x)) (5)\n+\n\u2212 \u2217\nHere, C, x, y and f(x) are loss, sample value, true level and predicted level respectively. If y f(x) 1 then\n\u2217 \u2265\nC(x,y, f(x))=0otherwiseC(x,y, f(x))=1 y f(x).\n\u2212 \u2217\nObjectivefunctionofSVMconcernwithregularizerandloss.Wewanttofindthedecisionsurfacethatismaximum\nfarawayfromanydatapoints.Regularizercontrolsthetradeoff,sothattrainingandtestingerrorscanbeminimizedto\nimprovetheperformanceofclassifierforunseendata.Ifsamplesareclassifiedcorrectlythenweightwcanbeupdated\nbythegradientusingequation6.\nFig.1.WorkFlowofPedestrianDetectionFramework w=w+n( 2\u03bdw) (6)\n\u2212",
        "tables": [],
        "forms": {},
        "confidence_score": 1.0
      },
      {
        "page_number": 3,
        "text_content": "Dushyant Kumar Singh et al. / Procedia Computer Science 171 (2020) 350\u2013359 353\nDushyantKumarSingh,SumitParoothi,MayankkumarRusiaetal./ProcediaComputerScience00(2019)000\u2013000 3 4 DushyantKumarSingh,SumitParoothi,MayankkumarRusiaetal./ProcediaComputerScience00(2019)000\u2013000\nbackgroundmotion,andlongscenechangingstatus.Theyobservedundesiredorsuspiciousactivitiesthroughanalysis Background Subtraction Technique [7] [17] is mostly used in real time scenario, as HOG could not work in\nof statistics rules and specified amount of time learning mechanism of common patterns of activities. Ricquebourg realtimeefficientllyduetoitshighrequirementofprocessingspeed.Therefore,insteadofprocessingeachframewe\nandBouthemy[15]usesspatiotemporalmechanismtotrackanddetectthepersonbyfindingthetemporaldifferences processtheframesinwhichthemotionisdetected.Formotiondetection,backgroundsubtractionalgorithmisused,\namongthreesuccessiveframesandafterthatcomparisonistakeplacebetweencurrentframetobackgroundreference inwhichthepixelpositionbetweentwoimagescanshowthetruedifferenceinintensitywithrespecttodisplacement.\nframewithconsideringintensitychange. If,weremovethebackgroundthenthisisassumedthatbackgroundpixelsarestaticandforegroundimagepixelsare\nAbdelkaderetal.[18]suggestedaframeworkfordetectingthegestureandactionrecognitionbasedonsilhouette in motion. When movement is detected by background subtraction algorithm, HOG + SVM Classifier is triggered\nmechanism.ThisrecognitionwasdoneviamodellingtrajectoriesonRiemannianshapemanifoldsmethodology.Kel- for human detection. Now, any threshold value can be taken accordingly to improve the accuracy of approach. If,\nlokumpuetal.[20]usedthedynamictexturebasedmethodstorecognizethehumanactivitiesinspatiotemporalway. movementinimageisfastthenhighervalueofthresholdisneeded.\nThe LBP-TOP is used for extracting the features in spatiotemporal space to identify human volumes as well as its HistogramofGradient(HOG)[11][21]isawell-knowntechniqueofthecomputervision.Itisbasicallyaglobal\nmovements.Asper[8]and[9],CCTVtechnologyhasraisedthedemandofCCTVdeploymentincommercialaswell imagedescriptorusedforobjectdetectionandfeatureextractionwithveryhighsuccessrate.Themainobjectiveof\naspublicandprivatesectorstofulfiltheneedofsecuritytask.Nowaday\u2019snumerousCCTVusershavingversatile HOGdescriptoristocounttheexistenceofgradientsorientationswithintheimagelocally.TheHOGusesnumberof\nskills,managementandtroubleshootingexperience.Forexample,USgovernmentallowswebuserstoseeliveCCTV stepsforextractingthefeaturesfromtheimage.Firststepistosplittheimageintotheblocks.Further,theseblocks\nvideofootagethroughinternettomonitorillegalcrossingandconveysuchtypeofactivitytorespectiveauthorities. areechelonedintothecells.Then,thehorizontalandverticalgradientsareevaluatedforeachpixelwithinthecell.\nResidentsofUKuseslivedigitalCCTVimagestorecognizeanysuspiciousorunpredictableactivitybysubscribing TheSobeloperatorcanbeusedforevaluatingthesegradientsasshowninequation1and2.\ncommunitysafetychannelsandrespondimmediatelytopoliceifsomethingfoundillegal.\nS (y,x)=Y(y,x+1) Y(y,x 1) (1)\nx\n\u2212 \u2212\nS (y,x)=Y(y+1,x) Y(y 1,x) (2)\n3. Methodology y \u2212 \u2212\nWhere the pixel intensity with co-ordinates value (x, y) is represented by Y(y, x). The horizontal and vertical\nThesystemwouldmakeuseoftheexistingCCTVinfrastructureinpublicplaceslikeparks,railwaystation,roads,\ngradientsarerepresentedbyS (y,x)andS (y,x)respectively.Thegradientsmagnitude(S)anditsorientation(\u03b8)can\nshoppingcomplexesetc.anddetectsuspiciousactivitiesusingsophisticatedcomputervisiontechniques.Thesystem x y\nbedrawnbyequation3andequation4.\nwould alert the main police control room (or Headquarter) and the nearest police station with relevant information\nin case any suspicious/unwanted activity is detected. The system consists of two distinct components i.e. activity\nS = S2+S2 (3)\ndetectionmoduleandcommunicationmodule. x y\n(cid:31)\nS\ny\n3.1. ActivityDetectionModule \u03b8=arctan( ) (4)\nS\nx\nThiscomponentdetectsthesuspicious/unwantedactivity[12][13][15]inrealtimefromthevideofeedandinforms Theninenumberofhistogrambinsareusedforstoringeachorientationofgradients.Later,thisprocessisrepeated\nto communication component. This system aims to detect pedestrian in the prohibited area and to identify violent foreachcell.Thecellhistogramsarecombinedforeachblock.Innextstep,blocknormalizationprocessisperformed\nbehaviorofcrowdforcitywidesurveillance. fornormalizingtheeachvalueofbins.Theblocknormalizationplaysanimportantroletomakethisdescriptorfree\nfromlightningvariations.Atlast,allthehistogramvectorsareconcatenatedintoonevector.Thisresultantvectorcan\nalsobereferencedasfeaturevectorwhichcanbeusedforobjectdetection.Further,thesefeaturevectorsareusedto\n3.1.1. PedestriandetectioninProhibitedArea\nbuildtheSVMclassifierwhichhelpstodecidewhethertheresultingobjectishumanornot.\nPedestrian detection in prohibited area is aimed to detect presence of people in a prohibited area and capture a\nSupport Vector Machine (SVM) is very effective and useful machine learning technique to classify different\npictureasevidence.Theobjectiveisachievedbytakingadvantageofbackgroundsubtractiontechniqueandhistogram\nclasses. SVM classifier technique basically used to maximize the marginal difference between two distinct classes.\noforientedgradients(HOG).Theworkflowofpedestriandetectionframeworkisillustratedinfigure1.\nPerformanceofSVMisbestinhighdimensionalspaceespeciallywhenavailabledatasetisless.SVMworkswellfor\nlinearhyperplaneaswellasmultidimensionalhyperplanebutitcanalsobegoodifdataisnotinlinearform.Sofor\nnon-linearclassificationSVMusesdifferentkernelfunctionforvariousdecisionfunction.Thisiscalledapproximation\nfunction.Twomostcommonlyusedfunctionsarelossfunctionandobjectivefunction.LossFunctiondescribeswhat\nto minimize to achieve best results. Hinge Loss is used for training classifier and maximize margin classification is\nusedforsupportvectormachine.\nC(x,y, f(x))=(1 y f(x)) (5)\n+\n\u2212 \u2217\nHere, C, x, y and f(x) are loss, sample value, true level and predicted level respectively. If y f(x) 1 then\n\u2217 \u2265\nC(x,y, f(x))=0otherwiseC(x,y, f(x))=1 y f(x).\n\u2212 \u2217\nObjectivefunctionofSVMconcernwithregularizerandloss.Wewanttofindthedecisionsurfacethatismaximum\nfarawayfromanydatapoints.Regularizercontrolsthetradeoff,sothattrainingandtestingerrorscanbeminimizedto\nimprovetheperformanceofclassifierforunseendata.Ifsamplesareclassifiedcorrectlythenweightwcanbeupdated\nbythegradientusingequation6.\nFig.1.WorkFlowofPedestrianDetectionFramework w=w+n( 2\u03bdw) (6)\n\u2212",
        "tables": [],
        "forms": {},
        "confidence_score": 1.0
      },
      {
        "page_number": 4,
        "text_content": "354 Dushyant Kumar Singh et al. / Procedia Computer Science 171 (2020) 350\u2013359\n6 DushyantKumarSingh,SumitParoothi,MayankkumarRusiaetal./ProcediaComputerScience00(2019)000\u2013000\nDushyantKumarSingh,SumitParoothi,MayankkumarRusiaetal./ProcediaComputerScience00(2019)000\u2013000 5\nFig.2.WorkFlowofViolentCrowdDetectionFramework\nIfsamplesaremisclassifiedthangradientofbothtermscanbeupdatedwithrespecttoweightvectorasshownin\neqation7.\nw=w+n(yx 2\u03bdw) (7)\ni i\n\u2212\nWherenisthelearningrateand\u03bdisregularizer.Asaregularizingparameterwehavetochoose1/epochs(epoch=\nNumberofiterationtotrainthemachine).\n3.1.2. ViolentCrowdBehavior\nFig.3.FlowofCommunicationModule\nViolentCrowdBehaviorfocusesonmonitoringthecrowdedeventsforoutbreaksofviolence.Efficientanalysisin\nrealtimeoperatingenvironmentcanbeachievedbyobservingdensityonaspecificpointandmotionofthecrowdin\nspecificdirection.Movementofthecrowdtowardscertainplacecanbeoneofthecausetoviolence.Forthepurpose,\n3.1.3. CommunicationModule\nwe have to find out high shape and intensive processing without compromising processing speed with respect to\nThis component is responsible for transferring of information from the site under surveillance to the pertinent\nchanges observed in vector magnitudes through time. The Violent Flows (ViF) descriptor is used here to extract\npolice control room. There are 3 types of nodes in communication module architecture i.e. main server, client A,\nthe motion information for sequence of frames. Further, these vectors are used svm classifier [18] [22] is used for\nclientB.ThebriefarchitectureofcommunicationmoduleforproposedsystemisshowninFigure3.\nclassifying the violent or non-violent behaviour. The complete process for violent crowd detection is presented in\nMainserverisresponsibleforallkindsofcommunicationsinthesystem.Asimplewebserverhasbeenutilized\nFigure2.\nforthispurpose.ARESTfulwebserviceismadetorunontheserverwhichrespondstotherequestofallotherclients.\nViF representation framework [23] [24] is used to notice the violence in the real time video sequences. It is\nCurrentlythebuilt-inserverofflaskhasbeenutilized.Italsomaintainsadatabasewhichstores:\ncalculated into various steps. First step is to evaluate the optical flow vectors between the couples of successive\nvideosequences.Theflowvectorsareprovidedforeachframepixels.ThepixelisrepresentedbyP andtheflow\nx,y,t\nConfigurationsettingsforeachsurveillancesite(discussedindetaillater).\nvectorsaredenotedbyu x,y,t andv x,y,t .Where,tisrepresentedbyindexofframe.Themagnitudeofthevectorscanbe \u2022\nIDs for each surveillance site, police control room along with description address, contact and other relevant\nrepresentedbyequation8. \u2022\ninfo.\nM = u2 +v2 (8) Messagesreceivedfromsurveillancesites.\nx,y,t x,y,t x,y,t \u2022\n(cid:31)\nNext,thebinaryindicator B x,y,t isevaluatedforeachframepixels.Itisevaluatedbyif | M x,y,t \u2212 M x,y,t \u2212 1| \u2264 \u03b8 then TheparticularsofthedatabaseschemacanbeprocuredfromFigure4.\nB = 1otherwise B = 0.Itshowstheimportanceofmagnitudesadjustmentamongvideoframes.Where,\u03b8 is\nx,y,t x,y,t\nthethresholdvaluethatisadaptivelyfixedtoaveragerateof M M .Here,theaveragemodificationchange\n| x,y,t \u2212 x,y,t \u2212 1|\nmapisevaluatedforindividualpixelsoverentiresequencesorframesbyaverageofthesebinaryvalues(asshownin\nequation9).\n1\nb = \u03a3 b (9)\nx,y x,y x,y,t\nT\nFurther,thereisneedtobeformedtheViF-descriptor.Itisformedbyapportioningthebintonon-overlappingcells\nofsizeMxN.ItisalsobeneededtocollectthemagnitudeadjustmentfrequencythatisalsohelpfultobuildtheViF-\ndescriptionforeachseparatedcell.Here,afixedsizeofhistogramisusedforrepresentingthechangeinmagnitude\ndistributionofeachcell.Finally,theseevaluatedhistogramsarefusedintoadistinctvector.Thisevaluatedvectoris\nrepresentedasViF-descriptorfeaturevector.Thenthesefeaturevectorcanbeusedtobuildanyoftheclassifier[6].\nIn this paper, the ViF-descriptor is used to extract the information from the video sequences. The information from\nFig.4.DatabaseDesign\nfeaturevectorisusedbytheSVMclassifiertoclassifytheviolentandnon-violentscenes.",
        "tables": [],
        "forms": {},
        "confidence_score": 1.0
      },
      {
        "page_number": 5,
        "text_content": "Dushyant Kumar Singh et al. / Procedia Computer Science 171 (2020) 350\u2013359 355\n6 DushyantKumarSingh,SumitParoothi,MayankkumarRusiaetal./ProcediaComputerScience00(2019)000\u2013000\nDushyantKumarSingh,SumitParoothi,MayankkumarRusiaetal./ProcediaComputerScience00(2019)000\u2013000 5\nFig.2.WorkFlowofViolentCrowdDetectionFramework\nIfsamplesaremisclassifiedthangradientofbothtermscanbeupdatedwithrespecttoweightvectorasshownin\neqation7.\nw=w+n(yx 2\u03bdw) (7)\ni i\n\u2212\nWherenisthelearningrateand\u03bdisregularizer.Asaregularizingparameterwehavetochoose1/epochs(epoch=\nNumberofiterationtotrainthemachine).\n3.1.2. ViolentCrowdBehavior\nFig.3.FlowofCommunicationModule\nViolentCrowdBehaviorfocusesonmonitoringthecrowdedeventsforoutbreaksofviolence.Efficientanalysisin\nrealtimeoperatingenvironmentcanbeachievedbyobservingdensityonaspecificpointandmotionofthecrowdin\nspecificdirection.Movementofthecrowdtowardscertainplacecanbeoneofthecausetoviolence.Forthepurpose,\n3.1.3. CommunicationModule\nwe have to find out high shape and intensive processing without compromising processing speed with respect to\nThis component is responsible for transferring of information from the site under surveillance to the pertinent\nchanges observed in vector magnitudes through time. The Violent Flows (ViF) descriptor is used here to extract\npolice control room. There are 3 types of nodes in communication module architecture i.e. main server, client A,\nthe motion information for sequence of frames. Further, these vectors are used svm classifier [18] [22] is used for\nclientB.ThebriefarchitectureofcommunicationmoduleforproposedsystemisshowninFigure3.\nclassifying the violent or non-violent behaviour. The complete process for violent crowd detection is presented in\nMainserverisresponsibleforallkindsofcommunicationsinthesystem.Asimplewebserverhasbeenutilized\nFigure2.\nforthispurpose.ARESTfulwebserviceismadetorunontheserverwhichrespondstotherequestofallotherclients.\nViF representation framework [23] [24] is used to notice the violence in the real time video sequences. It is\nCurrentlythebuilt-inserverofflaskhasbeenutilized.Italsomaintainsadatabasewhichstores:\ncalculated into various steps. First step is to evaluate the optical flow vectors between the couples of successive\nvideosequences.Theflowvectorsareprovidedforeachframepixels.ThepixelisrepresentedbyP andtheflow\nx,y,t\nConfigurationsettingsforeachsurveillancesite(discussedindetaillater).\nvectorsaredenotedbyu x,y,t andv x,y,t .Where,tisrepresentedbyindexofframe.Themagnitudeofthevectorscanbe \u2022\nIDs for each surveillance site, police control room along with description address, contact and other relevant\nrepresentedbyequation8. \u2022\ninfo.\nM = u2 +v2 (8) Messagesreceivedfromsurveillancesites.\nx,y,t x,y,t x,y,t \u2022\n(cid:31)\nNext,thebinaryindicator B x,y,t isevaluatedforeachframepixels.Itisevaluatedbyif | M x,y,t \u2212 M x,y,t \u2212 1| \u2264 \u03b8 then TheparticularsofthedatabaseschemacanbeprocuredfromFigure4.\nB = 1otherwise B = 0.Itshowstheimportanceofmagnitudesadjustmentamongvideoframes.Where,\u03b8 is\nx,y,t x,y,t\nthethresholdvaluethatisadaptivelyfixedtoaveragerateof M M .Here,theaveragemodificationchange\n| x,y,t \u2212 x,y,t \u2212 1|\nmapisevaluatedforindividualpixelsoverentiresequencesorframesbyaverageofthesebinaryvalues(asshownin\nequation9).\n1\nb = \u03a3 b (9)\nx,y x,y x,y,t\nT\nFurther,thereisneedtobeformedtheViF-descriptor.Itisformedbyapportioningthebintonon-overlappingcells\nofsizeMxN.ItisalsobeneededtocollectthemagnitudeadjustmentfrequencythatisalsohelpfultobuildtheViF-\ndescriptionforeachseparatedcell.Here,afixedsizeofhistogramisusedforrepresentingthechangeinmagnitude\ndistributionofeachcell.Finally,theseevaluatedhistogramsarefusedintoadistinctvector.Thisevaluatedvectoris\nrepresentedasViF-descriptorfeaturevector.Thenthesefeaturevectorcanbeusedtobuildanyoftheclassifier[6].\nIn this paper, the ViF-descriptor is used to extract the information from the video sequences. The information from\nFig.4.DatabaseDesign\nfeaturevectorisusedbytheSVMclassifiertoclassifytheviolentandnon-violentscenes.",
        "tables": [],
        "forms": {},
        "confidence_score": 1.0
      },
      {
        "page_number": 6,
        "text_content": "356 Dushyant Kumar Singh et al. / Procedia Computer Science 171 (2020) 350\u2013359\nDushyantKumarSingh,SumitParoothi,MayankkumarRusiaetal./ProcediaComputerScience00(2019)000\u2013000 7 8 DushyantKumarSingh,SumitParoothi,MayankkumarRusiaetal./ProcediaComputerScience00(2019)000\u2013000\nClient A is placed in each site under surveillance as a node. It will receive video feed from all CCTVs on this\nsite. It will process the feed and send a thorough account of any activity to the main server. Each site will have its\nownconfigurationsettings.Itwillretrievethesesettingsperiodicallyfromthemainserver.Themessagesenttomain\nserverwillcontainthesefields:SiteID,CCTVID,ActivityRecognized,TimeStampandImageSequence.\nConfigurationsettingsgovernthefunctioningofthisnodeandalsokeepdetailsaboutthenode.Thesesettingswill\nbestoredonthelocalfilesystemitself.EachclientAnodeperiodically(15seconds)requeststhemainserverforits\nconfigurationsettingsandworksaccordingly.\nClient B is situated in each police control room as a node. It will periodically request the main server (at small\nintervalof1-2seconds)tocheckifithasanynewmessagewhichisdestinedforthiscontrolRoom.ClientB2receives\nalerts from all surveillance sites. While, other control rooms as client B1 receive only if they are the nearest site. It\nis provided with a GUI which shows which site has reported an alert. The GUI makes it easier and intuitive to get\nFig.6.DetectionResults\ninformation about any site. Configuration Settings enable the server to have fine control over each surveillance site\nwithrespecttoitsfunctioningandself-information.\nConfigurationSettingsincludethesefields:SiteID,NearestPoliceControlRoomID,MainPoliceControlRoom 4.1. ActivityDetectionModule\nID,Description(Address,Contactetc.)andisAreaProhibited(True/False).\nGraphicaluserinterface-googlemaps(GUI)enablesthepersonnelatpoliceheadquarterstomonitorthecomplete Currentlythesystemsupportsdetectionoftwoactivities.Eachactivitywastestedunderdifferentbenchmarksas\ncity.Allthesurveillancesitesaremarkedonthemap.TheCCTVcamerasareusedheretomonitortheactivitiesof applicable.\npeople.Ifanysuspiciousactivityisrecorded,thesystemsalertsthepolicepersonnelandalsohighlightsthelocation HumandetectioninprohibitedareaisdonewithHOGdescriptor[15][23][24].Thresholdingareaparameterused\noftheactivity.Thepersonnelcanalsoseekinformationofanysurveillancesitegooglemapsjavascript.APIisused inbackgroundsubtractionalsoimpactsprocessingtime.Iffalsepositiveincreasesthanprocessingtimeincreasesand\nto embed map in the GUI. google maps geocoding API is used for conversion between addresses and Geolocation. viceversa.TheresultscanbeseeninFigure6.\nHTMLandCSShavebeenusedtocustomizealertwindowsinmap.QWebViewisaWebKitwidgetfromthePyQt Theperformance(intermofprocessingtime)oftheproposedalgorithmundervariousparametersareillustratedin\nlibrary. It enables us to use java script code from within a python Qt application. A button is also displayed on the Table1andTable2.BothtablesshowtheparametertuninginHOGdescriptoraccordingtoWinStrideandScaleFactor.\ncustomized info window on google maps. The person can view the video or image (evidence) as applicable to the Where, WinStride is the stepsize in x and y directions of sliding window and the ScaleFactor is used to control the\nactivity(showninfigure5). imagepyramid.\nTable1.ParameterResultstuninginHOGdescriptorasperWinStride.\n4. EXPERIMENTALRESULTS\nS.No. WinStride(x,y) ProcessingTime\nWeusedRaspberrypi3modelBrunningonraspbianstretchatsurveillancesite.Theanalysisofvideofeedhappens 1 4,4 0.47sec\nontheedgeitselfaswithedgecomputing.Theexperimentalresultsandanalysisforeachofthecomponentisshown 2 8,8 0.10sec\n3 16,16 0.071sec\ninfollowingsections:\nTable2.ParameterResultstuninginHOGdescriptorasperScaleFactor.\nS.No. ScaleFactor ProcessingTime\n1 1.01 0.50sec\n2 1.06 0.10sec\n3 1.3 0.03sec\n4 1.5 0.029sec\nViolentcrowdbehaviordetectionalgorithmisimplementedinpython.Here,1inevery3framesareprocessedto\nachieverealtimeperformanceforaccuratetemporaldetection.Eachsequenceof15frames(aftersampling)isclassi-\nfiedasviolentbehaviorornon-violentbehaviorusingSVM.Violentflowsdatabase[25]isanexpansionbenchmark\nforcrowdviolencedetection.Weusedtotal246videosastraining(outofwhich123areforviolenceandremaining\n123non-violencebehavioridentification).Videosthatwehaveusedaredownloadedfromthewebwithaverage3.60\nseconds and are under uncontrolled, in-the-wild conditions. We adopted five-fold cross validation method which is\nmostcommonformanyproblems.The246videosarealmostequallydistributedin5parts,eachpartcontainingequal\nnumberofviolentbehaviorandnon-violentbehaviorvideowhicharerandomlyselected.Thefirstfourpartsofvideo\naretakenfortrainingandfifthpartistakenfortestingpurposes.Thetrainingsetcontains46videosofwhich23depict\nFig.5.(A)CityView,(B)SuspiciousActivityDetected,(C)SafeSiteInfo,(D)SuspiciousActivityInfo\nviolenceand23depictnon-violence.",
        "tables": [],
        "forms": {},
        "confidence_score": 1.0
      },
      {
        "page_number": 7,
        "text_content": "Dushyant Kumar Singh et al. / Procedia Computer Science 171 (2020) 350\u2013359 357\nDushyantKumarSingh,SumitParoothi,MayankkumarRusiaetal./ProcediaComputerScience00(2019)000\u2013000 7 8 DushyantKumarSingh,SumitParoothi,MayankkumarRusiaetal./ProcediaComputerScience00(2019)000\u2013000\nClient A is placed in each site under surveillance as a node. It will receive video feed from all CCTVs on this\nsite. It will process the feed and send a thorough account of any activity to the main server. Each site will have its\nownconfigurationsettings.Itwillretrievethesesettingsperiodicallyfromthemainserver.Themessagesenttomain\nserverwillcontainthesefields:SiteID,CCTVID,ActivityRecognized,TimeStampandImageSequence.\nConfigurationsettingsgovernthefunctioningofthisnodeandalsokeepdetailsaboutthenode.Thesesettingswill\nbestoredonthelocalfilesystemitself.EachclientAnodeperiodically(15seconds)requeststhemainserverforits\nconfigurationsettingsandworksaccordingly.\nClient B is situated in each police control room as a node. It will periodically request the main server (at small\nintervalof1-2seconds)tocheckifithasanynewmessagewhichisdestinedforthiscontrolRoom.ClientB2receives\nalerts from all surveillance sites. While, other control rooms as client B1 receive only if they are the nearest site. It\nis provided with a GUI which shows which site has reported an alert. The GUI makes it easier and intuitive to get\nFig.6.DetectionResults\ninformation about any site. Configuration Settings enable the server to have fine control over each surveillance site\nwithrespecttoitsfunctioningandself-information.\nConfigurationSettingsincludethesefields:SiteID,NearestPoliceControlRoomID,MainPoliceControlRoom 4.1. ActivityDetectionModule\nID,Description(Address,Contactetc.)andisAreaProhibited(True/False).\nGraphicaluserinterface-googlemaps(GUI)enablesthepersonnelatpoliceheadquarterstomonitorthecomplete Currentlythesystemsupportsdetectionoftwoactivities.Eachactivitywastestedunderdifferentbenchmarksas\ncity.Allthesurveillancesitesaremarkedonthemap.TheCCTVcamerasareusedheretomonitortheactivitiesof applicable.\npeople.Ifanysuspiciousactivityisrecorded,thesystemsalertsthepolicepersonnelandalsohighlightsthelocation HumandetectioninprohibitedareaisdonewithHOGdescriptor[15][23][24].Thresholdingareaparameterused\noftheactivity.Thepersonnelcanalsoseekinformationofanysurveillancesitegooglemapsjavascript.APIisused inbackgroundsubtractionalsoimpactsprocessingtime.Iffalsepositiveincreasesthanprocessingtimeincreasesand\nto embed map in the GUI. google maps geocoding API is used for conversion between addresses and Geolocation. viceversa.TheresultscanbeseeninFigure6.\nHTMLandCSShavebeenusedtocustomizealertwindowsinmap.QWebViewisaWebKitwidgetfromthePyQt Theperformance(intermofprocessingtime)oftheproposedalgorithmundervariousparametersareillustratedin\nlibrary. It enables us to use java script code from within a python Qt application. A button is also displayed on the Table1andTable2.BothtablesshowtheparametertuninginHOGdescriptoraccordingtoWinStrideandScaleFactor.\ncustomized info window on google maps. The person can view the video or image (evidence) as applicable to the Where, WinStride is the stepsize in x and y directions of sliding window and the ScaleFactor is used to control the\nactivity(showninfigure5). imagepyramid.\nTable1.ParameterResultstuninginHOGdescriptorasperWinStride.\n4. EXPERIMENTALRESULTS\nS.No. WinStride(x,y) ProcessingTime\nWeusedRaspberrypi3modelBrunningonraspbianstretchatsurveillancesite.Theanalysisofvideofeedhappens 1 4,4 0.47sec\nontheedgeitselfaswithedgecomputing.Theexperimentalresultsandanalysisforeachofthecomponentisshown 2 8,8 0.10sec\n3 16,16 0.071sec\ninfollowingsections:\nTable2.ParameterResultstuninginHOGdescriptorasperScaleFactor.\nS.No. ScaleFactor ProcessingTime\n1 1.01 0.50sec\n2 1.06 0.10sec\n3 1.3 0.03sec\n4 1.5 0.029sec\nViolentcrowdbehaviordetectionalgorithmisimplementedinpython.Here,1inevery3framesareprocessedto\nachieverealtimeperformanceforaccuratetemporaldetection.Eachsequenceof15frames(aftersampling)isclassi-\nfiedasviolentbehaviorornon-violentbehaviorusingSVM.Violentflowsdatabase[25]isanexpansionbenchmark\nforcrowdviolencedetection.Weusedtotal246videosastraining(outofwhich123areforviolenceandremaining\n123non-violencebehavioridentification).Videosthatwehaveusedaredownloadedfromthewebwithaverage3.60\nseconds and are under uncontrolled, in-the-wild conditions. We adopted five-fold cross validation method which is\nmostcommonformanyproblems.The246videosarealmostequallydistributedin5parts,eachpartcontainingequal\nnumberofviolentbehaviorandnon-violentbehaviorvideowhicharerandomlyselected.Thefirstfourpartsofvideo\naretakenfortrainingandfifthpartistakenfortestingpurposes.Thetrainingsetcontains46videosofwhich23depict\nFig.5.(A)CityView,(B)SuspiciousActivityDetected,(C)SafeSiteInfo,(D)SuspiciousActivityInfo\nviolenceand23depictnon-violence.",
        "tables": [],
        "forms": {},
        "confidence_score": 1.0
      },
      {
        "page_number": 8,
        "text_content": "358 Dushyant Kumar Singh et al. / Procedia Computer Science 171 (2020) 350\u2013359\nDushyantKumarSingh,SumitParoothi,MayankkumarRusiaetal./ProcediaComputerScience00(2019)000\u2013000 9 10 DushyantKumarSingh,SumitParoothi,MayankkumarRusiaetal./ProcediaComputerScience00(2019)000\u2013000\ncomparedforpedestriandetectionandviolencecrowddetection.Formeasurementofprocessingtime,theRaspberry\nPi device and personal computer (PC) are used as processing unit. The system offers a host of bright prospects for\nextending the work to make it even more effective. It can be used in other various applications or detecting other\nunwantedactivitieslikeabnormalcrowddensity,multiplepeoplerunning(theftetc.),personfallingetc.fromCCTV\nfootageinrealtime.\nReferences\nFig.7.ConfusionMatrixforCrowdViolencedetectionmodel\n[1] PoliceDepartmentInformationSystemsTechnologyEnhancementProject(ISTEP).\u201dReportontheReviewoftheQueenslandPoliceService,\nBrisbane.\u201dDepartmentofJustice,OfficeofCommunityOrientedPolicingServices,Washington,DC.Bingham,M.,2014\n[2] T.Hassner,Y.Itcher,andO.Kliper-Gross.\u201dViolentFlows:Real-TimeDetectionofViolentCrowdBehavior.\u201d3rdIEEEInternationalWorkshop\nThe results can be visualized from the confusion matrix [24] [26] in Figure 7. We process selective short time onSociallyIntelligentSurveillanceandMonitoring(SISM)attheIEEEConf.onComputerVisionandPatternRecognition(CVPR),Rhode\nof delay in frame sequences separately, classifying them by labelling as either violent or nonviolent, if violent sub- Island,June2012.\nsequence of frames are detected then action take place. Acceptable results were obtained when we used PC for [3] H.Keval.\u201dEffective,design,configuration,anduseofdigitalCCTV.\u201dPhDthesis,UniversityCollegeLondon,2009\n[4] N.Petrovic,N.Jojic,andT.Huang.\u201dAdaptivevideofastforward.\u201dMultimediaToolsandApplications,26(3):327\u2013344,2005.\nprocessing. Extraction of feature vector (ViF descriptor) for a sequence of 15 frames took on an average 5 seconds\n[5] Y.Pritch,S.Ratovitch,A.Hendel,andS.Peleg.\u201dClusteredsynopsisofsurveillancevideo.\u201dInAdvancedVideoandSignalBasedSurveillance,\ntobeprocessed.Hence,appropriatesamplinghadtobedonewithoutcompromisingondelayinalerts.Experiments\npages195\u2013200,2009\nshowedthatnotmorethan15continuousframesarerequiredforclassification. [6] M.Shah,O.Javed,K.Shafique.\u201dAutomatedVisualSurveillanceinRealisticScenarios.\u201dIEEEMultiMedia,Vol.14,Issue:1,2007\nWhere,TP,FP,FNandTNareTruePositive,FalsePositive,FalseNegativeandTrueNegativerespectively.The [7] O.Javed,K.Shafique,andM.Shah.\u201dAHierarchicalApproachtoRobustBackgroundSubtractionUsingColorandGradientInformation.\u201d\nmathematicalevaluationofaccuracyandsensitivityareshowninequation10and11. Proc.IEEEWorkshoponMotionandVideoComputing,IEEECSPress,2002,pp.22-27\n[8] O.Javed.\u201dTrackingacrossMultipleCameraswithDisjointViews.\u201dProc.Proc.9thIEEEInt\u2019lConf.ComputerVision,pp.343-3572003\nTP+TN [9] R.Collins,A.Lipton,T.Kanade\u201dIntroductiontotheSpecialSectiononVideoSurveillance.\u201dIEEETrans.PatternAnalysisandMachine\nAccuracy= =81.25% (10)\nTP+FP+TN+FN Intelligence,vol.22,no.8,pp.745,2000\n[10] ManoharKarki,SaikatBasu,RobertDiBiano,SupratikMukhopadhyay,JerryWeltman,MalcolmStagg.\u201dAsymbolicframeworkforrecogniz-\nTP ingactivitiesinfullmotionsurveillancevideos.\u201dComputationalIntelligence(SSCI),pp.1-7,2016.\nSensitivity= =87.5% (11) [11] N.Dalal,B.Triggs.\u201dHistogramsoforientedgradientsforhumandetection.\u201dComputerVisionandPatternRecognition,2005.CVPR2005.\nTP+FN\nIEEE\n[12] Wrenetal.\u201dPfinder,Real-TimeTrackingoftheHumanBody.\u201dIEEETrans.PatternAnalysisandMachineIntelligence,vol.19,no.7,pp.\n4.2. CommunicationComponent\n780-785,1997.\n[13] J.Rehg,M.Loughlin,andK.Waters.\u201dVisionforaSmartKiosk.\u201dComputerVisionandPatternRecognition,IEEEPress,1997,pp.690-696\n3clientAsystemsand3clientBsystemsweremadetorunsimultaneouslywithinputfromlocallystoredvideos.A [14] StaufferandW.E.L.Grimson.\u201dLearningPatternsofActivityUsingReal-TimeTracking.\u201dIEEETrans.PatternAnalysisandMachineIntelli-\nprivatenetworkwasneededforcommunication.Thesystemwastestedusingbothmobilehotspotandcollegenetwork. gence,vol.22,no.8,2000,pp.747-757\n[15] Y.RicquebourgandP.Bouthemy.\u201dReal-TimeTrackingofMovingPersonsbyExploitingSpatiotemporalImageSlices.\u201dIEEETrans.Pattern\nNofaults/datalossincommunicationsystemwereobservedduringtesting.Therewerenoerrorsoratypicalbehavior\nAnalysisandMachineIntelligence,vol.22,no.8,2000,pp.797-808\ndetected in the communication component. The table 3 illustrates the average delay between time of occurence of\n[16] PushkarP.Goswami,DiwakarPaswan,DushyantKumarSingh.\u201dDetectingmovingobjectsintrafficsurveillancevideo.\u201dInternationalJournal\nactivity and raising of alert at client B. There was a maximum lag of 3 seconds between detection of activity at ofControlTheoryandapplications,Vol9.17,pp8423-8430(2016)\nsurveillancesiteandmessagealertatcontrolroom. [17] PushkarProtikGoswami,DushyantKumarSingh.\u201dAhybridapproachforreal-timeobjectdetectionandtrackingtocoverbackgroundturbu-\nlenceproblem.\u201dIndianJournalofScienceandTechnology,Vol9.45(2016)\n[18] Abdelkaderetal.\u201dSilhouette-basedgestureandactionrecognitionviamodelingtrajectoriesonRiemannianshapemanifolds.\u201dComputerVision\nTable3.Averagedelay(inseconds) andImageUnderstanding,115.3(2011):439-455\n[19] O.Kliper-Gross,T.Hassner,andL.Wolf.\u201dPfinder,Real-TimeTrackingoftheHumanBody.\u201dIEEETrans.PatternAnalysisandMachine\nDetection RaspberryPi PC\nIntelligence,pages31\u201345,2011\nPedestrianDetection 4.1sec 2.6sec [20] V.Kellokumpu,G.Zhao,andM.Pietikainen.\u201dHumanactivityrecognitionusingadynamictexturebasedmethod.\u201dInBMVC,pages1\u201310,\nViolentCrowdDetection 5.1sec 4.3sec 2008\n[21] O.Kliper-Gross,T.Hassner,andLiu.Beyond.\u201dPixels:ExploringNewRepresentationsandApplicationsforMotionAnalysis.\u201dPhDthesis,\nMassachusettsInstituteofTechnology,May2009\n[22] NikhilSingh,ShambhuShankarBharti,RupalSingh,DushyantKumarSingh.\u201dRemotelycontrolledhomeautomationsystem.\u201dInternational\nConferenceonAdvancesinEngineering&TechnologyResearch(ICAETR-2014),IEEE2014\n[23] Agarwal,Anshuman,ShivamGupta,andDushyantKumarSingh.\u201dReviewofopticalflowtechniqueformovingobjectdetection.\u201d2ndInter-\n5. CONCLUSIONANDFUTUREVISION\nnationalConferenceonContemporaryComputingandInformatics(IC3I),pages1\u201310,2016\n[24] KaelonLloydetal.\u201dDetectingViolentandAbnormalCrowdactivityusingTemporalAnalysisofGreyLevelCo-occurrenceMatrix(GLCM)\nDespiterecentadvancementintechnologyofcomputervisionandrelatedareas,therearestillsomemajorissues BasedTextureMeasures.\u201dComputerVisionandPatternRecognition,2017\nthatneedstobeovercomeformakingcompleterealtimeoperatingandreliableautomatedsurveillancesystem.These [25] Violent-Flows-CrowdViolence Non-violenceDatabaseandbenchmark-2017,www.cslab.openu.ac.il/download\nissuesincludetechnicalaspectasphysicalplacementtofullcoverage,installation,maintenanceofcamera,network [26] M.A.AnsariandM.Dixit.\u201dAnenhancedCBIRusingHSVquantization,discretewavelettransformandedgehistogramdescriptor.\u201d2017\nInternationalConferenceonComputing,CommunicationandAutomation(ICCCA),GreaterNoida,2017,pp.1136-1141.\nbandwidthrequiredtosupportcameraandingeneralaspectasrobustnessofcameraintypicalweatherandlightening\nconditions,installationcost,privacyconcernetc.However,demandofautomatedsurveillancesystemisbeingmade\nmore frequently specially in public safety and home security with respect to quality control parameters. Military\nintelligencecanalsobeoneoftheapplicationareaastechnologicalgrowthisalsoreachingtoheights.Thispaperis\nprovidingagoodresultforcrowdviolencedetectionintermsofaccuracyandsensitivity.Theprocessingtimeisalso",
        "tables": [],
        "forms": {},
        "confidence_score": 1.0
      },
      {
        "page_number": 9,
        "text_content": "Dushyant Kumar Singh et al. / Procedia Computer Science 171 (2020) 350\u2013359 359\nDushyantKumarSingh,SumitParoothi,MayankkumarRusiaetal./ProcediaComputerScience00(2019)000\u2013000 9 10 DushyantKumarSingh,SumitParoothi,MayankkumarRusiaetal./ProcediaComputerScience00(2019)000\u2013000\ncomparedforpedestriandetectionandviolencecrowddetection.Formeasurementofprocessingtime,theRaspberry\nPi device and personal computer (PC) are used as processing unit. The system offers a host of bright prospects for\nextending the work to make it even more effective. It can be used in other various applications or detecting other\nunwantedactivitieslikeabnormalcrowddensity,multiplepeoplerunning(theftetc.),personfallingetc.fromCCTV\nfootageinrealtime.\nReferences\nFig.7.ConfusionMatrixforCrowdViolencedetectionmodel\n[1] PoliceDepartmentInformationSystemsTechnologyEnhancementProject(ISTEP).\u201dReportontheReviewoftheQueenslandPoliceService,\nBrisbane.\u201dDepartmentofJustice,OfficeofCommunityOrientedPolicingServices,Washington,DC.Bingham,M.,2014\n[2] T.Hassner,Y.Itcher,andO.Kliper-Gross.\u201dViolentFlows:Real-TimeDetectionofViolentCrowdBehavior.\u201d3rdIEEEInternationalWorkshop\nThe results can be visualized from the confusion matrix [24] [26] in Figure 7. We process selective short time onSociallyIntelligentSurveillanceandMonitoring(SISM)attheIEEEConf.onComputerVisionandPatternRecognition(CVPR),Rhode\nof delay in frame sequences separately, classifying them by labelling as either violent or nonviolent, if violent sub- Island,June2012.\nsequence of frames are detected then action take place. Acceptable results were obtained when we used PC for [3] H.Keval.\u201dEffective,design,configuration,anduseofdigitalCCTV.\u201dPhDthesis,UniversityCollegeLondon,2009\n[4] N.Petrovic,N.Jojic,andT.Huang.\u201dAdaptivevideofastforward.\u201dMultimediaToolsandApplications,26(3):327\u2013344,2005.\nprocessing. Extraction of feature vector (ViF descriptor) for a sequence of 15 frames took on an average 5 seconds\n[5] Y.Pritch,S.Ratovitch,A.Hendel,andS.Peleg.\u201dClusteredsynopsisofsurveillancevideo.\u201dInAdvancedVideoandSignalBasedSurveillance,\ntobeprocessed.Hence,appropriatesamplinghadtobedonewithoutcompromisingondelayinalerts.Experiments\npages195\u2013200,2009\nshowedthatnotmorethan15continuousframesarerequiredforclassification. [6] M.Shah,O.Javed,K.Shafique.\u201dAutomatedVisualSurveillanceinRealisticScenarios.\u201dIEEEMultiMedia,Vol.14,Issue:1,2007\nWhere,TP,FP,FNandTNareTruePositive,FalsePositive,FalseNegativeandTrueNegativerespectively.The [7] O.Javed,K.Shafique,andM.Shah.\u201dAHierarchicalApproachtoRobustBackgroundSubtractionUsingColorandGradientInformation.\u201d\nmathematicalevaluationofaccuracyandsensitivityareshowninequation10and11. Proc.IEEEWorkshoponMotionandVideoComputing,IEEECSPress,2002,pp.22-27\n[8] O.Javed.\u201dTrackingacrossMultipleCameraswithDisjointViews.\u201dProc.Proc.9thIEEEInt\u2019lConf.ComputerVision,pp.343-3572003\nTP+TN [9] R.Collins,A.Lipton,T.Kanade\u201dIntroductiontotheSpecialSectiononVideoSurveillance.\u201dIEEETrans.PatternAnalysisandMachine\nAccuracy= =81.25% (10)\nTP+FP+TN+FN Intelligence,vol.22,no.8,pp.745,2000\n[10] ManoharKarki,SaikatBasu,RobertDiBiano,SupratikMukhopadhyay,JerryWeltman,MalcolmStagg.\u201dAsymbolicframeworkforrecogniz-\nTP ingactivitiesinfullmotionsurveillancevideos.\u201dComputationalIntelligence(SSCI),pp.1-7,2016.\nSensitivity= =87.5% (11) [11] N.Dalal,B.Triggs.\u201dHistogramsoforientedgradientsforhumandetection.\u201dComputerVisionandPatternRecognition,2005.CVPR2005.\nTP+FN\nIEEE\n[12] Wrenetal.\u201dPfinder,Real-TimeTrackingoftheHumanBody.\u201dIEEETrans.PatternAnalysisandMachineIntelligence,vol.19,no.7,pp.\n4.2. CommunicationComponent\n780-785,1997.\n[13] J.Rehg,M.Loughlin,andK.Waters.\u201dVisionforaSmartKiosk.\u201dComputerVisionandPatternRecognition,IEEEPress,1997,pp.690-696\n3clientAsystemsand3clientBsystemsweremadetorunsimultaneouslywithinputfromlocallystoredvideos.A [14] StaufferandW.E.L.Grimson.\u201dLearningPatternsofActivityUsingReal-TimeTracking.\u201dIEEETrans.PatternAnalysisandMachineIntelli-\nprivatenetworkwasneededforcommunication.Thesystemwastestedusingbothmobilehotspotandcollegenetwork. gence,vol.22,no.8,2000,pp.747-757\n[15] Y.RicquebourgandP.Bouthemy.\u201dReal-TimeTrackingofMovingPersonsbyExploitingSpatiotemporalImageSlices.\u201dIEEETrans.Pattern\nNofaults/datalossincommunicationsystemwereobservedduringtesting.Therewerenoerrorsoratypicalbehavior\nAnalysisandMachineIntelligence,vol.22,no.8,2000,pp.797-808\ndetected in the communication component. The table 3 illustrates the average delay between time of occurence of\n[16] PushkarP.Goswami,DiwakarPaswan,DushyantKumarSingh.\u201dDetectingmovingobjectsintrafficsurveillancevideo.\u201dInternationalJournal\nactivity and raising of alert at client B. There was a maximum lag of 3 seconds between detection of activity at ofControlTheoryandapplications,Vol9.17,pp8423-8430(2016)\nsurveillancesiteandmessagealertatcontrolroom. [17] PushkarProtikGoswami,DushyantKumarSingh.\u201dAhybridapproachforreal-timeobjectdetectionandtrackingtocoverbackgroundturbu-\nlenceproblem.\u201dIndianJournalofScienceandTechnology,Vol9.45(2016)\n[18] Abdelkaderetal.\u201dSilhouette-basedgestureandactionrecognitionviamodelingtrajectoriesonRiemannianshapemanifolds.\u201dComputerVision\nTable3.Averagedelay(inseconds) andImageUnderstanding,115.3(2011):439-455\n[19] O.Kliper-Gross,T.Hassner,andL.Wolf.\u201dPfinder,Real-TimeTrackingoftheHumanBody.\u201dIEEETrans.PatternAnalysisandMachine\nDetection RaspberryPi PC\nIntelligence,pages31\u201345,2011\nPedestrianDetection 4.1sec 2.6sec [20] V.Kellokumpu,G.Zhao,andM.Pietikainen.\u201dHumanactivityrecognitionusingadynamictexturebasedmethod.\u201dInBMVC,pages1\u201310,\nViolentCrowdDetection 5.1sec 4.3sec 2008\n[21] O.Kliper-Gross,T.Hassner,andLiu.Beyond.\u201dPixels:ExploringNewRepresentationsandApplicationsforMotionAnalysis.\u201dPhDthesis,\nMassachusettsInstituteofTechnology,May2009\n[22] NikhilSingh,ShambhuShankarBharti,RupalSingh,DushyantKumarSingh.\u201dRemotelycontrolledhomeautomationsystem.\u201dInternational\nConferenceonAdvancesinEngineering&TechnologyResearch(ICAETR-2014),IEEE2014\n[23] Agarwal,Anshuman,ShivamGupta,andDushyantKumarSingh.\u201dReviewofopticalflowtechniqueformovingobjectdetection.\u201d2ndInter-\n5. CONCLUSIONANDFUTUREVISION\nnationalConferenceonContemporaryComputingandInformatics(IC3I),pages1\u201310,2016\n[24] KaelonLloydetal.\u201dDetectingViolentandAbnormalCrowdactivityusingTemporalAnalysisofGreyLevelCo-occurrenceMatrix(GLCM)\nDespiterecentadvancementintechnologyofcomputervisionandrelatedareas,therearestillsomemajorissues BasedTextureMeasures.\u201dComputerVisionandPatternRecognition,2017\nthatneedstobeovercomeformakingcompleterealtimeoperatingandreliableautomatedsurveillancesystem.These [25] Violent-Flows-CrowdViolence Non-violenceDatabaseandbenchmark-2017,www.cslab.openu.ac.il/download\nissuesincludetechnicalaspectasphysicalplacementtofullcoverage,installation,maintenanceofcamera,network [26] M.A.AnsariandM.Dixit.\u201dAnenhancedCBIRusingHSVquantization,discretewavelettransformandedgehistogramdescriptor.\u201d2017\nInternationalConferenceonComputing,CommunicationandAutomation(ICCCA),GreaterNoida,2017,pp.1136-1141.\nbandwidthrequiredtosupportcameraandingeneralaspectasrobustnessofcameraintypicalweatherandlightening\nconditions,installationcost,privacyconcernetc.However,demandofautomatedsurveillancesystemisbeingmade\nmore frequently specially in public safety and home security with respect to quality control parameters. Military\nintelligencecanalsobeoneoftheapplicationareaastechnologicalgrowthisalsoreachingtoheights.Thispaperis\nprovidingagoodresultforcrowdviolencedetectionintermsofaccuracyandsensitivity.Theprocessingtimeisalso",
        "tables": [],
        "forms": {},
        "confidence_score": 1.0
      }
    ]
  },
  {
    "file_path": "pdfs\\3rd paper.pdf",
    "total_pages": 5,
    "combined_text": "CROWD MANAGEMENT, CRIME DETECTION,\nWORK MONITORING USING AI/ML\nManoj Kumar .R P.R.Adithya\nAssistant Professor, School of Computer Science and UG Scholar, School of Computer Science and\nEngineering Engineering\nVellore Institute of Technology, Vellore Institute of Technology\nChennai, India Chennai, India\nManojkumar.r@vit.ac.in Adithya.pr2022vitstudent.ac.in\nAkash\nUG Scholar, School of Computer Science and Dheepak\nEngineering UG Scholar, School of Computer Science and\nVellore Institute of Technology Engineering\nChennai, India Vellore Institute of Technology\nakash.b2022@vitstudent.ac.in Chennai, India\ndheepak.s2022@vitstudent.ac.in\nHARSHINI .V SAI LAKSHANA\nUG Scholar,School of computer science and ug scholar,school of computer science and\nengineering engineering\nvellore institute of technology vellore institute of technology\nchennai,india chennai,india\nharshini.v2022@vitstudent.ac.in sailakshana@vitstudent.ac.in\nAbstract:\nI. Introduction\nThis research endeavors to harness the potential of\nexisting Closed-Circuit Television (CCTV) networks for a\nIn our project, we leveraged the power of teachable\ncomprehensive approach to crowd management, crime\nmachines to revolutionize crime detection and crowd\nprevention, and workplace monitoring through the management. Harnessing the potential of artificial\nintelligence, particularly in computer vision, we\nintegration of Artificial Intelligence (AI) and Machine\nemployed advanced algorithms to analyze image\nLearning (ML) technologies. The primary objective is to inputs, including CCTV videos, with unprecedented\ndevelop and implement advanced algorithms capable of precision. These CCTV videos have been converted\nto images in .jpg format using an online tool, and\nreal-time analysis of video feeds, enabling the\nthese converted images were used to train the model.\nidentification and assessment of crowd dynamics, early The model employs a Convolutional Neural Network\ndetection of potential criminal activities, and continuous (CNN) to classify the input video as either normal or\nindicative of criminal activity. Furthermore, our\nmonitoring of workplace environments. By leveraging\napplication extends beyond crime prevention to\nAI/ML, the project aims to optimize surveillance crowd management, where the same technology\nfacilitates real-time monitoring and analysis of\ncapabilities, thereby enhancing public safety measures\ncrowded spaces. The teachable machines not only\nand improving organizational productivity. This initiative\nenable us to identify potential security threats but\nunderscores the transformative impact that intelligent also provide valuable insights for optimizing public\nsafety strategies. This project represents a significant\nvideo analytics can have on existing infrastructure,\nstep forward in the fusion of cutting-edge technology\nmitigating the need for extensive system overhauls while and public safety, showcasing the potential of\nsignificantly advancing security and operational artificial intelligence in creating safer and more\nsecure environments for communities.\nefficiency.\nNow to know, how a convolution neural\nnetwork works lets break it into parts. the 3\nmost important parts of this convolution neural\nnetworks are,\n1. Convolution A. To enhance image recognition in a neural network,\n2. Pooling it's crucial to identify and focus on the most relevant\n3. Flattening features while discarding unnecessary pixel\ninformation. Convolution, despite its reputation as a\nConvolution complex topic, is a fundamental technique that\nConsider a 28x28 image, like those in the MNIST simplifies this process. In essence, convolution\ndataset used for handwritten digit recognition. In a involves sliding a filter, or kernel, across an image to\nbasic artificial neural network setup, each pixel's detect various features. Kernels, represented as 2D\nvalue is treated as an individual feature input, matrices with distinct weights, traverse the image,\nresulting in 784 input nodes. While this approach replacing pixel values with the weighted average of\nmay yield satisfactory results, it falls short in neighboring values.\nrecognizing crucial features within the image. The\nmodel essentially processes each pixel independently, B. This method allows us to pinpoint significant\npotentially missing important patterns. features in an image, providing a more nuanced\nunderstanding. By applying multiple randomly\nScaling this concept to a larger image, such as a generated kernels, we can extract diverse features,\n1920x1080 Ultra HD image, poses significant enriching the model's ability to recognize intricate\nchallenges. Applying the same methodology would patterns.\nresult in an impractical 2 million input nodes. Even C. Following the convolution layer, the next step\nwith a relatively modest hidden layer of 64 nodes, involves pooling these features. Pooling is a\nwhich is insufficient for such a large input, the technique used to condense and emphasize essential\nnetwork would involve a staggering 130 million information while reducing computational\nweights. This massive scale of parameters necessitates complexity. This ensures that the model retains\nan enormous computational load, overwhelming the crucial details while discarding redundant\ncapabilities of most machines. The sheer volume of information, contributing to improved image\ncalculations involved makes it unfeasible for effective recognition performance.\nimage recognition, emphasizing the need for more\nsophisticated approaches in handling high-resolution\nD. Pooling\nimages.\nAfter identifying crucial features through\nconvolution, the challenge remains in handling the\nlarge number of inputs, a task addressed by pooling.\nPooling serves to shrink the image dimensions while\nretaining the features uncovered during convolution.\nTake, for instance, the MaxPooling method, which\noperates on a matrix shape and outputs the\nmaximum value within that region. This technique\nallows for image compression without sacrificing the\nessential features, facilitating a more manageable\ninput size for the subsequent layers of the neural\nnetwork.\nE. Flattening\nFlattening is the process of transforming a 3D or 2D\nmatrix into a 1D format, serving as the final step in\npreparing the image for input into the model. This\nstep involves converting the structured In the dynamic landscape of contemporary workplaces,\nrepresentation of the image into a linear, one- the need for efficient and effective work monitoring has\ndimensional input. The flattened data can then be become paramount. Organizations strive to optimize\nseamlessly connected to a fully connected dense layer, productivity, ensure employee safety, and maintain a\nfacilitating subsequent stages of classification in the secure work environment. The advent of machine learning\nneural network technologies has opened up new avenues for addressing\nthese challenges. This project report delves into the\n.\nimplementation of YOLO (You Only Look Once), a state-\nof-the-art object detection algorithm, as a pioneering\nsolution for work monitoring.\nYOLO(YOU ONLY LOOK ONCE) MODULE\nUnlike traditional object detection methods that involve\nmultiple stages, YOLO streamlines the process, allowing\nfor real-time detection with impressive speed and\naccuracy. Here's a step-by-step explanation of how the\nYOLO module works:\nLibraries used in this project: Input Processing:The input image undergoes grid-based\n\u27a2 OpenCV: Used to read the video input and division, forming the foundation for subsequent\nsplitting videointo frames for analysing. predictions.\n\u27a2 Keras: Used to implement neural networks. It\nis a high-level neural network library that\nruns on top of tensorflow\nBounding Box Prediction: YOLO predicts multiple\n\u27a2 Numpy: Used to process images as the image\nbounding boxes within each grid cell, each associated\npixel is in the form of matrix\nwith parameters (x, y) for the box's center, width (w),\n\u27a2 Pushbullet: It is an API used for sending SMS\nheight (h), confidence score, and class probabilities.\nto mobile phone after detecting crime.\nSample images used in Dataset: Class Prediction: YOLO determines the probability of\neach class for all bounding boxes in a grid cell, enabling\nsimultaneous detection of multiple object classes in a\ngiven image.\nConfidence Score: A confidence score indicates the\nmodel's certainty that a bounding box contains an object,\nwith a range from 0 to 1.\nNon-Maximum Suppression: Following predictions for\nall grid cells, a post-processing step, non-maximum\nsuppression, removes redundant and low-confidence\nbounding boxes, retaining only the most confident and\nnon-overlapping ones.\nInput &Output:\nThe input is a video that is being monitored for\npotential criminal activities. The output indicates\nOutput: The YOLO module produces a final output of\nwhether the video involves suspicious activities or not.\nbounding boxes, each linked to a class and a confidence\nIn the event of criminal behaviour, a notification is\nscore, representing the detected objects in the input image.\nsent to the relevant authorities.\nINTERSECTION OVER UNION\nII. OBJECT DETECTION\nA.EMPLOYEE MONITORING\nIntersection over Union (IoU) is a metric used to evaluate\nthe accuracy of an object detection algorithm, particularly 3)Uses Microsoft azure cognitive services and cloud\nin tasks such as image segmentation and bounding box system for implementation.Provides a comparative study\nprediction. IoU measures the overlap between the of traditional methodologies used such as Haar Cascade.[\npredicted bounding box and the ground truth bounding Proposed System for Criminal Detection and Recognition\nbox for a given object in an image.The IoU is calculated on CCTV Data Using Cloud and Machine Learning]\nas the ratio of the area of intersection between the\npredicted and ground truth bounding boxes to the area of Samit Shirsat, Aakash Naik, Darshan Tamse\ntheir union. The formula for IoU is:\n4) Uses pre trained deep leaning model VGGNet-19 which\ndetects gun and knife.Uses SMS sending module to send\nIOU= Area of Intersection/Area of Union alert.[ Crime Intention Detection System Using Deep\nLearning]\nHere's a breakdown of the terms: Umadevi V Navalgund, Priyadharshini.K\n1. Area of Intersection:The region where the 5) Focuses on identifying patterns and trends in crime\npredicted bounding box and the ground truth bounding occurrences.Uses ML and DL algorithms to predict crime\nbox overlap. related activities.[ Crime Prediction Using Machine\nLearning and Deep Learning: A Systematic Review and\nFuture Directions]\n2. Area of Union:The combined region covered by\nboth the predicted bounding box and the ground truth VarunMandalapu , Lavanya Elluri\nbounding box.\nThe IoU value ranges from 0 to 1, where:\nIV. Methodology\nIoU=0 indicates no overlap between the predicted and\nground truth bounding boxes. EMPLOYEE MONITORING\nIn the dynamic landscape of contemporary workplaces,\nthe need for efficient and effective work monitoring has\nIoU=1 indicates a perfect overlap between the predicted\nbecome paramount. Organizations strive to optimize\nand ground truth bounding boxes.\nproductivity, ensure employee safety, and maintain a\nsecure work environment. The advent of machine learning\ntechnologies has opened up new avenues for addressing\nthese challenges. This project report delves into the\nimplementation of YOLO (You Only Look Once), a state-\nof-the-art object detection algorithm, as a pioneering\nIII. LITERATURE SURVEY solution for work monitoring.\n1) Uses OpenCV for object detection in computer\nYOLO(YOU ONLY LOOK ONCE) MODULE\nVision.LSTM (Long Short-Term Memory) is used to\nclassify any event or behaviour as a crime or not.\n[Autonomous Anomaly Detection System for Crime\nUnlike traditional object detection methods that involve\nMonitoring and Alert Generation]\nmultiple stages, YOLO streamlines the process, allowing\nfor real-time detection with impressive speed and\nJyoti Kukad, Swapnil Soner, Sagar Pandya\naccuracy. Here's a step-by-step explanation of how the\nYOLO module works:\n2) Uses state-of-the-art face identification system\nUses deepneural networks (DNN).\nInput Processing:The input image undergoes grid-based\n[Face Detection and Recognition for Criminal\ndivision, forming the foundation for subsequent\nIdentification System]\npredictions.\nSanika Tanmay, Aamani Tandasi, Shipra Saraswat\nBounding Box Prediction: YOLO predicts multiple Upon capturing an input image indicative of theft or\nbounding boxes within each grid cell, each associated criminal activity, the system triggers an alert mechanism.\nwith parameters (x, y) for the box's center, width (w), This mechanism not only highlights the suspicious event\nheight (h), confidence score, and class probabilities. but also sends an immediate alert message to designated\nauthorities. The integration of heatmap visualization\nenhances the alert system by providing a visual\nClass Prediction: YOLO determines the probability of representation of the anomaly, allowing authorities to\neach class for all bounding boxes in a grid cell, enabling swiftly assess the situation and respond effectively.\nsimultaneous detection of multiple object classes in a\ngiven image. One of the project's standout features is the seamless\nintegration of heatmap visualization. This graphical\nrepresentation method offers a clear and intuitive display\nof numerical data, indicating the intensity of activities\nConfidence Score: A confidence score indicates the\nwithin the monitored space. In the context of work\nmodel's certainty that a bounding box contains an object,\nmonitoring and crime detection, the heatmap becomes a\nwith a range from 0 to 1.\npowerful tool, showcasing the concentration and\ndistribution of work hours and identifying anomalies that\nmay indicate criminal behavior.\nNon-Maximum Suppression: Following predictions for\nall grid cells, a post-processing step, non-maximum\nsuppression, removes redundant and low-confidence\nbounding boxes, retaining only the most confident and\nnon-overlapping ones.\nVI. Conclusion\nOutput: The YOLO module produces a final output of This project marks a significant advancement in the\nbounding boxes, each linked to a class and a confidence convergence of work monitoring and crime detection,\nscore, representing the detected objects in the input image. offering a holistic solution that promotes both workplace\nefficiency and security. The synergy between advanced\nalgorithms, specialized datasets, and heatmap\nvisualization sets this system apart, exemplifying the\npotential of technology to revolutionize surveillance and\nsafety measures in various domains. At the core of the\nV. RESULTS AND DISCUSSIONS\nsystem lies the utilization of sophisticated AI/ML\nalgorithms, particularly the YOLO model, to\nsimultaneously monitor work activities and detect\nIn the realm of advanced surveillance systems, the\ncriminal incidents. The YOLO model, renowned for its\nintegration of work monitoring and crime detection has\nefficiency in object detection, ensures precise tracking of\nreached new heights, offering a comprehensive solution to\nindividuals and objects within the monitored space. The\nenhance security measures. This innovative project\nproject's specialized dataset focuses on capturing both\nleverages cutting-edge technologies, merging work\nwork-related scenarios and criminal activities, enabling\nmonitoring outputs with crime detection capabilities,\nthe model to distinguish between routine work tasks and\nultimately contributing to a safer and more efficient\npotential thefts.\nenvironment.\n[3] Proposed System for Criminal Detection and\nVII. Reference Recognition on CCTV Data Using Cloud and Machine\nLearning\nSamit Shirsat, Aakash Naik, Darshan Tamse\n[4] Crime Intention Detection System Using Deep\n[1] Autonomous Anomaly Detection System for Crime\nLearning\nMonitoring and Alert Generation\nUmadevi V Navalgund, Priyadharshini.K\nJyoti Kukad, Swapnil Soner, Sagar Pandya\n[5] Crime Prediction Using Machine Learning and Deep\n[2] Face Detection and Recognition for Criminal\nLearning: A Systematic Review and Future Directions\nIdentification System\nVarun Mandalapu , Lavanya Elluri\nSanika Tanmay, Aamani Tandasi, Shipra Saraswat",
    "metadata": {
      "Title": "Paper Title (use style: paper title)",
      "Author": "TVL Bharathwaj",
      "Creator": "Microsoft\u00ae Word 2019",
      "CreationDate": "D:20231121153053+05'30'",
      "ModDate": "D:20231121153053+05'30'",
      "Producer": "Microsoft\u00ae Word 2019"
    },
    "error_message": null,
    "pages": [
      {
        "page_number": 0,
        "text_content": "CROWD MANAGEMENT, CRIME DETECTION,\nWORK MONITORING USING AI/ML\nManoj Kumar .R P.R.Adithya\nAssistant Professor, School of Computer Science and UG Scholar, School of Computer Science and\nEngineering Engineering\nVellore Institute of Technology, Vellore Institute of Technology\nChennai, India Chennai, India\nManojkumar.r@vit.ac.in Adithya.pr2022vitstudent.ac.in\nAkash\nUG Scholar, School of Computer Science and Dheepak\nEngineering UG Scholar, School of Computer Science and\nVellore Institute of Technology Engineering\nChennai, India Vellore Institute of Technology\nakash.b2022@vitstudent.ac.in Chennai, India\ndheepak.s2022@vitstudent.ac.in\nHARSHINI .V SAI LAKSHANA\nUG Scholar,School of computer science and ug scholar,school of computer science and\nengineering engineering\nvellore institute of technology vellore institute of technology\nchennai,india chennai,india\nharshini.v2022@vitstudent.ac.in sailakshana@vitstudent.ac.in\nAbstract:\nI. Introduction\nThis research endeavors to harness the potential of\nexisting Closed-Circuit Television (CCTV) networks for a\nIn our project, we leveraged the power of teachable\ncomprehensive approach to crowd management, crime\nmachines to revolutionize crime detection and crowd\nprevention, and workplace monitoring through the management. Harnessing the potential of artificial\nintelligence, particularly in computer vision, we\nintegration of Artificial Intelligence (AI) and Machine\nemployed advanced algorithms to analyze image\nLearning (ML) technologies. The primary objective is to inputs, including CCTV videos, with unprecedented\ndevelop and implement advanced algorithms capable of precision. These CCTV videos have been converted\nto images in .jpg format using an online tool, and\nreal-time analysis of video feeds, enabling the\nthese converted images were used to train the model.\nidentification and assessment of crowd dynamics, early The model employs a Convolutional Neural Network\ndetection of potential criminal activities, and continuous (CNN) to classify the input video as either normal or\nindicative of criminal activity. Furthermore, our\nmonitoring of workplace environments. By leveraging\napplication extends beyond crime prevention to\nAI/ML, the project aims to optimize surveillance crowd management, where the same technology\nfacilitates real-time monitoring and analysis of\ncapabilities, thereby enhancing public safety measures\ncrowded spaces. The teachable machines not only\nand improving organizational productivity. This initiative\nenable us to identify potential security threats but\nunderscores the transformative impact that intelligent also provide valuable insights for optimizing public\nsafety strategies. This project represents a significant\nvideo analytics can have on existing infrastructure,\nstep forward in the fusion of cutting-edge technology\nmitigating the need for extensive system overhauls while and public safety, showcasing the potential of\nsignificantly advancing security and operational artificial intelligence in creating safer and more\nsecure environments for communities.\nefficiency.\nNow to know, how a convolution neural\nnetwork works lets break it into parts. the 3\nmost important parts of this convolution neural\nnetworks are,",
        "tables": [
          [
            [
              "In our project, we leveraged the power of teachable"
            ],
            [
              "machines to revolutionize crime detection and crowd"
            ],
            [
              "management. Harnessing the potential of artificial"
            ],
            [
              "intelligence, particularly in computer vision, we"
            ],
            [
              "employed advanced algorithms to analyze image"
            ],
            [
              "inputs, including CCTV videos, with unprecedented"
            ],
            [
              "precision. These CCTV videos have been converted"
            ],
            [
              "to images in .jpg format using an online tool, and"
            ],
            [
              "these converted images were used to train the model."
            ],
            [
              "The model employs a Convolutional Neural Network"
            ],
            [
              "(CNN) to classify the input video as either normal or"
            ],
            [
              "indicative of criminal activity. Furthermore, our"
            ],
            [
              "application extends beyond crime prevention to"
            ],
            [
              "crowd management, where the same technology"
            ],
            [
              "facilitates real-time monitoring and analysis of"
            ],
            [
              "crowded spaces. The teachable machines not only"
            ],
            [
              "enable us to identify potential security threats but"
            ],
            [
              "also provide valuable insights for optimizing public"
            ],
            [
              "safety strategies. This project represents a significant"
            ],
            [
              "step forward in the fusion of cutting-edge technology"
            ],
            [
              "and public safety, showcasing the potential of"
            ],
            [
              "artificial intelligence in creating safer and more"
            ],
            [
              "secure environments for communities."
            ]
          ],
          [
            [
              "Now to know, how a convolution neural"
            ],
            [
              "network works lets break it into parts. the 3"
            ],
            [
              "most important parts of this convolution neural"
            ],
            [
              "networks are,"
            ]
          ]
        ],
        "forms": {},
        "confidence_score": 1.0
      },
      {
        "page_number": 1,
        "text_content": "1. Convolution A. To enhance image recognition in a neural network,\n2. Pooling it's crucial to identify and focus on the most relevant\n3. Flattening features while discarding unnecessary pixel\ninformation. Convolution, despite its reputation as a\nConvolution complex topic, is a fundamental technique that\nConsider a 28x28 image, like those in the MNIST simplifies this process. In essence, convolution\ndataset used for handwritten digit recognition. In a involves sliding a filter, or kernel, across an image to\nbasic artificial neural network setup, each pixel's detect various features. Kernels, represented as 2D\nvalue is treated as an individual feature input, matrices with distinct weights, traverse the image,\nresulting in 784 input nodes. While this approach replacing pixel values with the weighted average of\nmay yield satisfactory results, it falls short in neighboring values.\nrecognizing crucial features within the image. The\nmodel essentially processes each pixel independently, B. This method allows us to pinpoint significant\npotentially missing important patterns. features in an image, providing a more nuanced\nunderstanding. By applying multiple randomly\nScaling this concept to a larger image, such as a generated kernels, we can extract diverse features,\n1920x1080 Ultra HD image, poses significant enriching the model's ability to recognize intricate\nchallenges. Applying the same methodology would patterns.\nresult in an impractical 2 million input nodes. Even C. Following the convolution layer, the next step\nwith a relatively modest hidden layer of 64 nodes, involves pooling these features. Pooling is a\nwhich is insufficient for such a large input, the technique used to condense and emphasize essential\nnetwork would involve a staggering 130 million information while reducing computational\nweights. This massive scale of parameters necessitates complexity. This ensures that the model retains\nan enormous computational load, overwhelming the crucial details while discarding redundant\ncapabilities of most machines. The sheer volume of information, contributing to improved image\ncalculations involved makes it unfeasible for effective recognition performance.\nimage recognition, emphasizing the need for more\nsophisticated approaches in handling high-resolution\nD. Pooling\nimages.\nAfter identifying crucial features through\nconvolution, the challenge remains in handling the\nlarge number of inputs, a task addressed by pooling.\nPooling serves to shrink the image dimensions while\nretaining the features uncovered during convolution.\nTake, for instance, the MaxPooling method, which\noperates on a matrix shape and outputs the\nmaximum value within that region. This technique\nallows for image compression without sacrificing the\nessential features, facilitating a more manageable\ninput size for the subsequent layers of the neural\nnetwork.\nE. Flattening\nFlattening is the process of transforming a 3D or 2D\nmatrix into a 1D format, serving as the final step in\npreparing the image for input into the model. This",
        "tables": [
          [
            [
              "A. To enhance image recognition in a neural network,"
            ],
            [
              "it's crucial to identify and focus on the most relevant"
            ],
            [
              "features while discarding unnecessary pixel"
            ],
            [
              "information. Convolution, despite its reputation as a"
            ],
            [
              "complex topic, is a fundamental technique that"
            ],
            [
              "simplifies this process. In essence, convolution"
            ],
            [
              "involves sliding a filter, or kernel, across an image to"
            ],
            [
              "detect various features. Kernels, represented as 2D"
            ],
            [
              "matrices with distinct weights, traverse the image,"
            ],
            [
              "replacing pixel values with the weighted average of"
            ],
            [
              "neighboring values."
            ],
            [
              "B. This method allows us to pinpoint significant"
            ],
            [
              "features in an image, providing a more nuanced"
            ],
            [
              "understanding. By applying multiple randomly"
            ],
            [
              "generated kernels, we can extract diverse features,"
            ],
            [
              "enriching the model's ability to recognize intricate"
            ],
            [
              "patterns."
            ],
            [
              "C. Following the convolution layer, the next step"
            ],
            [
              "involves pooling these features. Pooling is a"
            ],
            [
              "technique used to condense and emphasize essential"
            ],
            [
              "information while reducing computational"
            ],
            [
              "complexity. This ensures that the model retains"
            ],
            [
              "crucial details while discarding redundant"
            ],
            [
              "information, contributing to improved image"
            ],
            [
              "recognition performance."
            ]
          ],
          [
            [
              "",
              "1. Convolution",
              ""
            ],
            [
              "",
              "2. Pooling",
              ""
            ],
            [
              "",
              "3. Flattening",
              ""
            ],
            [
              "",
              "",
              ""
            ],
            [
              "Convolution",
              "",
              ""
            ],
            [
              "Consider a 28x28 image, like those in the MNIST",
              "",
              ""
            ],
            [
              "dataset used for handwritten digit recognition. In a",
              "",
              ""
            ],
            [
              "basic artificial neural network setup, each pixel's",
              "",
              ""
            ],
            [
              "value is treated as an individual feature input,",
              "",
              ""
            ],
            [
              "resulting in 784 input nodes. While this approach",
              "",
              ""
            ],
            [
              "may yield satisfactory results, it falls short in",
              "",
              ""
            ],
            [
              "recognizing crucial features within the image. The",
              "",
              ""
            ],
            [
              "model essentially processes each pixel independently,",
              "",
              ""
            ],
            [
              "potentially missing important patterns.",
              "",
              ""
            ],
            [
              "",
              "",
              ""
            ],
            [
              "Scaling this concept to a larger image, such as a",
              "",
              ""
            ],
            [
              "1920x1080 Ultra HD image, poses significant",
              "",
              ""
            ],
            [
              "challenges. Applying the same methodology would",
              "",
              ""
            ],
            [
              "result in an impractical 2 million input nodes. Even",
              "",
              ""
            ],
            [
              "with a relatively modest hidden layer of 64 nodes,",
              "",
              ""
            ],
            [
              "which is insufficient for such a large input, the",
              "",
              ""
            ],
            [
              "network would involve a staggering 130 million",
              "",
              ""
            ],
            [
              "weights. This massive scale of parameters necessitates",
              "",
              ""
            ],
            [
              "an enormous computational load, overwhelming the",
              "",
              ""
            ],
            [
              "capabilities of most machines. The sheer volume of",
              "",
              ""
            ],
            [
              "calculations involved makes it unfeasible for effective",
              "",
              ""
            ],
            [
              "image recognition, emphasizing the need for more",
              "",
              ""
            ],
            [
              "sophisticated approaches in handling high-resolution",
              "",
              ""
            ],
            [
              "images.",
              "",
              ""
            ],
            [
              "",
              "",
              ""
            ],
            [
              "",
              "",
              ""
            ]
          ],
          [
            [
              "",
              ""
            ],
            [
              "D. Pooling",
              ""
            ],
            [
              "After identifying crucial features through",
              ""
            ],
            [
              "convolution, the challenge remains in handling the",
              ""
            ],
            [
              "large number of inputs, a task addressed by pooling.",
              ""
            ],
            [
              "Pooling serves to shrink the image dimensions while",
              ""
            ],
            [
              "retaining the features uncovered during convolution.",
              ""
            ],
            [
              "Take, for instance, the MaxPooling method, which",
              ""
            ],
            [
              "operates on a matrix shape and outputs the",
              ""
            ],
            [
              "maximum value within that region. This technique",
              ""
            ],
            [
              "allows for image compression without sacrificing the",
              ""
            ],
            [
              "essential features, facilitating a more manageable",
              ""
            ],
            [
              "input size for the subsequent layers of the neural",
              ""
            ],
            [
              "network.",
              ""
            ],
            [
              "",
              ""
            ],
            [
              "E. Flattening",
              ""
            ],
            [
              "Flattening is the process of transforming a 3D or 2D",
              ""
            ],
            [
              "matrix into a 1D format, serving as the final step in",
              ""
            ],
            [
              "preparing the image for input into the model. This",
              ""
            ]
          ]
        ],
        "forms": {},
        "confidence_score": 1.0
      },
      {
        "page_number": 2,
        "text_content": "step involves converting the structured In the dynamic landscape of contemporary workplaces,\nrepresentation of the image into a linear, one- the need for efficient and effective work monitoring has\ndimensional input. The flattened data can then be become paramount. Organizations strive to optimize\nseamlessly connected to a fully connected dense layer, productivity, ensure employee safety, and maintain a\nfacilitating subsequent stages of classification in the secure work environment. The advent of machine learning\nneural network technologies has opened up new avenues for addressing\nthese challenges. This project report delves into the\n.\nimplementation of YOLO (You Only Look Once), a state-\nof-the-art object detection algorithm, as a pioneering\nsolution for work monitoring.\nYOLO(YOU ONLY LOOK ONCE) MODULE\nUnlike traditional object detection methods that involve\nmultiple stages, YOLO streamlines the process, allowing\nfor real-time detection with impressive speed and\naccuracy. Here's a step-by-step explanation of how the\nYOLO module works:\nLibraries used in this project: Input Processing:The input image undergoes grid-based\n\u27a2 OpenCV: Used to read the video input and division, forming the foundation for subsequent\nsplitting videointo frames for analysing. predictions.\n\u27a2 Keras: Used to implement neural networks. It\nis a high-level neural network library that\nruns on top of tensorflow\nBounding Box Prediction: YOLO predicts multiple\n\u27a2 Numpy: Used to process images as the image\nbounding boxes within each grid cell, each associated\npixel is in the form of matrix\nwith parameters (x, y) for the box's center, width (w),\n\u27a2 Pushbullet: It is an API used for sending SMS\nheight (h), confidence score, and class probabilities.\nto mobile phone after detecting crime.\nSample images used in Dataset: Class Prediction: YOLO determines the probability of\neach class for all bounding boxes in a grid cell, enabling\nsimultaneous detection of multiple object classes in a\ngiven image.\nConfidence Score: A confidence score indicates the\nmodel's certainty that a bounding box contains an object,\nwith a range from 0 to 1.\nNon-Maximum Suppression: Following predictions for\nall grid cells, a post-processing step, non-maximum\nsuppression, removes redundant and low-confidence\nbounding boxes, retaining only the most confident and\nnon-overlapping ones.\nInput &Output:\nThe input is a video that is being monitored for\npotential criminal activities. The output indicates\nOutput: The YOLO module produces a final output of\nwhether the video involves suspicious activities or not.\nbounding boxes, each linked to a class and a confidence\nIn the event of criminal behaviour, a notification is\nscore, representing the detected objects in the input image.\nsent to the relevant authorities.\nINTERSECTION OVER UNION\nII. OBJECT DETECTION\nA.EMPLOYEE MONITORING",
        "tables": [
          [
            [
              "step involves converting the structured"
            ],
            [
              "representation of the image into a linear, one-"
            ],
            [
              "dimensional input. The flattened data can then be"
            ],
            [
              "seamlessly connected to a fully connected dense layer,"
            ],
            [
              "facilitating subsequent stages of classification in the"
            ],
            [
              "neural network"
            ],
            [
              "."
            ],
            [
              ""
            ]
          ],
          [
            [
              "In the dynamic landscape of contemporary workplaces,"
            ],
            [
              "the need for efficient and effective work monitoring has"
            ],
            [
              "become paramount. Organizations strive to optimize"
            ],
            [
              "productivity, ensure employee safety, and maintain a"
            ],
            [
              "secure work environment. The advent of machine learning"
            ],
            [
              "technologies has opened up new avenues for addressing"
            ],
            [
              "these challenges. This project report delves into the"
            ],
            [
              "implementation of YOLO (You Only Look Once), a state-"
            ],
            [
              "of-the-art object detection algorithm, as a pioneering"
            ],
            [
              "solution for work monitoring."
            ],
            [
              "YOLO(YOU ONLY LOOK ONCE) MODULE"
            ],
            [
              "Unlike traditional object detection methods that involve"
            ],
            [
              "multiple stages, YOLO streamlines the process, allowing"
            ],
            [
              "for real-time detection with impressive speed and"
            ],
            [
              "accuracy. Here's a step-by-step explanation of how the"
            ],
            [
              "YOLO module works:"
            ],
            [
              "Input Processing:The input image undergoes grid-based"
            ],
            [
              "division, forming the foundation for subsequent"
            ],
            [
              "predictions."
            ],
            [
              "Bounding Box Prediction: YOLO predicts multiple"
            ],
            [
              "bounding boxes within each grid cell, each associated"
            ],
            [
              "with parameters (x, y) for the box's center, width (w),"
            ],
            [
              "height (h), confidence score, and class probabilities."
            ],
            [
              "Class Prediction: YOLO determines the probability of"
            ],
            [
              "each class for all bounding boxes in a grid cell, enabling"
            ],
            [
              "simultaneous detection of multiple object classes in a"
            ],
            [
              "given image."
            ],
            [
              "Confidence Score: A confidence score indicates the"
            ],
            [
              "model's certainty that a bounding box contains an object,"
            ],
            [
              "with a range from 0 to 1."
            ],
            [
              "Non-Maximum Suppression: Following predictions for"
            ],
            [
              "all grid cells, a post-processing step, non-maximum"
            ],
            [
              "suppression, removes redundant and low-confidence"
            ],
            [
              "bounding boxes, retaining only the most confident and"
            ],
            [
              "non-overlapping ones."
            ],
            [
              "Output: The YOLO module produces a final output of"
            ],
            [
              "bounding boxes, each linked to a class and a confidence"
            ],
            [
              "score, representing the detected objects in the input image."
            ],
            [
              "INTERSECTION OVER UNION"
            ]
          ]
        ],
        "forms": {},
        "confidence_score": 1.0
      },
      {
        "page_number": 3,
        "text_content": "Intersection over Union (IoU) is a metric used to evaluate\nthe accuracy of an object detection algorithm, particularly 3)Uses Microsoft azure cognitive services and cloud\nin tasks such as image segmentation and bounding box system for implementation.Provides a comparative study\nprediction. IoU measures the overlap between the of traditional methodologies used such as Haar Cascade.[\npredicted bounding box and the ground truth bounding Proposed System for Criminal Detection and Recognition\nbox for a given object in an image.The IoU is calculated on CCTV Data Using Cloud and Machine Learning]\nas the ratio of the area of intersection between the\npredicted and ground truth bounding boxes to the area of Samit Shirsat, Aakash Naik, Darshan Tamse\ntheir union. The formula for IoU is:\n4) Uses pre trained deep leaning model VGGNet-19 which\ndetects gun and knife.Uses SMS sending module to send\nIOU= Area of Intersection/Area of Union alert.[ Crime Intention Detection System Using Deep\nLearning]\nHere's a breakdown of the terms: Umadevi V Navalgund, Priyadharshini.K\n1. Area of Intersection:The region where the 5) Focuses on identifying patterns and trends in crime\npredicted bounding box and the ground truth bounding occurrences.Uses ML and DL algorithms to predict crime\nbox overlap. related activities.[ Crime Prediction Using Machine\nLearning and Deep Learning: A Systematic Review and\nFuture Directions]\n2. Area of Union:The combined region covered by\nboth the predicted bounding box and the ground truth VarunMandalapu , Lavanya Elluri\nbounding box.\nThe IoU value ranges from 0 to 1, where:\nIV. Methodology\nIoU=0 indicates no overlap between the predicted and\nground truth bounding boxes. EMPLOYEE MONITORING\nIn the dynamic landscape of contemporary workplaces,\nthe need for efficient and effective work monitoring has\nIoU=1 indicates a perfect overlap between the predicted\nbecome paramount. Organizations strive to optimize\nand ground truth bounding boxes.\nproductivity, ensure employee safety, and maintain a\nsecure work environment. The advent of machine learning\ntechnologies has opened up new avenues for addressing\nthese challenges. This project report delves into the\nimplementation of YOLO (You Only Look Once), a state-\nof-the-art object detection algorithm, as a pioneering\nIII. LITERATURE SURVEY solution for work monitoring.\n1) Uses OpenCV for object detection in computer\nYOLO(YOU ONLY LOOK ONCE) MODULE\nVision.LSTM (Long Short-Term Memory) is used to\nclassify any event or behaviour as a crime or not.\n[Autonomous Anomaly Detection System for Crime\nUnlike traditional object detection methods that involve\nMonitoring and Alert Generation]\nmultiple stages, YOLO streamlines the process, allowing\nfor real-time detection with impressive speed and\nJyoti Kukad, Swapnil Soner, Sagar Pandya\naccuracy. Here's a step-by-step explanation of how the\nYOLO module works:\n2) Uses state-of-the-art face identification system\nUses deepneural networks (DNN).\nInput Processing:The input image undergoes grid-based\n[Face Detection and Recognition for Criminal\ndivision, forming the foundation for subsequent\nIdentification System]\npredictions.\nSanika Tanmay, Aamani Tandasi, Shipra Saraswat",
        "tables": [
          [
            [
              "Intersection over Union (IoU) is a metric used to evaluate"
            ],
            [
              "the accuracy of an object detection algorithm, particularly"
            ],
            [
              "in tasks such as image segmentation and bounding box"
            ],
            [
              "prediction. IoU measures the overlap between the"
            ],
            [
              "predicted bounding box and the ground truth bounding"
            ],
            [
              "box for a given object in an image.The IoU is calculated"
            ],
            [
              "as the ratio of the area of intersection between the"
            ],
            [
              "predicted and ground truth bounding boxes to the area of"
            ],
            [
              "their union. The formula for IoU is:"
            ],
            [
              "IOU= Area of Intersection/Area of Union"
            ],
            [
              "Here's a breakdown of the terms:"
            ],
            [
              "1. Area of Intersection:The region where the"
            ],
            [
              "predicted bounding box and the ground truth bounding"
            ],
            [
              "box overlap."
            ],
            [
              "2. Area of Union:The combined region covered by"
            ],
            [
              "both the predicted bounding box and the ground truth"
            ],
            [
              "bounding box."
            ],
            [
              "The IoU value ranges from 0 to 1, where:"
            ],
            [
              "IoU=0 indicates no overlap between the predicted and"
            ],
            [
              "ground truth bounding boxes."
            ],
            [
              "IoU=1 indicates a perfect overlap between the predicted"
            ],
            [
              "and ground truth bounding boxes."
            ]
          ],
          [
            [
              "In the dynamic landscape of contemporary workplaces,"
            ],
            [
              "the need for efficient and effective work monitoring has"
            ],
            [
              "become paramount. Organizations strive to optimize"
            ],
            [
              "productivity, ensure employee safety, and maintain a"
            ],
            [
              "secure work environment. The advent of machine learning"
            ],
            [
              "technologies has opened up new avenues for addressing"
            ],
            [
              "these challenges. This project report delves into the"
            ],
            [
              "implementation of YOLO (You Only Look Once), a state-"
            ],
            [
              "of-the-art object detection algorithm, as a pioneering"
            ],
            [
              "solution for work monitoring."
            ],
            [
              "YOLO(YOU ONLY LOOK ONCE) MODULE"
            ],
            [
              "Unlike traditional object detection methods that involve"
            ],
            [
              "multiple stages, YOLO streamlines the process, allowing"
            ],
            [
              "for real-time detection with impressive speed and"
            ],
            [
              "accuracy. Here's a step-by-step explanation of how the"
            ],
            [
              "YOLO module works:"
            ],
            [
              "Input Processing:The input image undergoes grid-based"
            ],
            [
              "division, forming the foundation for subsequent"
            ],
            [
              "predictions."
            ]
          ]
        ],
        "forms": {},
        "confidence_score": 1.0
      },
      {
        "page_number": 4,
        "text_content": "Bounding Box Prediction: YOLO predicts multiple Upon capturing an input image indicative of theft or\nbounding boxes within each grid cell, each associated criminal activity, the system triggers an alert mechanism.\nwith parameters (x, y) for the box's center, width (w), This mechanism not only highlights the suspicious event\nheight (h), confidence score, and class probabilities. but also sends an immediate alert message to designated\nauthorities. The integration of heatmap visualization\nenhances the alert system by providing a visual\nClass Prediction: YOLO determines the probability of representation of the anomaly, allowing authorities to\neach class for all bounding boxes in a grid cell, enabling swiftly assess the situation and respond effectively.\nsimultaneous detection of multiple object classes in a\ngiven image. One of the project's standout features is the seamless\nintegration of heatmap visualization. This graphical\nrepresentation method offers a clear and intuitive display\nof numerical data, indicating the intensity of activities\nConfidence Score: A confidence score indicates the\nwithin the monitored space. In the context of work\nmodel's certainty that a bounding box contains an object,\nmonitoring and crime detection, the heatmap becomes a\nwith a range from 0 to 1.\npowerful tool, showcasing the concentration and\ndistribution of work hours and identifying anomalies that\nmay indicate criminal behavior.\nNon-Maximum Suppression: Following predictions for\nall grid cells, a post-processing step, non-maximum\nsuppression, removes redundant and low-confidence\nbounding boxes, retaining only the most confident and\nnon-overlapping ones.\nVI. Conclusion\nOutput: The YOLO module produces a final output of This project marks a significant advancement in the\nbounding boxes, each linked to a class and a confidence convergence of work monitoring and crime detection,\nscore, representing the detected objects in the input image. offering a holistic solution that promotes both workplace\nefficiency and security. The synergy between advanced\nalgorithms, specialized datasets, and heatmap\nvisualization sets this system apart, exemplifying the\npotential of technology to revolutionize surveillance and\nsafety measures in various domains. At the core of the\nV. RESULTS AND DISCUSSIONS\nsystem lies the utilization of sophisticated AI/ML\nalgorithms, particularly the YOLO model, to\nsimultaneously monitor work activities and detect\nIn the realm of advanced surveillance systems, the\ncriminal incidents. The YOLO model, renowned for its\nintegration of work monitoring and crime detection has\nefficiency in object detection, ensures precise tracking of\nreached new heights, offering a comprehensive solution to\nindividuals and objects within the monitored space. The\nenhance security measures. This innovative project\nproject's specialized dataset focuses on capturing both\nleverages cutting-edge technologies, merging work\nwork-related scenarios and criminal activities, enabling\nmonitoring outputs with crime detection capabilities,\nthe model to distinguish between routine work tasks and\nultimately contributing to a safer and more efficient\npotential thefts.\nenvironment.\n[3] Proposed System for Criminal Detection and\nVII. Reference Recognition on CCTV Data Using Cloud and Machine\nLearning\nSamit Shirsat, Aakash Naik, Darshan Tamse\n[4] Crime Intention Detection System Using Deep\n[1] Autonomous Anomaly Detection System for Crime\nLearning\nMonitoring and Alert Generation\nUmadevi V Navalgund, Priyadharshini.K\nJyoti Kukad, Swapnil Soner, Sagar Pandya\n[5] Crime Prediction Using Machine Learning and Deep\n[2] Face Detection and Recognition for Criminal\nLearning: A Systematic Review and Future Directions\nIdentification System\nVarun Mandalapu , Lavanya Elluri\nSanika Tanmay, Aamani Tandasi, Shipra Saraswat",
        "tables": [
          [
            [
              "Bounding Box Prediction: YOLO predicts multiple"
            ],
            [
              "bounding boxes within each grid cell, each associated"
            ],
            [
              "with parameters (x, y) for the box's center, width (w),"
            ],
            [
              "height (h), confidence score, and class probabilities."
            ],
            [
              "Class Prediction: YOLO determines the probability of"
            ],
            [
              "each class for all bounding boxes in a grid cell, enabling"
            ],
            [
              "simultaneous detection of multiple object classes in a"
            ],
            [
              "given image."
            ],
            [
              "Confidence Score: A confidence score indicates the"
            ],
            [
              "model's certainty that a bounding box contains an object,"
            ],
            [
              "with a range from 0 to 1."
            ],
            [
              "Non-Maximum Suppression: Following predictions for"
            ],
            [
              "all grid cells, a post-processing step, non-maximum"
            ],
            [
              "suppression, removes redundant and low-confidence"
            ],
            [
              "bounding boxes, retaining only the most confident and"
            ],
            [
              "non-overlapping ones."
            ],
            [
              "Output: The YOLO module produces a final output of"
            ],
            [
              "bounding boxes, each linked to a class and a confidence"
            ],
            [
              "score, representing the detected objects in the input image."
            ]
          ]
        ],
        "forms": {},
        "confidence_score": 1.0
      }
    ]
  },
  {
    "file_path": "pdfs\\4th paper.pdf",
    "total_pages": 6,
    "combined_text": "495\nINTERNATIONAL JOURNAL OF SCIENTIFIC & ENGINEERING RESEARCH, VOLUME 12, ISSUE 1, JANUARY-2021\nI SSN 2229-5518\nCRIMINAL ACTIVITY MONITORING AND\nPREVENTION USING CCTV SURVEILLANCE\nSumaira Hedaoo, Riddhi Shanbhag, Sakshi Sonalkar , Sayam Tukra, Prof. Shailesh Hule\nAbstract\u2014 In this paper, we describe a surveillance program which is to be designed that can automatically detect the gestures or signs of\naggression and brutality on real time. A single CCTV human operator can handle very limited set of operations, so as the number of CCTV\ncamera increases the need of the human intervention also increases which can also cause human errors so automation for certain detection\noperations is necessary. Basically, our proposed system consists of 2 main modules which are capable of detecting the actions of objectional\nobjects and humans in the frame for example gun and knife. Here in this project, we propose algorithms which are able to make the people\nattentive about (1) Presence of a any hazardous act, the danger is detected when the objectional object appears in the frame with the\npresence of human. (2) An abnormal activity of human when they be handling the weapons or acts of assaults, In this project, we focus to\nallow the real time application, we completely focus on reducing the number of negative alarms.\nIndex Terms\u2014 Objectional Objects, CCTV Surveillance, Gun, knife, alarm, abnormal activities.\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014 \u25c6 \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\n1.INTRODUCTION\nCrime and theft are the social irritation for the society. It has\nmajor contribution to the crime rate in the world. To overcome So, in the continuation it responses to the trigger with an alert or\nsuch situations of crimes recently CCTVS are being used. Lately buzzer noise which helps in scenarios to escalate further. So, this\nhere has been a sustainable growth in the use of CCTV system can be validated for ensuring the security privacy and\nIJSER\nsurveillance cameras in order to prevent criminal activities. In confidentiality. There is no human intervention. The CCTV\nbetween the public expansion, as well the concerns for stabbing camera is inactive when there is no human presence. Basically, the\nand getting robbed, there is an alarming need for a proof -based human presence is checked by the motion detection algorithms.\napproach with the CCTV Services. Cases of harassment in public When there is any criminal activity, there is an alarm or buzzer\nplaces are also becoming very significant. With the growing that will ring that is connected to the main system.\ninsertion of CCTV Cameras surveillances, almost now every area\ncan be monitored, so through this if any crimes are committed\nthe vital evidences can be presented from the crime scene. It is 2.EXISTING SYSTEM\nvery essential to expect an alert or buzzer system for the ongoing The existing system of the CCTV surveillance is done by the\n/to be happen accidents and criminal activities, where certain human operator and also the automated system of CCTV\nactions should be taken on time as it can be question of life and surveillance is not that accurate for making decision and give\ndeath situation. Such structure is to be identified and monitored responses as per the bizarre situation.\nby live footages. But as the number of CCTVS per unit are rising, Normally monitoring in dynamic situations is mainly to\nthe personnel viewing experience by a human operator is not recognize, detect and locate the certain thing or object from the\npossible. So, we require a particular surveillance unit which are provided image and moreover to understand the object behavior.\ncapable of flourishing these situations with minimum human So, our main objective is to develop an intelligent Monitoring\nintervention. Model system and can take over the existing passive CCTV\nWe shall define a \u201cscenario of need\u201d or \u201cvital scenario\u201d as any surveillance system which proves to be inefficient and ineffective\nsensitive scenario that can lead to any of mishaps in the public as, the number of CCTV increase the number of human operator\nplaces. We consider the idea of smart supervision which is or human intervention to observe the system also increases.\nactivated only when there is the movement in room. Furthermore, if we are placing the cameras as human eyes in\nSo this means ,at all other times the surveillance is not active so certain organization then the main aim of the surveillance is to\nthat it helps in keeping the privacy of the person and also hide accomplish the task to be as automated as possible.\ncertain confidentiality of work. But, the traditional surveillance Visual Monitoring the dynamic scenarios has a huge range of\ntechnically can be more risky as it does not hide any private potential applications such as a for the Guards, traffic, building in\ninformation and does not follow any privacy concerns. city expressways Detection of objects and human activities in\nSo, the \u201cvideo feed\u201d would be recorded only when there is a Mall, Airways etc.\n\u201cneed of hour\u201d in case there need to be any evidence for crime In this paper we focus on monitoring of people and objects in the\ncommitted. full range in the frame.\nWe also focus on detecting the knives and pistols these objectional\nobjects which are mainly overseen in the existing passive CCTV\nIJSER \u00a9 2021\nhttp://www .ijser.org\n496\nINTERNATIONAL JOURNAL OF SCIENTIFIC & ENGINEERING RESEARCH, VOLUME 12, ISSUE 1, JANUARY-2021\nI SSN 2229-5518\nsystem.\nMonitoring applications which involve people include the\nfollowing: -\na) Access control in special areas: People with certain specific\nidentity or authorized person are allowed to enter in private or\nsecurity centered locations such as certain government office and\nmilitary camps. Certain security system has biometric features\nwhich helps in deciding the difference between authorized and\nnon-authorized person which basically gets recorded in\nbiometric database.\nWhen someone is about to enter the biometric centered place then\nthe system automatically records the persons features, it\ncompares the captured biometric with the saved ones and decide\nwhether or not the person is allowed to enter the place.\nb) Person-specific identification in certain scenes: The security\nguards /police can catch criminals or suspects with the assistance\nof personal identification at a distance by a smart surveillance\nsystem. The police database may contain or store new suspect\nbiometric and this can be obtained using visual CCTV system\nfrom the places where the suspect is usually seen as casinos and\nsubway stations etc. So, after this the system is automatically\nenable to recognize whether or not the person in view are\nsuspects. If yes, then the alarms will be given immediately. Even\nthough such systems with faIce recoJgnition havSe already been ER\nused at public sites, their reliability is too low for police\nrequirements.\nc) Crowd flux statistics and congestion analysis: The flux of\npeople at important public areas such as stores, can be\nDrawbacks\nautomatically computed by the surveillance systems, using\n\u2022 The passive system is used only in the military bases but\ntechniques for human detection. It can then provide congestion\nnot metro stations, airports and mall etc.\nanalysis to assist in the management of the people. Similarly,\n\u2022 It is a complex architecture. In passive system there is\nexpressways and junctions of the roads can be monitored\nnot individual safety level considered.\nthrough visual surveillance systems, and further analyze the\n\u2022 Basically, the video in passive system is captured\nstatus of road congestion and traffic.\ncontinuously which can lead to privacy issue.\n\u2022 Also, the system does not have efficient alarm system\nd) Ambiguity detection and alarming: At sometimes, it is very\nthat responses in return of any act of abuse or bullying.\nvital to analyze the behavior or characteristics of people and to\n\u2022 There is nor blurring out of the sensitive information in\ncheck whether they are normal or abnormal. For example,\npresent system.\nabnormal behaviors indicative of theft, can be analyzed using the\nvisual surveillance systems, which can be placed in supermarkets\nand parking lots. Basically, there are so many ways of giving alert\nsuch as by ringing the buzzer. One way is to make recorded\ndeclaration automatically whether certain abnormal behavior is\ndetected.\ne) Face detection and Object Detection: Convolutional neural\nnetworks are widely used in addressing image-based problems,\nsuch as object/character detection and face recognition. In this\npaper we will be Using Faster-RCNN Because its Faster Than the\ntraditional CNN and also saves time as it detects the things\nregion-wise and searches only the region that is needed rather\nthan searching for all the regions marked in the image as the\nCNN does.\nIJSER \u00a9 2021\nhttp://www .ijser.org\n497\nINTERNATIONAL JOURNAL OF SCIENTIFIC & ENGINEERING RESEARCH, VOLUME 12, ISSUE 1, JANUARY-2021\nI SSN 2229-5518\n3.PROPOSED SYSTEM respective authorities about the criminal activity taking place\nThe main aim of our project is to detect the criminal activities and they take instant action.\nwith the utmost accuracy that take place in the public areas\nthrough the CCTV that take place in the public areas. Our current\nmodel consists of 2 detection functions, one for detecting crimes\nwhich are done when there are little movements detected (e.g.,\nRobbery or people armed with weapons) and another one for\ndetecting crimes that committed with the heavy or large\nmovements (e.g., stabbing). To avoid the negative/false alarms\ndetection, we have executed a system where the already stored\ndetections are all cleared and only the recently detected.\nOther crime prediction system software perform certain citizen\ntrustworthiness analyzes which is basically based on data that is\nprovided or obtained from organization such as police, hospital,\nschool, banks and from social media. But this solution we believe\nthat such a system can potentially be prone to certain\ndiscriminations against certain discrimination against certain\nsituation that may involve in any crime.\nIn our particular solution, there is no such social credit system\nthat keeps permanent log of all the activity, So in this there is no\nneed of continuous monitoring and assessment. Instead of this\nMachine Learning is used to detect the crime or criminal activity\nand react to the scenario by responding or alerting user or\nrelevant authority. IJSER\nThe flowchart here explains the flow of our system\nWhenever there is any action in the room the CCTV surveillance\ngets active.\n4. METHODOLOGY\n1)If there is any \u201csituation of concern\u201d then the CCTV cam is\nIn this paper we will be using certain algorithms to determine\ndirected towards it and the frame is captured.\nthe human and object detection in the surveillance.\nFor Detection system we are using faster R-CNN deep learning\n2)Now the algorithms detect if the frame has a knife or a pistol. If\nalgorithm. As in traditional CNN we take the image then we\nthe frame detects the knife then optical flow algorithm calculates\ndivide into regions. we will then consider each as separate image\nthe velocity at which the knife is being moved from one frame to\nthen pass all these regions to CNN as classify them into various\nthe other. It then calculates the probability of stabbing by which\nclasses. Once every region is divided into its certain classes, then\nintensity the process is taking place.\nwe combine all the regions to get original image with the detected\nobjects. We pass an object to the network and it then goes through\n3)If the probability taken is greater than the threshold value that\nvarious loops and pooling layers and then we get the output from\nis being set by us. The threshold needs to be set reasonably low\nthe object class. We basically use R-CNN as it implements\nto ensure that the tip of knife is detected as a corner, even at the\nselective search to extract a bunch of regions in the image rather\nexpense of more corners being detected. Basically, the range is\nthan the massive number of regions to check if any of these boxes\nfrom 0 to 255. Here the threshold set is 204 as low as possible,\ncontain any object that we require for our process. Process for R-\nwhen we set 204 as threshold it detects the knife in the frame with\nCNN is as follows: -\nthe utmost accuracy.\n1)It takes the images\n2)It generates initial sub segmentation so that multiple images\n4)Then when the knife is detected with much accuracy then the\nfrom the original image\nthere is an alert that is the buzzer goes on. After which the\n3)The technique then combines certain same and relatable regions\nrespective authorities are informed about the criminal activity\nto form a layer region which is based on color, similarity, texture\ntaking place and they can take instant action.\nsimilarity, size similarity and shape compatibility.\nBut this algorithm has certain drawback that are overcome by\n5)If the knife is not detected then the surveillance is continued.\nFaster-RCNN. The slowest part in R-CNN in selective search or\nNow, secondly if the pistol is detected in the given frame with a\nEdge boxes. Faster RCNN replaces selective search with a very\nhuman handling the object then the buzzer goes on and alert the\nsmall convolution network called Region Proposal network to\nIJSER \u00a9 2021\nhttp://www .ijser.org\n498\nINTERNATIONAL JOURNAL OF SCIENTIFIC & ENGINEERING RESEARCH, VOLUME 12, ISSUE 1, JANUARY-2021\nI SSN 2229-5518\ngenerate regions of Interest. Faster R-CNN introduces idea of\nanchor.\nSecondly, for Motion Estimation we earlier used Open Pose\nlibrary it created problems when the knife detector is integrated\nit hardly distinguished between human in the frame and who is\nholding the knife. Looking for alternative we found Motion\nestimation using Optical Flow to determine average speed of\nhuman Optical flow is basically a motion of object between\nconsecutive frame of sequences, caused by the relative movement\nbetween object and camera. It is basically 2D vector field where\neach vector is a displacement vector showing movement of\npoints from first frame to second. So Basically, for our project it\nis necessary as we have a knife and a person if person makes\ncertain movement with the knife it should be captured instantly.\nFig 4: Events Center\nIt works on several assumptions\n1)The intensities of pixel for the item do not change between the\nframe that are one after the other\n2)The pixels that are present nearby have similar motion.\nLastly, for very low levels of lightning can render the knife\nundetectable by our algorithm, hence we are using gamma\ncorrection to try and increase the brightness of frame gamma\ncorrection also blurs the picture so we need to find a good level\nof gamma correction such that our object is not blurred.\nIJSER\n5.RESULTS AND DISCUSSION\nIn this paper, the result is the final design of the project on topic\nFig 5: Camera Feed for Gun Detection\n\u201cCriminal Activity Monitoring and Prevention Using CCTV\nSurveillance\u201d.\nSince there it has intuitive GUI which displays the feeds and a\nevent manager tab to store the images of the detected events.\nWhenever there is a \u201csituation of need\u201d then the CCTV is active\nand captures the situation and responses accordingly. Here,\nalgorithms are designed such that it can alert the respective\nauthorities when there is a presence of any dangerous act, or an\nabnormal behavior of a person is detected. This can assure\nsecurity to the public in the public places as well as in other\nlocations such as offices, cinema halls etc. The major advantage\nof the project includes efficiency, fast to access and uniqueness.\nThe behavioral analysis algorithm also makes it easier to monitor\nand prevent the crimes.\nFig 5: Event Center\n6.CONCLUSION\nSurveillance by using the CCTV system has reached its great\nheights. Also, whenever we will be sending the information or\ndata through the networks to any server the transmission process\nis a very crucial work.\nCCTV surveillance systems are mostly managed by governments\nprofessional. As CCTV Information are very sensitive and\nconfidential and also very difficult to handle. In this project, we\nhave proposed certain algorithms that are able to alert the\nFig 3: Camera feed for knife detection respective authorities if any abnormal behavior of a person is\ndetected. We have limited the number of negative alarms in order\nto allow for a real-time working of the system to process well. This\nIJSER \u00a9 2021\nhttp://www .ijser.org\n499\nINTERNATIONAL JOURNAL OF SCIENTIFIC & ENGINEERING RESEARCH, VOLUME 12, ISSUE 1, JANUARY-2021\nI SSN 2229-5518\nproposed system we will implement first implement it at low T.A.P., 2013, Smart Attendance using Real Time Face Recognation\nscale then further we can escalate things to higher (Smart-FR), SAITM Research Symposium on Engineering\nimplementation. In the future, we will enhance the proposed Advancement, Sri Lanka.\nsystem by implementing the night vision surveillance using the\nInfrared image enhancement. So that our project progress well\nand also gives us more coverage to handle the situation at night.\n7.REFERENCES\n[1] Choi Woo Chul and Na Joon Yeop, \u201cRelative Importance for\nCrime Prevention Technologies as Part of Smart City based on\nSpatial Information\u201d, IEEE Journal, Smart Cities Symposium\nPrague, 13 July 2017\n[2] Robin Singh Sidhu and Mrigank Sharad, \u201cSmart Surveillance\nSystem for Detecting Interpersonal Crime\u201d, presented in\nInternational Conference on Communication and Signal\nProcessing, April 6-8, 2016, published in IEEE Journal, 24\nNovember 2016\n[3] Huang J, Rathod V, Sun C, Zhu M, Korattikara A, Fathi A,\nFischer I, Wojna Z, Song Y, Guadarrama S, Murphy K,\n\"Speed/accuracy trade-offs for modern convolutional object\ndetectors\", CVPR 2017\nIJSE R\n[4] Klima M., Pazderak J., Bernas M., Pata J., Hozman J., Roubik\nK., Objective and Subjective Image Quality Evaluation for\nSecurity Purposes, 35th IEEE International Carnahan Conference\non Security Technology, London, U.K.\n[5] Padaruth, S., Indiwarsingh, F. & Bhugun, N., 2013, A Unified\nIntrusion Alert System using Motion Detection and Faces\nRecognition, 2nd International Conference on Mechine Learning\nand Computer Science (IMLCS), Kuala Lumpur.\n[6] Namrata, Pradeep & Sagar, R., 2013. Cognitive Security\nSystem Based on Image Comparison and Motion Detection with\nAble Memory Usage. International Journal of Advances in\nEngineering & Technology, VI(2), pp.850-61.\n[7] Putro, M.D., Adji, T.B. & Winduratna, B., 2012, Sistem Deteksi\nWajah dengan Menggunakan Metode Viola-Jones, Proseding\nSeminar Nasional \"Science, Engineering and Technology\".\nMalang.\n[8] Santoso, H. & Harjoko, A., 2013. Haar Cascade Classifier dan\nAlgoritma Adaboost untuk Deteksi Banyak Wajah dalam Ruang\nKelas. Jurnal Teknologi, VI(2), pp.108-15\n[9] Febrianto, A.J., 2012, Pengenalan Wajah Dengan Metode\nPrinciple Component Analysis (PCA) Pada sistem Absensi Real\nTime, Tesis, Magister Teknik Elektro, Universitas Gadjah Mada,\nYogyakarta.\n[10] Tharanga, J.G.R., Samarakoon, S.M.C. & Karunarathne,\nIJSER \u00a9 2021\nhttp://www .ijser.org\n500\nINTERNATIONAL JOURNAL OF SCIENTIFIC & ENGINEERING RESEARCH, VOLUME 12, ISSUE 1, JANUARY-2021\nI SSN 2229-5518\nIJSER\nIJSER \u00a9 2021\nhttp://www .ijser.org",
    "metadata": {
      "Author": "Sumaira Hedaoo, Riddhi Shanbhag, Sakshi Sonalkar , Sayam Tukra, Prof. Shailesh Hule",
      "CreationDate": "D:20210120212821+05'30'",
      "Creator": "Microsoft\u00ae Word 2019",
      "Keywords": "Objectional Objects, CCTV Surveillance, Gun, knife, alarm, abnormal activities",
      "ModDate": "D:20210125115206+05'30'",
      "Producer": "Microsoft\u00ae Word 2019",
      "Subject": "International Journal of Scientific & Engineering Research Volume 12, Issue 1, January-2021\r\n",
      "Title": "CRIMINAL ACTIVITY MONITORING AND PREVENTION USING CCTV SURVEILLANCE"
    },
    "error_message": null,
    "pages": [
      {
        "page_number": 0,
        "text_content": "495\nINTERNATIONAL JOURNAL OF SCIENTIFIC & ENGINEERING RESEARCH, VOLUME 12, ISSUE 1, JANUARY-2021\nI SSN 2229-5518\nCRIMINAL ACTIVITY MONITORING AND\nPREVENTION USING CCTV SURVEILLANCE\nSumaira Hedaoo, Riddhi Shanbhag, Sakshi Sonalkar , Sayam Tukra, Prof. Shailesh Hule\nAbstract\u2014 In this paper, we describe a surveillance program which is to be designed that can automatically detect the gestures or signs of\naggression and brutality on real time. A single CCTV human operator can handle very limited set of operations, so as the number of CCTV\ncamera increases the need of the human intervention also increases which can also cause human errors so automation for certain detection\noperations is necessary. Basically, our proposed system consists of 2 main modules which are capable of detecting the actions of objectional\nobjects and humans in the frame for example gun and knife. Here in this project, we propose algorithms which are able to make the people\nattentive about (1) Presence of a any hazardous act, the danger is detected when the objectional object appears in the frame with the\npresence of human. (2) An abnormal activity of human when they be handling the weapons or acts of assaults, In this project, we focus to\nallow the real time application, we completely focus on reducing the number of negative alarms.\nIndex Terms\u2014 Objectional Objects, CCTV Surveillance, Gun, knife, alarm, abnormal activities.\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014 \u25c6 \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\n1.INTRODUCTION\nCrime and theft are the social irritation for the society. It has\nmajor contribution to the crime rate in the world. To overcome So, in the continuation it responses to the trigger with an alert or\nsuch situations of crimes recently CCTVS are being used. Lately buzzer noise which helps in scenarios to escalate further. So, this\nhere has been a sustainable growth in the use of CCTV system can be validated for ensuring the security privacy and\nIJSER\nsurveillance cameras in order to prevent criminal activities. In confidentiality. There is no human intervention. The CCTV\nbetween the public expansion, as well the concerns for stabbing camera is inactive when there is no human presence. Basically, the\nand getting robbed, there is an alarming need for a proof -based human presence is checked by the motion detection algorithms.\napproach with the CCTV Services. Cases of harassment in public When there is any criminal activity, there is an alarm or buzzer\nplaces are also becoming very significant. With the growing that will ring that is connected to the main system.\ninsertion of CCTV Cameras surveillances, almost now every area\ncan be monitored, so through this if any crimes are committed\nthe vital evidences can be presented from the crime scene. It is 2.EXISTING SYSTEM\nvery essential to expect an alert or buzzer system for the ongoing The existing system of the CCTV surveillance is done by the\n/to be happen accidents and criminal activities, where certain human operator and also the automated system of CCTV\nactions should be taken on time as it can be question of life and surveillance is not that accurate for making decision and give\ndeath situation. Such structure is to be identified and monitored responses as per the bizarre situation.\nby live footages. But as the number of CCTVS per unit are rising, Normally monitoring in dynamic situations is mainly to\nthe personnel viewing experience by a human operator is not recognize, detect and locate the certain thing or object from the\npossible. So, we require a particular surveillance unit which are provided image and moreover to understand the object behavior.\ncapable of flourishing these situations with minimum human So, our main objective is to develop an intelligent Monitoring\nintervention. Model system and can take over the existing passive CCTV\nWe shall define a \u201cscenario of need\u201d or \u201cvital scenario\u201d as any surveillance system which proves to be inefficient and ineffective\nsensitive scenario that can lead to any of mishaps in the public as, the number of CCTV increase the number of human operator\nplaces. We consider the idea of smart supervision which is or human intervention to observe the system also increases.\nactivated only when there is the movement in room. Furthermore, if we are placing the cameras as human eyes in\nSo this means ,at all other times the surveillance is not active so certain organization then the main aim of the surveillance is to\nthat it helps in keeping the privacy of the person and also hide accomplish the task to be as automated as possible.\ncertain confidentiality of work. But, the traditional surveillance Visual Monitoring the dynamic scenarios has a huge range of\ntechnically can be more risky as it does not hide any private potential applications such as a for the Guards, traffic, building in\ninformation and does not follow any privacy concerns. city expressways Detection of objects and human activities in\nSo, the \u201cvideo feed\u201d would be recorded only when there is a Mall, Airways etc.\n\u201cneed of hour\u201d in case there need to be any evidence for crime In this paper we focus on monitoring of people and objects in the\ncommitted. full range in the frame.\nWe also focus on detecting the knives and pistols these objectional\nobjects which are mainly overseen in the existing passive CCTV\nIJSER \u00a9 2021\nhttp://www .ijser.org",
        "tables": [],
        "forms": {},
        "confidence_score": 1.0
      },
      {
        "page_number": 1,
        "text_content": "496\nINTERNATIONAL JOURNAL OF SCIENTIFIC & ENGINEERING RESEARCH, VOLUME 12, ISSUE 1, JANUARY-2021\nI SSN 2229-5518\nsystem.\nMonitoring applications which involve people include the\nfollowing: -\na) Access control in special areas: People with certain specific\nidentity or authorized person are allowed to enter in private or\nsecurity centered locations such as certain government office and\nmilitary camps. Certain security system has biometric features\nwhich helps in deciding the difference between authorized and\nnon-authorized person which basically gets recorded in\nbiometric database.\nWhen someone is about to enter the biometric centered place then\nthe system automatically records the persons features, it\ncompares the captured biometric with the saved ones and decide\nwhether or not the person is allowed to enter the place.\nb) Person-specific identification in certain scenes: The security\nguards /police can catch criminals or suspects with the assistance\nof personal identification at a distance by a smart surveillance\nsystem. The police database may contain or store new suspect\nbiometric and this can be obtained using visual CCTV system\nfrom the places where the suspect is usually seen as casinos and\nsubway stations etc. So, after this the system is automatically\nenable to recognize whether or not the person in view are\nsuspects. If yes, then the alarms will be given immediately. Even\nthough such systems with faIce recoJgnition havSe already been ER\nused at public sites, their reliability is too low for police\nrequirements.\nc) Crowd flux statistics and congestion analysis: The flux of\npeople at important public areas such as stores, can be\nDrawbacks\nautomatically computed by the surveillance systems, using\n\u2022 The passive system is used only in the military bases but\ntechniques for human detection. It can then provide congestion\nnot metro stations, airports and mall etc.\nanalysis to assist in the management of the people. Similarly,\n\u2022 It is a complex architecture. In passive system there is\nexpressways and junctions of the roads can be monitored\nnot individual safety level considered.\nthrough visual surveillance systems, and further analyze the\n\u2022 Basically, the video in passive system is captured\nstatus of road congestion and traffic.\ncontinuously which can lead to privacy issue.\n\u2022 Also, the system does not have efficient alarm system\nd) Ambiguity detection and alarming: At sometimes, it is very\nthat responses in return of any act of abuse or bullying.\nvital to analyze the behavior or characteristics of people and to\n\u2022 There is nor blurring out of the sensitive information in\ncheck whether they are normal or abnormal. For example,\npresent system.\nabnormal behaviors indicative of theft, can be analyzed using the\nvisual surveillance systems, which can be placed in supermarkets\nand parking lots. Basically, there are so many ways of giving alert\nsuch as by ringing the buzzer. One way is to make recorded\ndeclaration automatically whether certain abnormal behavior is\ndetected.\ne) Face detection and Object Detection: Convolutional neural\nnetworks are widely used in addressing image-based problems,\nsuch as object/character detection and face recognition. In this\npaper we will be Using Faster-RCNN Because its Faster Than the\ntraditional CNN and also saves time as it detects the things\nregion-wise and searches only the region that is needed rather\nthan searching for all the regions marked in the image as the\nCNN does.\nIJSER \u00a9 2021\nhttp://www .ijser.org",
        "tables": [],
        "forms": {},
        "confidence_score": 1.0
      },
      {
        "page_number": 2,
        "text_content": "497\nINTERNATIONAL JOURNAL OF SCIENTIFIC & ENGINEERING RESEARCH, VOLUME 12, ISSUE 1, JANUARY-2021\nI SSN 2229-5518\n3.PROPOSED SYSTEM respective authorities about the criminal activity taking place\nThe main aim of our project is to detect the criminal activities and they take instant action.\nwith the utmost accuracy that take place in the public areas\nthrough the CCTV that take place in the public areas. Our current\nmodel consists of 2 detection functions, one for detecting crimes\nwhich are done when there are little movements detected (e.g.,\nRobbery or people armed with weapons) and another one for\ndetecting crimes that committed with the heavy or large\nmovements (e.g., stabbing). To avoid the negative/false alarms\ndetection, we have executed a system where the already stored\ndetections are all cleared and only the recently detected.\nOther crime prediction system software perform certain citizen\ntrustworthiness analyzes which is basically based on data that is\nprovided or obtained from organization such as police, hospital,\nschool, banks and from social media. But this solution we believe\nthat such a system can potentially be prone to certain\ndiscriminations against certain discrimination against certain\nsituation that may involve in any crime.\nIn our particular solution, there is no such social credit system\nthat keeps permanent log of all the activity, So in this there is no\nneed of continuous monitoring and assessment. Instead of this\nMachine Learning is used to detect the crime or criminal activity\nand react to the scenario by responding or alerting user or\nrelevant authority. IJSER\nThe flowchart here explains the flow of our system\nWhenever there is any action in the room the CCTV surveillance\ngets active.\n4. METHODOLOGY\n1)If there is any \u201csituation of concern\u201d then the CCTV cam is\nIn this paper we will be using certain algorithms to determine\ndirected towards it and the frame is captured.\nthe human and object detection in the surveillance.\nFor Detection system we are using faster R-CNN deep learning\n2)Now the algorithms detect if the frame has a knife or a pistol. If\nalgorithm. As in traditional CNN we take the image then we\nthe frame detects the knife then optical flow algorithm calculates\ndivide into regions. we will then consider each as separate image\nthe velocity at which the knife is being moved from one frame to\nthen pass all these regions to CNN as classify them into various\nthe other. It then calculates the probability of stabbing by which\nclasses. Once every region is divided into its certain classes, then\nintensity the process is taking place.\nwe combine all the regions to get original image with the detected\nobjects. We pass an object to the network and it then goes through\n3)If the probability taken is greater than the threshold value that\nvarious loops and pooling layers and then we get the output from\nis being set by us. The threshold needs to be set reasonably low\nthe object class. We basically use R-CNN as it implements\nto ensure that the tip of knife is detected as a corner, even at the\nselective search to extract a bunch of regions in the image rather\nexpense of more corners being detected. Basically, the range is\nthan the massive number of regions to check if any of these boxes\nfrom 0 to 255. Here the threshold set is 204 as low as possible,\ncontain any object that we require for our process. Process for R-\nwhen we set 204 as threshold it detects the knife in the frame with\nCNN is as follows: -\nthe utmost accuracy.\n1)It takes the images\n2)It generates initial sub segmentation so that multiple images\n4)Then when the knife is detected with much accuracy then the\nfrom the original image\nthere is an alert that is the buzzer goes on. After which the\n3)The technique then combines certain same and relatable regions\nrespective authorities are informed about the criminal activity\nto form a layer region which is based on color, similarity, texture\ntaking place and they can take instant action.\nsimilarity, size similarity and shape compatibility.\nBut this algorithm has certain drawback that are overcome by\n5)If the knife is not detected then the surveillance is continued.\nFaster-RCNN. The slowest part in R-CNN in selective search or\nNow, secondly if the pistol is detected in the given frame with a\nEdge boxes. Faster RCNN replaces selective search with a very\nhuman handling the object then the buzzer goes on and alert the\nsmall convolution network called Region Proposal network to\nIJSER \u00a9 2021\nhttp://www .ijser.org",
        "tables": [],
        "forms": {},
        "confidence_score": 1.0
      },
      {
        "page_number": 3,
        "text_content": "498\nINTERNATIONAL JOURNAL OF SCIENTIFIC & ENGINEERING RESEARCH, VOLUME 12, ISSUE 1, JANUARY-2021\nI SSN 2229-5518\ngenerate regions of Interest. Faster R-CNN introduces idea of\nanchor.\nSecondly, for Motion Estimation we earlier used Open Pose\nlibrary it created problems when the knife detector is integrated\nit hardly distinguished between human in the frame and who is\nholding the knife. Looking for alternative we found Motion\nestimation using Optical Flow to determine average speed of\nhuman Optical flow is basically a motion of object between\nconsecutive frame of sequences, caused by the relative movement\nbetween object and camera. It is basically 2D vector field where\neach vector is a displacement vector showing movement of\npoints from first frame to second. So Basically, for our project it\nis necessary as we have a knife and a person if person makes\ncertain movement with the knife it should be captured instantly.\nFig 4: Events Center\nIt works on several assumptions\n1)The intensities of pixel for the item do not change between the\nframe that are one after the other\n2)The pixels that are present nearby have similar motion.\nLastly, for very low levels of lightning can render the knife\nundetectable by our algorithm, hence we are using gamma\ncorrection to try and increase the brightness of frame gamma\ncorrection also blurs the picture so we need to find a good level\nof gamma correction such that our object is not blurred.\nIJSER\n5.RESULTS AND DISCUSSION\nIn this paper, the result is the final design of the project on topic\nFig 5: Camera Feed for Gun Detection\n\u201cCriminal Activity Monitoring and Prevention Using CCTV\nSurveillance\u201d.\nSince there it has intuitive GUI which displays the feeds and a\nevent manager tab to store the images of the detected events.\nWhenever there is a \u201csituation of need\u201d then the CCTV is active\nand captures the situation and responses accordingly. Here,\nalgorithms are designed such that it can alert the respective\nauthorities when there is a presence of any dangerous act, or an\nabnormal behavior of a person is detected. This can assure\nsecurity to the public in the public places as well as in other\nlocations such as offices, cinema halls etc. The major advantage\nof the project includes efficiency, fast to access and uniqueness.\nThe behavioral analysis algorithm also makes it easier to monitor\nand prevent the crimes.\nFig 5: Event Center\n6.CONCLUSION\nSurveillance by using the CCTV system has reached its great\nheights. Also, whenever we will be sending the information or\ndata through the networks to any server the transmission process\nis a very crucial work.\nCCTV surveillance systems are mostly managed by governments\nprofessional. As CCTV Information are very sensitive and\nconfidential and also very difficult to handle. In this project, we\nhave proposed certain algorithms that are able to alert the\nFig 3: Camera feed for knife detection respective authorities if any abnormal behavior of a person is\ndetected. We have limited the number of negative alarms in order\nto allow for a real-time working of the system to process well. This\nIJSER \u00a9 2021\nhttp://www .ijser.org",
        "tables": [],
        "forms": {},
        "confidence_score": 1.0
      },
      {
        "page_number": 4,
        "text_content": "499\nINTERNATIONAL JOURNAL OF SCIENTIFIC & ENGINEERING RESEARCH, VOLUME 12, ISSUE 1, JANUARY-2021\nI SSN 2229-5518\nproposed system we will implement first implement it at low T.A.P., 2013, Smart Attendance using Real Time Face Recognation\nscale then further we can escalate things to higher (Smart-FR), SAITM Research Symposium on Engineering\nimplementation. In the future, we will enhance the proposed Advancement, Sri Lanka.\nsystem by implementing the night vision surveillance using the\nInfrared image enhancement. So that our project progress well\nand also gives us more coverage to handle the situation at night.\n7.REFERENCES\n[1] Choi Woo Chul and Na Joon Yeop, \u201cRelative Importance for\nCrime Prevention Technologies as Part of Smart City based on\nSpatial Information\u201d, IEEE Journal, Smart Cities Symposium\nPrague, 13 July 2017\n[2] Robin Singh Sidhu and Mrigank Sharad, \u201cSmart Surveillance\nSystem for Detecting Interpersonal Crime\u201d, presented in\nInternational Conference on Communication and Signal\nProcessing, April 6-8, 2016, published in IEEE Journal, 24\nNovember 2016\n[3] Huang J, Rathod V, Sun C, Zhu M, Korattikara A, Fathi A,\nFischer I, Wojna Z, Song Y, Guadarrama S, Murphy K,\n\"Speed/accuracy trade-offs for modern convolutional object\ndetectors\", CVPR 2017\nIJSE R\n[4] Klima M., Pazderak J., Bernas M., Pata J., Hozman J., Roubik\nK., Objective and Subjective Image Quality Evaluation for\nSecurity Purposes, 35th IEEE International Carnahan Conference\non Security Technology, London, U.K.\n[5] Padaruth, S., Indiwarsingh, F. & Bhugun, N., 2013, A Unified\nIntrusion Alert System using Motion Detection and Faces\nRecognition, 2nd International Conference on Mechine Learning\nand Computer Science (IMLCS), Kuala Lumpur.\n[6] Namrata, Pradeep & Sagar, R., 2013. Cognitive Security\nSystem Based on Image Comparison and Motion Detection with\nAble Memory Usage. International Journal of Advances in\nEngineering & Technology, VI(2), pp.850-61.\n[7] Putro, M.D., Adji, T.B. & Winduratna, B., 2012, Sistem Deteksi\nWajah dengan Menggunakan Metode Viola-Jones, Proseding\nSeminar Nasional \"Science, Engineering and Technology\".\nMalang.\n[8] Santoso, H. & Harjoko, A., 2013. Haar Cascade Classifier dan\nAlgoritma Adaboost untuk Deteksi Banyak Wajah dalam Ruang\nKelas. Jurnal Teknologi, VI(2), pp.108-15\n[9] Febrianto, A.J., 2012, Pengenalan Wajah Dengan Metode\nPrinciple Component Analysis (PCA) Pada sistem Absensi Real\nTime, Tesis, Magister Teknik Elektro, Universitas Gadjah Mada,\nYogyakarta.\n[10] Tharanga, J.G.R., Samarakoon, S.M.C. & Karunarathne,\nIJSER \u00a9 2021\nhttp://www .ijser.org",
        "tables": [],
        "forms": {},
        "confidence_score": 1.0
      },
      {
        "page_number": 5,
        "text_content": "500\nINTERNATIONAL JOURNAL OF SCIENTIFIC & ENGINEERING RESEARCH, VOLUME 12, ISSUE 1, JANUARY-2021\nI SSN 2229-5518\nIJSER\nIJSER \u00a9 2021\nhttp://www .ijser.org",
        "tables": [],
        "forms": {},
        "confidence_score": 1.0
      }
    ]
  },
  {
    "file_path": "pdfs\\5th paper.pdf",
    "total_pages": 17,
    "combined_text": "applied\nsciences\nReview\nCrowd Monitoring and Localization Using Deep\nConvolutional Neural Network: A Review\nAkbarKhan 1,JawadAliShah1,* ,KushsairyKadir1,WaleedAlbattah2\nandFaizullahKhan3\n1 ElectronicSection,UniversitiKualaLumpurBritishMalaysianInstitute,Selangor53100, Malaysia;\nakbar.khan@s.unikl.edu.my(A.K.);kushsairy@unikl.edu.my(K.K.)\n2 DepartmentofInformationTechnology,CollegeofComputer,QassimUniversity,\n51921Buraydah,SaudiArabia;w.albattah@qu.edu.sa\n3 DepartmentofTelecommunicationEngineering,BalochistanUniversityofInformationTechnology,\nEngineeringandManagementSciences,Quetta87300,Pakistan;faizullah.khan@buitms.edu.pk\n* Correspondence:jawad@unikl.edu.my\nReceived:18May2020;Accepted:26June2020;Published:11July2020\nAbstract: Crowd management and monitoring is crucial for maintaining public safety and is an\nimportant research topic. Developing a robust crowd monitoring system (CMS) is a challenging\ntask as it involves addressing many key issues such as density variation, irregular distribution\nof objects, occlusions, pose estimation, etc. Crowd gathering at various places like hospitals,\nparks, stadiums, airports, cultural and religious points are usually monitored by Close Circuit\nTelevision(CCTV)cameras.ThedrawbacksofCCTVcamerasare: limitedareacoverage,installation\nproblems, movability, high power consumption and constant monitoring by the operators.\nTherefore,manyresearchershaveturnedtowardscomputervisionandmachinelearningthathave\novercome these issues by minimizing the need of human involvement. This review is aimed to\ncategorize,analyzeaswellasprovidethelatestdevelopmentandperformanceevolutionincrowd\nmonitoringusingdifferentmachinelearningtechniquesandmethodsthatarepublishedinjournals\nandconferencesoverthepastfiveyears.\nKeywords: crowdmonitoring;crowdcounting;crowddensityestimation;deepconvolutionalneural\nnetworks(DCNN);crowdbehavior\n1. Introduction\nCrowdissameordifferentsetofpeoplearrangedinonegroupandmotivatedbycommongoals.\nTherearetwotypesofcrowdnamelystructuredcrowdandunstructuredcrowd.Intheformer,thedirection\nofthemovementistowardsacommonpointandpeoplearenotinscatteredformwhileinthelatertypethe\ndirectionofthepeopleisnottowardsacommonpointandtheyareusuallyinscatteredform[1].\nCrowdmonitoringhasawiderangeofapplicationssuchas,safetymonitoring,disastermanagement,\ntrafficmonitoringanddesignofpublicspaces.Thisvarietyofapplicationshaveencouragedresearchers\nthroughout numerous fields to develop models for crowd monitoring and associated tasks such as\ncounting [2\u20135], density estimation [6\u20138], tracking [9], scene understanding [10], localization [11] and\nbehaviordetection[12,13].Amongthese,thecrowdcountinganddensityestimationareimportanttasks\nand represent fundamental building blocks for several other applications [14]. There are three most\ncommonlyusedmethodsforcrowdcountingnamely,objectdetectionbasedcounting[15\u201317],clustered\nbasedcounting[18]andregressionbasedcounting[19,20].Inobjectdetectionbasedcounting;theobject\ndetectorsaretrainedtolocalizethepositionofeverypersoninthecrowdforcounting.Clusterbasedcrowd\ncountingconsistsofidentifyingandtrackingvisualfeatures.Featuretrajectoriesthatshowcoherentmotion\nareclusteredandthenumbersofclusterisregardedasaestimationofmovingobjects[21]. Regression\nAppl.Sci.2020,10,4781;doi:10.3390/app10144781 www.mdpi.com/journal/applsci\nAppl.Sci.2020,10,4781 2of17\nbasedcountingestimatesthecrowdcountbycarryingoutregressionbetweentheimagefeaturesandcrowd\nsize.Thoughtheregressionbasedmethodsareworkinggoodinsituationsofhighdensityastheycapture\ngeneralizeddensityinformationfromtheimageofcrowd,stillithastwomainlimitationsi.e.,performance\ndegradationduetooverestimatingofcountinlowdensitysituationsandimproperdistributionofcrowdin\nthescene[22].Onemayconsiderthatconventionalcrowdcountingmethodssuchaspatchbasedapproach,\nobjectbasedapproachandrichfeatureapproach[8,23,24],whichdependoneitherdetectionorregression,\narelimitedwhenhandlingrealscenewithunavoidabledensityvariations.However,thesemethodscan\nonlybeusedtoestimateandcountthepeopleinlowdensitysituationsinwhichallpartsofthepeople\narefullyvisible.Theperformanceofthesemethodsdeteriorateswhenappliedinhighdensitysituations.\nAnidealcountingmethodshouldhavetheadaptiveabilitytochoosetheappropriatecountingmode\naccordingtocrowddensity[20]. Exactcrowdcountingandlocalizationareindispensableforhandling\nhighdensitycrowds. Localizationmeanstogettheaccuratelocationoftheheadsinanimage. Head\nis the only visible part through which localization can be found in highly dense crowd images or\nvideos[25]. Althoughnumerousstepshavebeentakeninthedetectionofhumanheads[16,17,26],\nheaddetectionisstillachallengingtask. Asaresultofthevariationinthescaleappearanceofheads,\nitisstillabigproblemtoexactlydiscriminatehumanheadsfromthebackground. Forlocalizationin\ncrowdedscene,densitymaphasbeenusedasaregularizerduringthedetection[27]. Incomputer\nvision,crowdbehaviordetectioninvideosurveillanceisoneofthelatestresearchareas[1]. Crowd\nbehavior detection has many application domains such as automatic detection of riots or chaotic\nactsincrowdandlocalizationofabnormalregions[28]. Detectionofcrowdbehaviorisextensively\nusedinordertomonitorandmaintainthesurveillanceofpublicplaceslikesportsevents,markets,\nreligiousandpoliticalgatherings,etc. Therearetwocategoriesofcrowdbehaviordetectionnamely\nglobalcrowdbehaviordetectionandlocalcrowdbehaviordetection[29]. Inglobalcrowdbehavior\ndetection a large area is affected, while in the local crowd behavior detection it affects the limited\nareaofthecrowdandthebehaviorofanindividualisquitedifferentfromitsneighbor. Multiscale\ntextureanalysisisusedforassessingthebehaviorofcrowdinvideosequences[30]. Theaimofvideo\nsurveillanceisusedtodetectabnormalhumanbehavior[31].\n1.1. Rational\nReviewingtheliteratureisoneofthemostimportantactivitiesinresearch.Thispaperisthefirstpaper\ninaseriesofresearchpapersinthefieldofcrowdmonitoring.Accordingtoourplanofcurrentworking\nonasubstantialcrowdmonitoringprojectfundedbyMinistryofEducationinSaudiArabia,itisavery\ncrucialphasetostudytheliteratureandanalyzeitinordertoaddressdifferentaspectsofthesubjectunder\ninvestigations.Anycontributioninanysubjectorfieldcannotbeachievedwithaconsiderableknowledge\nofthestateoftheart.Webelievethispaperwillprovideusaswellasinterestedresearcherswithoverview\nofexistingstudiesinthefieldofcrowdmonitoringandmanagement.Wealsoexpectthisreviewprovides\nanovelsynthesisofthecurrentresearchworks,whichwehopecanleadtoanewmeansofconsidering\ncrowdmonitoringaswellasfindinganyavailablegaps.\n1.2. Datasets\nVariousdatasetscontainingcrowdvideosandimagesarepubliclyavailableandarebeingusedto\nvalidatetheexperimentalresults. Someofthepubliclyavailabledatasetsalongwithitsdescriptionare\nshowninTable1.\nUCSDdatasetwasthefirstdatasetusedforpeoplecounting[32]. Thedatahasobtainedthrough\na camera installed on a pedestrian pathway. The dataset includes 2000 frames 238 \u00d7 158 of video\nsequences,alongwithgroundtruthannotationofeachpedestrianineveryfifthframehaving49,885\npedestrianintotal. TheMalldatasethasbeencollectedbymeansofsurveillancecamerasinstalled\ninashoppingmall[33]. Ithasatotalof2000frameswithsizeof320\u00d7240. UCF_CC_50dataset[2]\nisachallengingdatasetcomprisingofawidevarietyofdensitiesandvariousscenes. Thisdataset\nhas been obtained from different places like concerts, political protests, stadiums and marathons.\nAppl.Sci.2020,10,4781 3of17\nTheentirenumbersofannotatedimagesare50containing1279individualsperimage. Thisdataset\nhasavaryingresolutionandtheindividualsdifferfrom94to4543representingalargevariationinthe\nimage. Thedrawbackofthistypeofdatasetisthat,thereisonlylimitednumberofimagesavailablefor\ntrainingandevaluation. WorldExpo\u201910datasetintroducedin[34]hasbeenusedforcrossscenecrowd\ncounting. Thedatasetcomprisesof3980framesofsize576\u00d7720with199,923labeledpedestrians.\nThemaximumcrowdcountthroughthisdatasetislimitedto220andisinsufficientforevaluating\nextremelydensecrowdscounting. TheShanghaiTechdataset[35]hasbeenintroducedforlargescale\ncrowdcountingcontainingof1198imageswith330,165annotatedheads. Intermsofannotatedheads,\nthisdatasetisoneofthelargest. Thedatasetcontainstwotypes,namelyPartA,PartB.PartAismade\nof482imagestakenfromtheinternetrandomly.WhereasPartBcomprisesof716imagescollectedfrom\nthemetropolitanstreetinshanghai. ThemostrecentdatasetisUCF-QNRF[11]having1535images.\nWithinthisdataset,thenumberofpeoplefluctuatesfrom49to12,865makingamassivevariation\nindensity. Furthermore,ithasahugeimageresolutionrangingfrom400\u00d7300to9000\u00d76000and\nconsists of crowd videos with varying densities and perspective scales. CUHK dataset has been\ncollected from diverse locations namely, street, shopping malls, airports and parks. The dataset\ncomprisesof474videosclipsfrom215scenes[36].\nTable1.Descriptionofdatasets.\nOverall\nDatasets Description No.ofImages Resolution Min Ave Max Accessibility\nCount\nUCSD Peoplecounting 2000 238\u00d7158 11 25 46 49,885 Yes\nMALL Peoplecounting 2000 320\u00d7240 13 - 53 62,325 Yes\nUCF_CC_50 Densityestimation 50 Variable 94 1279 4543 63,974 Yes\nCrossscene\nWorldExpo\u201910 3980 576\u00d7720 1 50 253 199,923 Yes\ncrowdcounting\nVariable,\nShanghaiTechA,B Crowdcounting 482 33 501 3139 241,677 Yes\n768\u00d71024\nCrowdcounting 400\u00d7300to\nUCF-QNRF 716 9 123 578 88,488 Yes\nandlocalization 9000\u00d76000\nCUHK Crowdbehavior 1535 Variable 49 815 12,865 - Yes\nThis review paper is mainly focusing on the crowd monitoring (crowd counting,\ncrowdlocalizationandbehaviordetection). Therestofthepaperisorganizedasfollows. Section2\nshowssearchmethodologyandTaxonomyLevel. InSection3,crowdmonitoringapproachestaken\nfromthepreviousliteraturearesummarizedintabularform. Section4showscrowdrelatedresearch\napproaches. Section5showstheconvolutionalneuralnetworkanddeepCNNframeworks. Section6\nrepresentsthediscussionandSection7elaboratesconclusions.\n2. SearchMethodologyandTaxonomyLevel\nFigure 1 shows the growth of published papers on crowd monitoring (Crowd counting,\nlocalization and behavior) using different machine learning methods and techniques. The graphs\nshowthetwomaindatabasesofscopusandwebofscienceinwhichtheresearchpapershavebeen\npublishedfromyear2014to2019. Wehavesearchedcrowdmonitoringkeywordinbothdatabases\nandfounddifferentpaperswithdifferentmethodsandtechniques. Thedetailsofthepaperspublished\ninbothdatabasesareshowningraphicalviewasshowninFigure1.\nTaxonomyLevel\nTheentiretaxonomylevelofcrowdmonitoringhasbeenshownintheformofflowchart.Basically\nthecrowdrelatedresearchapproacheshavebeencategorizedintotwodomainsbasedontheliterature\nreview namely crowd management and crowd monitoring. Then we have made categories of crowd\nmonitoringi.e.,counting,localizationandbehavior. Focusingoncrowdmonitoring,wejustreviewthe\nAppl.Sci.2020,10,4781 4of17\ncrowdmanagementanddidnotcategorizeitfurther. Afterthat,thecountingsectionhasbeendivided\nintotwopartsnamelydensityestimationandpeoplecounting. Inlocalizationtherearethreemainsub\ncategoriesi.e.,countingandlocalization,estimationandlocalizationandanomalydetectionandlocalization.\nFinally,thebehaviorcategoryhasbeendividedintothreesubcategoriesi.e.,individualbehaviorestimation,\nanomalousbehaviordetectionandnormalandabnormalbehaviordetection.Thetaxonomylevelofcrowd\nmonitoring(crowdcounting,localizationandbehavior)hasbeenshowninFigure2.\nWeb of science Scopus No.of papers\n30 27\n2019 8\n25\n19 20 2018 7\n20 18\n16\n14 2017 10\n15 12 12 13\n10 2016 6\n10 7 8\n2015 5\n5\n2014 2\n0\n2014 2015 2016 2017 2018 2019 0 2 4 6 8 10 12\na) Web of science and scopus b) Machine learning on crowd monitoring\nFigure1.(a)Thelistofresearchpaperspublishedinwebofscienceandscopusoncrowdmonitoring\nfrom2014\u20132019while(b)showsmachinelearningoncrowdmonitoring\nCrowd Related\nResearch\nApproaches\nCrowd Crowd\nManagement Monitoring\nCounting Localization Behavior\nCounting Individual\nE D si e t n m s a it t y io n c P o e u o n p t l i e n g loca a li n z d at ion es b t e im ha a v t i i o o r n\nEstimation Anomalous\nand behavior\nlocalization detection\nAnomaly Normal and\ndetection abnormal\nand behavior\nLocalization detection\nFigure2.Crowdrelatedresearchapproaches.\n3. CrowdMonitoring(CM)Approaches\nTable2showscompletelyaboutalltheresearchpapersaboutcrowdcounting,localizationand\nbehaviordetection. Differentresearchpaperswithakeyword\u201ccrowdmonitoring\u201dhavebeensearched\nwhileusingtwomainlyuseddatabasesi.e.,webofscienceandscopusfrom2014\u20132019,respectively.\nWehavefounddifferentmethodsandtechniquesrelatedtocrowdmonitoringandtriedtoreview\nitcompletely. Furthertheelaborationofthecrowdcounting, localizationandbehaviorhavebeen\ndeliberatelydiscussedintabularformasshowninTable2.\nAppl.Sci.2020,10,4781 5of17\nTable2.CrowdMonitoring(CM)approaches.\nRef Process Frameworks/Methods Performance Conclusion/Result\nCombinationofcrowdsizeestimationand Classificationwith\n[4] Crowdmonitoring(Counting) Accuracy90%\ncounting MSE0.0081\nNeuralnetworkandregression BPNNcandeal9\n[37] Crowdmonitoring(Counting) BPNNprovidesthebestestimation\ntreesusingfisheyecamera framesinsecond\nICrowdframeworkwasdesigned\nonathree-layerapproach,device\n[38] Crowdmonitoring(Estimation) Noexperimentalresults Capableoflocationupdates\nlayer,middlewarelayerandthe\napplicationlayer\nAirbornecamerasystems, Dependsongoodtraining Gaborfilterplaysaprominent\n[39] Densityestimation(Estimation)\nsupportvectormachineandGaborfilter samplesandsimilarimages roleinrealscenesofimages\nReal-timecrowddensity\nCrowdmonitoring Informationmanagement Expertsystemmodule\n[40] measurementsand\n(Controllingcrowdmovement) moduleanddecisionsupportsystem performswell\ncommunicationsduringhajj\nCrowdCounting\n[41] MedianfilterandKalmanfilter Accuracy95.5% Robustsmartsurveillancesystem\n(Normal/abnormalevent)\nEstimationand\n[11] DeepCNNnetworks Specificity75.8% Decreaseerrorrate\nlocalization(Densitymap)\nDetectionandlocalization Globalandlocaldescriptors, Achievedgoodandcompetingmethods\n[42] Accuracy99.6%\n(Anomalydetection) withtwoclassifierswereproposed withlowcomputationalcomplexity\nDISAMoutperformsforUCSD\nCountingandLocalization andWorldExpo\u201910datasetswith\n[43] CNN Reductionofclassificationtime\n(Humanheads) thelowestMAEof1.01and8.65,\nrespectively\nAveragePrecisionandaverage Reductionofclassificationtimeand\n[25] Countingandlocalization SD-CNNModel\nrecallrate73.58and71.68 improvementindetectionaccuracy\n99.1%accuracyand\n[44] Crowdmonitoring(Behavior) EHCAF HighlyaccurateandlowFNR\nFNRof2.8%,\nCrowdmonitoring\n[45] Isometricmapping(ISOMAP) Reducedfeaturespace Reductionofcomputationtime\n(Behaviordetection)\nAppl.Sci.2020,10,4781 6of17\nTable2.Cont.\nRef Process Frameworks/Methods Performance Conclusion/Result\nVisualdescriptorshaveextractedand\nCrowdbehavior\n[46] Spatio-temporalmodel Accuracy98%and88% consideredforbothindividual\nanalysis(Behavior)\nandinteractivebehaviors\nCrowdevacuation Correlationscoreswere\n[47] LegionEvacsoftware Reducedevacuationtime\n(Evacuationbehavior) positive\nDetectionofanomaly OpticalflowandHorn Computationofdistance Anovelapproachofabnormal\n[48]\n(Normalandabnormal) Schunckalgorithm betweencentroids eventdetectionhasproposed\nTheSTTmethoddemonstrates\ncomparableresultsof\nCrowdbehaviordetection Spatio-Temporal Crowdanomalydetection\n[49] Spatio-temporalCompositions\n(Identifybehavior) Texturemodel frameworkwasintroduced\n(STC)andInferenceby\nComposition(IBC)\nApproximatemedianfilter\nCrowdbehavior Arobustunsupervisedabnormal\n[50] andforeground Lowerfalserate\ndetection(Behavior) crowdbehaviordetectionhasachieved\nsegmentationalgorithm\nCombiningcompressivesensingand\nViolentbehavior Hybridrandommatrix(HRM)\n[51] Accuracy90.17%and91.61% deeplearningtoidentifyviolentcrowd\ndetection anddeepneuralnetwork\nbehavior\nTheperformanceofthis Holisticapproachforabnormal\n[52] Crowdbehaviordetection Holisticapproach\nmethodsyieldsbetterresults crowdbehaviordetectionhasproposed\nThecombinationofSIFTandgenetic\nCrowdbehavior Scale-invariantfeature\n[1] Accuracy95% algorithmhasachievedbettersimulation\ndetection(Realtime) transform(SIFT)\nresults\nCrowdbehaviormonitoring Fixed-widthclustering Accuracyisbetween Theapproachhasasuperior\n[53]\n(Eventdetection) algorithmandYOLO 80%-95.7% performanceonsixvideos\nAbnormalbehavior Opticalflowmethod\n[54] 87.4%accuracy Higherdetectionrateforanomaly\ndetection(Abnormality) andSVM\nAppl.Sci.2020,10,4781 7of17\n4. CrowdRelatedResearchApproaches\nAfter going through an important number of papers, it has been determined that the overall\nworkcanbedividedintotwocategories,namelycrowdmanagingandcrowdmonitoring(counting,\nlocalizationandbehavior). Hereisabriefdescription:\n4.1. CrowdManagement\nCrowdmanagementhasmadeenormousprogressoverthelastfewyears. Withintheliterature,\nvariousmodelshavebeenproposed. Likein[37],theauthorshaveproposedaFiniteStateMachine\n(FSM)modeltosimulatethemovementofcrowdduringTawaf(tomovearoundtheKaabaseven\ntimesaspartofthehajjinMecca). ThemodelcanbeusedtomonitorandmanagecrowdinMataf\n(place of Tawaf) in real time. Similarly in [38], the authors have proposed a framework weighted\nroundrobintoovercomethecongestionandovercrowdedduringhajj(pilgrimage). Theframework\nwasdesignedtobeproactiveinaccuratelypredictingpotentialproblemsthroughtheuseofsmart\nmonitoringofeachpathofritualslocations. Amodelhasplannedtocountthenumberofpeople\nbyusingnon-participatory(non-intrusive)techniquesupportedbystatisticalapproachbyDesign\nofExperiments(DOE)forcrowdsafetyandmanagement[39]. In[40],aninformationmanagement\nmodule and decision support system was used to monitor and manage the crowd. The proposed\nframework provides an automated approach for detecting and evaluating the video scene and\nclassifyingcrowdsandtrafficmanagement[55].\n4.2. CrowdMonitoring\nCrowd monitoring can further be categorized into crowd counting, crowd localization and\ncrowdbehavior.\n4.2.1. CrowdCounting\nCountingmeansspecificallycountthenumberofpeopleinthecrowd. Thecrowdcountinghas\nbeendiscussedbymanyauthorsintheliterature. Khanetal. [43]proposesanovelheadcountingand\nlocalizationtechnique,DensityIndependentandScaleAwareModel(DISAM),thatperformswellfor\nhighdensitycrowedwherehumanheadistheonlyvisiblepartintheimages. CNNisfirstusedas\nheaddetectorandlaterforcomputingresponsematrixfromthescaleawareheadproposalstoobtain\ntheprobabilitiesofheadintheimages. In[56],youonlylookonce(YOLO)isadetectiontechnique\nwhichisbroadlyusedforthedetectionofobjectsinanimagewithhighlevelofperspectivevalues\ni.e.,maximumthresholdvalue. Xuetal. [57]recommendedCNNandlearntoscalethatgenerate\nmultipolarnormalizeddensitymapsforcrowdcounting. Itextractsapatch-leveldensitymapby\naprocessofdensityestimationandclustersthenintomultiplelevelsofdensity. Eachpatchdensity\nmap is normalized via an online learning strategy for the center with multi polar loss. In [58] the\ncrowddensityofsurveillancevideosismeasuredusingCNNandshorttermmemory. Twoclassical\ndeepconvolutionalnetworksnamelyGooglenet[59]andVGGnet[5]wereusedforestimatingcrowd\ndensity [60]. Similarly, [4] first approximate crowd size estimation and secondly count the exact\nnumberofpeopleinthecrowd. Theefficiencyremainsunchangedinthetermsofitsaccuracyof(90%).\n4.2.2. CrowdLocalization\nLocalizationofcrowdsincrowdedimagesreceivedlessattentionfromtheresearchcommunity.\nWithlocalizationinformation,onecanfigureouthowpeoplearedistributedinthearea,whichisvery\nimportantforcrowdmanagers[43]. Informationaboutlocalizationcanbeusedtodetectandmonitor\nanindividualindensecrowds [61]. Inordertoidentifythelocationofheadinanimagearegression\nguideddetectionnetwork(RDNet)hasproposedforRGB-Datasetsthatcansimultaneouslyestimate\nheadcountsandlocalizeheadswithboundingboxes[62]. Similarlyin[27],adensitymaphasbeen\nusedtolocalizetheheadsindenseimagewithaccurateresults. In[63],localizationhasbeenidentified\nAppl.Sci.2020,10,4781 8of17\nwhileusingLSC-CNNwiththehelpofametricnamedasMeanLocalizationError(MLE).Thismodel\nhasachievedaremarkableperformanceintermsofitsaccuracy. CompressedSensingbasedOutput\nEncoding(CSOE)hasbeenproposedwhichcanhelptoimprovetheefficiencyoflocalizationinhighly\ndensecrowdedsituation[64].\n4.2.3. CrowdBehavior\nCrowdbehavioranalysisanddetectionhavebecomeaprimaryparteverywhereforpeaceful\neventorganization[65]. Thedifficultiesofbehavioridentificationandabnormalbehaviordetecting\nareveryimportantissuesinvideoprocessing[66]. Theresearchershaveproposeddifferentmethods\nandtechniquesforthecrowdbehaviordetection. Someofthecloserelatedworksareelaboratedhere.\nIn[66,67],imageprocessingwithopticalflowandmotionhistoryimagetechniqueswereusedtodetect\nthebehaviorofcrowd. Similarlyin[54],anopticalflowmethodwithSupportVectorMachine(SVM)\nwasusedforabnormalbehaviordetection. In[68],CascadeDeepAutoEncoder(CDA)andwiththe\ncombinationofmulti-frameopticalflowinformationhavebeenproposedforthedetectionofcrowd\nbehavior. IsometricMapping(ISOMAP)[45],spatio-temporal[46]andspatio-temporaltexture[49]\nmodelswereusedtodetecttheanomalouscrowddetection. In[51]HybridRandomMatrix(HRM)\nanddeepneuralnetworkwereusedforthedetectionofviolentbehaviordetection. OneusesSIFT\nfeatureextractiontechnique[1]andFixed-widthclusteringalgorithmwithYOLOwereusedtodetect\ncrowdbehavior[53].\n5. CNNandDeepCNNFrameworks\nDeep CNNs are special types of Artificial Neural Networks (ANNs) that learn hierarchical\nrepresentationfromthespatialinformationcontainedindigitalimages. Itwasoriginallydesignto\nprocessmultidimensional(2Dand3D)arraysofhighresolutioninputdatasetssuchasimagesand\nvideos[69\u201371]. ThefirstdeepCNNarchitecturewasAlexNet[69]havingsevenhiddenlayerswith\nmillionsofparameters. Deepconvolutionalneuralnetworkshaveachievedgreatsuccessonimage\nclassification[70],objectdetection[71],crowdcounting[72]andpeoplelocalization[73]. Thebrief\nstructureofdeepCNNisshowninFigure3.\nFigure3.Structureofdeepconvolutionalneuralnetworks(CNN)[74].\nThesuccessofconvolutionalneuralnetworks(CNN)anddeepconvolutionalneuralnetworks\n(DCNN)invariouscomputervisiontaskshasinspiredresearcherstoleveragetheirabilitytolearn\nAppl.Sci.2020,10,4781 9of17\nnonlinearfunctionsfromcrowdimagestotheirrespectivedensitymapsorcounts[75]. Avarietyof\nCNNsandDCNNshavebeenproposedintheliteraturewhichareusedforcrowdmonitoring. Recent\nstudiesareincludedinTable3whichrepresentsdifferentmethodsandtechniquesusedforcrowd\nmonitoringi.e.,crowdcounting,densityestimationandlocalization.Theselectionofresearchpapersis\nfrom2017-2019respectivelyinwhichdeepconvolutionalneuralnetworks,scaledrivenconvolutional\nnetworksandsimpleconvolutionalneuralnetworkshavebeenused. Theexperimentalresultswere\nevaluatedusingdifferentdatasetsnamelyUCSD[32],worldexpo\u201910[19],UCF-CC-50[2],Shanghai\nTechPartA,B[35],UCF-QNRF[11]andsubway-carriage[60]datasets. Furtherdetailsarepresentedin\nTable3.\nTable3.CNNanddeepCNN\nMethods/\nRef CrowdMonitoring Datasets Researchfocus Accuracy\nTechniques\nUCSD,world\nPeoplecounting SD-CNN Reducedthe\n[25] Expo\u201910and Countandlocalize\nandlocalization model classificationtime\nUCF-CC-50\nCrowdcounting UCSDand Detection,estimation Reducedthe\n[43] DISAM\nandLocalization Worldexpo\u201910 andlocalization classificationtime\nShanghaiTechA,B, Largevariationin\n4.2%,14.3%,27.1%and\n[57] Crowdcounting SPN+L2SM UCF-CC-50 densityforcrowd\n20.1%ratesofMAE\nandUCF-QNRF counting\nShanghaiTechPartA, Decreasedthe\nCounting,estimation\nEstimationand ShanghaiTechB, errorrateof\n[11] Compositionloss ofdensitymap\nlocalization UCF-CC-50and compositional\nandlocalization.\nUCF-QNRFdatasets loss.\nCrowdmanaging\n[60] DeepCNN Subway-carriagescenes Crowddensityestimation 91.73%\nandmonitoring\n6. Discussion\nThis section describes the pairwise comparison of various methods and test datasets.\nFor comparison we have selected some state of the art models which are widely used for crowd\nmonitoring. Table 4 contains the summary of pairwise comparison using MAE and MSE as\nbenchmarkingparameters. TheCNNbasedcrowdcountingandlocalizationalgorithmspresented\nin[8,22,34,35,76]arecomparedwithScaleDrivenConvolutionalNeuralNetwork(SD-CNN)using\nUCSD, UCF-CC-50, WorldExpo\u201910 and ShanghaiTech Part A, B datasets. The MAE and MSE of\nSD-CNN are lesser than those of other models on UCSD, UCF-CC-50 and WorldExpo\u201910 datasets,\nrespectively. Comparingthesemodels,SD-CNNhastheabilitytocountandlocalizethehumanheads\ninbothlowandhighleveldensitycrowdimages. Similarly,DensityIndependentandScaleAware\nModel(DISAM)hasthelowestMAEascomparedtoothermodelstestedonUCSDandWorldExpo\u201910\ndatasets. Unlikepreviousmodelswhichcanonlycountthepeopleindensecrowd,DISAMhasthe\nabilitytohandlebothcountingandlocalizingpeopleinthedensecrowd. Finally,wehavecompared\nSD-CNNwithDISAMondifferentdatasetsandconcludedthatSD-CNNhaslowerrateofMSEon\nUCSDdatasetandMAEonWorldExpo\u201910,respectively.\nIncrowdmonitoringandcountingproblems,researcherstrytoexplorethedomainandhave\napplieddifferentmachinelearningtechniquesandmethodologiestocountandlocalizethecrowdas\nwellastheiranomalousbehavior. In[4]theauthorshavepresentedtheapproximatesizeestimation\nandcountingtheaccuratenumberofpeopleinthecrowd. Themodelhasachieved90%accuracyand\nithasbeenshownthattheefficiencyisnotaffectedbyincreasingthenumberofpeople. TheKalman\nfilteringapproachandKL-divergencetechniquehavebeenused[77]tomonitorandcountthecrowd\ninsmartcity. Similarlytomonitorandestimatecrowddensity,deepconvolutionalnetworkhasbeen\nusedin[60]. Therearethreedifferentclassifiersnamelymultiplelinearregressions,backpropagation\nneuralnetworkandregressiontreeswhichhavebeenappliedandusedin[78]tomonitorandcount\nthenumberofpeopleindensecrowd. In[79]theproposedmodelhasbeendividedintotwofolds;\nAppl.Sci.2020,10,4781 10of17\nfirstly,toproposedensityestimationofthecrowdandsecondly,tocountthenumberofpeopleusing\nK-Gaussian Mixture Model (GMM). A new model has been proposed namely identifiable crowd\nmonitoring(iCrowd)usingthreelayersapproachi.e.,devicelayer,middlewarelayerandapplication\nlayer to identify and monitor the crowd [80]. A Feature From Accelerated Segment Test (FAST)\nalgorithmisintroducedin[81]todetectandestimatethenumberofpeopleinacrowd. Thedeep\nlearning frameworks like CNN and Long Short Term Memory (LSTM) have been used in [58] for\ncrowd density estimation. In [38] an expert crowd control and management system for hajj has\nbeen used with three strategies i.e., to address congestion and overcrowded situation using; First\nIn First Out (FIFO), priority queuing and Weighted Round Robin (WRR). An automatic multiple\nhumandetectionmethodusinghybridadaptiveGaussianmixturemodelwasintroducedin[82]for\nhumandetection. Theefficiencyofproposedmethodhasfurtherevaluatedandanalyzedbyusing\nReceiverOperating/OutputCharacteristics(ROC),MeanAbsoluteError(MAE)andMeanRelative\nError (MRE). The proposed method has shown better results. The crowd can also be monitored\nandcountedbyusingmobilephonesandadoptingclusteringmethods. Throughthesemethodsthe\nmodelhasachieved92%accuracy[83]. TheairbornecamerasystemswiththetechniquesofSupport\nVectorMachine(SVM)andGaborfiltershavealsobeenusedin[84]forcrowdmonitoringanddensity\nestimation. Thequalityofresultsclearlydependsupongoodtrainingsamplesandsimilarimages.\nIn[40],the authorshavedevelopedadecisionsupportsystemandinformationmanagementmodule\nfortherealtimecrowddensitymeasurements. Thismodelhasalsobeenimplementedforthecrowd\nmonitoringduringHajj. In[44],theauthorshavepresentedaframeworkforcrowdbehaviorusingan\nEnhancedContext-AwareFrameworkandachievedexperimentalresultswiththeaccuracyof99.1%\nand2.8%ofFalseNegativeRate(FNR)indicatingasignificantimprovementoverthe92.0%accuracy\nandFNRof31.3%oftheBasicContext-AwareFramework(BCF).Thedetectionofanomalouscrowd\nbehaviorhasbeenmonitoredin[45]usingIsometricMapping(ISOMAP).Duringthemonitoringand\ndetectingofcrowdbehaviortheISOMAPhasreducedthecomputationaltimesignificantly.Toquantify\nthecrowdbehavioranalysisaspatio-temporalmodelhasbeenproposedin[46]onCUHKandUMN\ndatasets. Theaccuracyachievedforbothdatasetsare98%and88%,respectively. Asoftwarenamedas\nlegionEvachasbeenusedin[47]forthebehaviorofcrowdevacuation,duringsimulatinglegionEvac\ncalculatesvariousmetricsthatreflectaholisticpatternofcrowdevacuation,whichcapturethebehavior\nofcrowd. In[85],authorshavedevelopedaprobabilisticdetectionofcrowdevents(running,walking,\nsplitting,merging,localdispersionandevacuation)onOpticalFlowManifolds(OFM)usingOptical\nFlowVector(OFV)andOpticalFlowBundles(OFB).Dealingwiththeissueofdifferentbehaviors\ncapturedinsurveillancevideofortheuseofnormalandabnormalbehavioraldetection,clustering\nbasedgroupanalysishasbeenusedin[48]anddescribedcertaingroupbehavior,suchascollectivity,\nuniformityandconflict. In[49],theauthorshaveproposedaSpatialTemporalTexture(STT)model\nwhichcanautomateandidentifycrowdbehaviorundercomplexreallifesituation. Theanticipated\nSTTmethoddemonstratessimilarresultsofSpatio-TemporalComposition(STC)andInferenceby\nComposition(IBC)andusedlesstimeandasmalleramountofsystemmemoryresources. In[50],\ntheauthorshavepresentedanunsupervisedabnormalcrowdbehaviordetectionusingapproximate\nmedianfilterandforegroundsegmentationalgorithms. In[51]theauthorshaveproposedamodel\nthatmaydetectandidentifytheviolentbehaviorofcrowdusinghybridrandommatrixanddeep\nneuralnetwork. In[52],theauthorspresentedanovelmethodfordetectingcrowdbehaviorinvideo\nsequencesusingprobabilitymodelofspeedanddirection. Thismethodcomprisesoftwomainphases;\nbuilding the motion model (speed and direction) and comparing the model of different frames to\ndetectanomalies. AScaleInvariantFeatureTransform(SIFT)techniqueisusedin[1]fordetection\nofcrowdbehaviorinrealtimevideosequences. Similarly, afixedwidthclusteringalgorithmand\nYOLOhavebeenusedin[53]todetectthecrowdbehaviorinvideosurveillance. In[54]theauthors\nhavesuggestedaneffectiveandconcretemethodfordetectingabnormalitiesonthebasisofoptical\nflowpathofthejointpointsforeachhumanbody. Themethodhasanexpressivelyhigherdetection\nrateonthepublicdatasetwith87.4%accuracy. In[41],theauthorshavepresentedauniquemulti\nAppl.Sci.2020,10,4781 11of17\npersontrackingsystemforcrowdcountingandnormal/abnormalindoorandoutdoormonitoring\nsystemusingmedianandKalmanfilters,andhaveobtained95.5%accuracyineventdetection. In[42]\ntheauthorsproposedasystemforidentificationandlocalizationofanomaliesincrowdedsensein\nrealtime. Theperformancewascalculatedonthebasisofitsaccuracyi.e.,99.6%. Similarly,in[43]\ntheauthorshaveproposedanovelmodelnamelyDensityIndependentandScaleAwaremodelfor\ncrowdcountingandlocalizationinhighlydensecrowdandevaluatedthemodelonMeanAbsolute\nError(MAE).In[25]theScaleDrivenConvolutionalNeuralNetworkmodelhasproposedtocount\nandlocalizethecrowd. Thisstrategyreducedtheclassificationtimesignificantlyandimprovedthe\ndetectionaccuracy. Therearemanycommonproblemsinresearchrelatedtocrowdmonitoringsuch\nasscalevariation,complexbackground,localization,etc. whichneedtobesolvedbyusingdifferent\ntechniques. Forscalevariation,SD-CNNhasbeenproposedinliteraturewiththeassumptionthat\ntheheadistheonlyvisiblefeatureinthecrowd. Inadensecrowd,theissueofscalevariationhas\nbeenaddressedbygeneratingascaleawareobjectproposal. Similarly, forlargedensityvariation,\nlearningtoscalemodelhasalsobeenproposed. Localizationofobjectsincomplexbackgroundisstill\nachallengingtask. DISAMcanbeusedthathastheabilitytopreciselylocalizetheheadsincomplex\nscenes. Localizationperformanceisprimarilyaffectedbychangingthethresholdvalue,sofindingan\noptimumstrategyforthisissueisanewdirectionofresearch.\nTable4.Comparisonofsurveyedmethodsandtestdatasets.\nRef UCSD UCF-CC-50 WorldExpo\u201910 ShanghaiTechPartA,B UCF-QNRF\nMAE MSE MAE MSE MAE MAE MSE MAE MSE\n[34] 1.6 3.31 467 498.5 12.9 - - - -\n[8] 1.61 4.4 235.74 345.6 - - - - -\n[22] 1.03 1.37 302.3 411.6 - 49.25 76.25 - -\n[35] 1.07 1.35 377.6 509.1 11.6 68.3 107.25 - -\n[76] 2.89 9.25 - - 26.87 - - - -\n[25] 1.01 1.28 235.74 345.6 7.42 - - - -\n[34] 1.6 3.31 467 498.5 12.9 - - - -\n[22] 1.03 1.37 302.3 411.6 - - - - -\n[35] 1.07 1.35 377.6 509.1 11.6 68.3 107.25 - -\n[86] 1.17 2.15 406.2 404 14.7 - - - -\n[87] 1.62 2.1 318.1 439.2 9.4 60.65 91.75 - -\n[88] - - 295.8 320.9 8.86 46.85 68.25 - -\n[20] 1.03 - - - 9.23 20.75 29.42 - -\n[43] 1.01 - - - 8.65 - - - -\n[35] - - 377.6 509.1 - 68.3 107.25 277 -\n[87] 1.62 2.1 318.1 439.2 - 60.65 91.75 252 514\n[88] - - 295.8 320.9 8.86 46.85 68.25 - -\n[89] - - 322.8 397.9 - 46.85 71.05 - -\n[90] 1.04 1.35 291 404.6 7.5 46.45 65.05 - -\n[91] - - 279.6 388.9 - 43.65 66.7 - -\n[92] - - 288.4 404.7 9.1 46.1 69.15 - -\n[93] 1.16 1.47 266.1 397.5 8.6 39.4 65.5 - -\n[94] - - 260.9 365.5 10.3 40.25 66.65 - -\n[95] 1.02 1.29 258.4 334.9 8.2 37.7 59.05 - -\n[11] - - - - - - - 132 191\n[57] - - 188.4 315.3 - 35.7 54.75 104 173.6\n[35] - - - - - - - 315 508\n[2] - - - - - - - 277 426\n[87] 1.62 2.1 318.1 439.2 9.4 60.65 91.75 270 478\n[89] - - 322.8 397.9 9.23 46.85 71.05 252 514\n[96] - - - - - - - 228 445\n[97] - - - - - - - 190 277\n[98] - - - - - - - 163 226\n7. Conclusions\nInconclusion,thisreviewpaperprovidesacomprehensiveliteraturereviewoncrowdmonitoring\nusingdifferentmachinelearningtechniquesandmethods. Existingapproachesoncrowdmonitoring\nwerethoroughlyreviewed. Fromthisreview,weconcludedthatScaleDrivenConvolutionalNeural\nAppl.Sci.2020,10,4781 12of17\nNetwork(SD-CNN)andDISAMmodelsaretobeconsideredasnovelmodelsforcrowdcounting\nandlocalizationindensecrowdimageswithhighestaccuracyondifferentdatasets. These models\nhavetheapplicationstodetectthevisibleheadsinanimagewithrespecttoitsscaleanddensitymap.\nExtensiveexperimentsondifferentdatasetsdemonstratethatthesemodelshaveachievedasignificant\nimprovement over the previous models as explained in the literature review section. The future\ndevelopment of deep CNN on crowd monitoring and localization has different opportunities\nandchallenges.\nAuthorContributions:A.K.andJ.A.S.havecollectedandpreparedthedata.A.K.,K.K.andF.K.havecontributed\ntoreviewandanalysis.W.A.hassupervisedtheprocessofthisreview.ThemanuscriptwaswrittenbyA.K.and\nJ.A.S.Allauthorshavereadandagreedtothepublishedversionofthemanuscript.\nFunding: ThisresearchwasfundedbyMinistryofEducationinSaudiArabiathroughprojectnumberQURDO001.\nAcknowledgments: The authors extend their appreciation to the Deputyship for Research and Innovation,\nMinistryofEducationinSaudiArabiaforfundingthisresearchworkthroughtheprojectnumberQURDO001.\nProjecttitle:IntelligentReal-TimeCrowdMonitoringSystemUsingUnmannedAerialVehicle(UAV)Videoand\nGlobalPositioningSystems(GPS)Data.\nConflictsofInterest: Theauthorsdeclarenoconflictofinterest.\nReferences\n1. Choudhary,S.;Ojha,N.;Singh,V.Real-timecrowdbehaviordetectionusingSIFTfeatureextractiontechnique\ninvideosequences.InProceedingsoftheInternationalConferenceonIntelligentComputingandControl\nSystems(ICICCS),Madurai,India,15\u201316June2017;pp.936\u2013940.\n2. Idrees,H.;Saleemi,I.;Seibert,C.;Shah,M.Multi-sourcemulti-scalecountinginextremelydensecrowd\nimages.InProceedingsoftheIEEEConferenceonComputerVisionandPatternRecognition,Portland,OR,\nUSA,23\u201328June2013;pp.2547\u20132554.\n3. Chan,A.B.;Vasconcelos,N.Countingpeoplewithlow-levelfeaturesandBayesianregression.IEEETrans.\nImageProcess.2011,21,2160\u20132177[CrossRef][PubMed]\n4. Bharti,Y.;Saharan,R.;Saxena,A.CountingtheNumberofPeopleinCrowdasaPartofAutomaticCrowd\nMonitoring: ACombinedApproach. InInformationandCommunicationTechnologyforIntelligentSystems;\nSpringer:Berlin/Heidelberg,Germany, 2019;pp.545\u2013552.\n5. Boominathan, L.; Kruthiventi, S.S.; Babu, R.V. Crowdnet: A deep convolutional network for dense\ncrowdcounting. InProceedingsofthe24thACMInternationalConferenceonMultimedia,Amsterdam,\nThe Netherlands,12\u201316October2016;pp.640\u2013644.\n6. Wang, Y.; Zou, Y. Fast visual object countingvia example-based density estimation. InProceedings of\ntheIEEEInternationalConferenceonImageProcessing(ICIP),Phoenix,AZ,USA,25\u201328September2016;\npp.3653\u20133657.\n7. Chen,K.;Loy,C.C.;Gong,S.;Xiang,T.Featureminingforlocalisedcrowdcounting.InProceedingsofthe\nBritishMachineVisionConference2012,Surrey,UK,3\u20137September2012;p.3.\n8. Pham,V.-Q.;Kozakaya,T.;Yamaguchi,O.;Okada,R.Countforest:Co-votinguncertainnumberoftargets\nusingrandomforestforcrowddensityestimation.InProceedingsoftheIEEEInternationalConferenceon\nComputerVision,Santiago,Chile,18February2016.\n9. Zhu,F.;Wang,X.;Yu,N.Crowdtrackingwithdynamicevolutionofgroupstructures.InProceedingsofthe\nEuropeanConferenceonComputerVision,Zurich,Switzerland,6\u201312September2014;pp.139\u2013154.\n10. Shao, J.; Loy, C.C.; Wang, X. Scene-independent group profiling in crowd. In Proceedings of the\nIEEEConferenceonComputerVisionandPatternRecognition, Columbus, OH,USA,23\u201328June2014;\npp.2219\u20132226.\n11. Idrees,H.;Tayyab,M.;Athrey,K.;Zhang,D.;Al-Maadeed,S.;Rajpoot,N.Compositionlossforcounting,\ndensitymapestimationandlocalizationindensecrowds.InProceedingsoftheEuropeanConferenceon\nComputerVision(ECCV),Munich,Germany,8\u201314September2018;pp.532\u2013546.\n12. Deep,S.;Zheng,X.;Karmakar,C.;Yu,D.;Hamey,L.;Jin,J.ASurveyonAnomalousBehaviorDetectionfor\nElderlyCareusingDense-sensingNetworks.IEEECommun.Surv.Tutor.2020,22,352\u2013370.[CrossRef]\nAppl.Sci.2020,10,4781 13of17\n13. Ito,R.;Tsukada,M.;Kondo,M.;Matsutani,H.AnAdaptiveAbnormalBehaviorDetectionusingOnline\nSequentialLearning.InProceedingsofthe2019IEEEInternationalConferenceonComputationalScience\nandEngineering(CSE)andIEEEInternationalConferenceonEmbeddedandUbiquitousComputing(EUC),\nNewYork,NY,USA,1\u20133August2019;pp.436\u2013440.\n14. Sindagi,V.A.;Patel,V.M.Asurveyofrecentadvancesincnn-basedsingleimagecrowdcountinganddensity\nestimation.PatternRecognit.Lett.2018,107,3\u201316.[CrossRef]\n15. Zeng,L.;Xu,X.;Cai,B.;Qiu,S.;Zhang,T.Multi-scaleconvolutionalneuralnetworksforcrowdcounting.\nInProceedingsoftheIEEEInternationalConferenceonImageProcessing(ICIP),Beijing, China, 17\u201320\nSeptember2017;pp.465\u2013469.\n16. Saqib,M.;Khan,S.D.;Sharma,N.;Blumenstein,M.Personheaddetectioninmultiplescalesusingdeep\nconvolutionalneuralnetworks.InProceedingsoftheInternationalJointConferenceonNeuralNetworks\n(IJCNN),RiodeJaneiro,Brazil,8\u201313July2018;pp.1\u20137.\n17. Shami,M.B.;Maqbool,S.;Sajid,H.;Ayaz,Y.;Cheung,S.-C.S.Peoplecountingindensecrowdimagesusing\nsparseheaddetections.IEEETrans.CircuitsSyst.VideoTechnol.2018,29,2627\u20132636.[CrossRef]\n18. Saleh,S.A.M.;Suandi,S.A.;Ibrahim,H.Recentsurveyoncrowddensityestimationandcountingforvisual\nsurveillance.Eng.Appl.Artif.Intell.2015,41,103\u2013114.[CrossRef]\n19. Zhang,Y.;Zhou,C.;Chang,F.;Kot,A.C.Multi-resolutionattentionconvolutionalneuralnetworkforcrowd\ncounting.Neurocomputing2019,329,144\u2013152.[CrossRef]\n20. Liu,J.;Gao,C.;Meng,D.;Hauptmann,A.G.Decidenet:Countingvaryingdensitycrowdsthroughattention\nguideddetectionanddensityestimation.InProceedingsoftheIEEEConferenceonComputerVisionand\nPatternRecognition,SaltLakeCity,UT,USA,18\u201323June2018.\n21. Luo,J.;Wang,J.;Xu,H.;Lu,H.Real-timepeoplecountingforindoorscenes.SignalProcess.2016,124,27\u201335.\n[CrossRef]\n22. Zhu,L.;Li,C.;Yang,Z.;Yuan,K.;Wang,S.Crowddensityestimationbasedonclassificationactivationmap\nandpatchdensitylevel.NeuralComput.Appl.2019.[CrossRef]\n23. Lempitsky,V.;Zisserman,A.Learningtocountobjectsinimages.InProceedingsoftheAdvancesinNeural\nInformationProcessingSystems,Vancouver,BC,Canada,6\u20139December2010;pp.1324\u20131332.\n24. Xu,B.;Qiu,G.Crowddensityestimationbasedonrichfeaturesandrandomprojectionforest.InProceedings\noftheIEEEWinterConferenceonApplicationsofComputerVision(WACV),LakePlacid,NY,USA,7\u201310\nMarch2016;pp.1\u20138.\n25. Basalamah,S.;Khan,S.D.;Ullah,H.ScaleDrivenConvolutionalNeuralNetworkModelForPeopleCounting\nandLocalizationinCrowdScenes.IEEEAccess2019,7,71576\u201371584.[CrossRef]\n26. Li,W.;Li,H.;Wu,Q.;Meng,F.;Xu,L.;Ngan,K.N.Headnet:Anend-to-endadaptiverelationalnetworkfor\nheaddetection.IEEETrans.CircuitsSyst.VideoTechnol.2020,30,482\u2013494.[CrossRef]\n27. Rodriguez, M.; Laptev, I.; Sivic, J.; Audibert, J.-Y. Density-aware person detection and tracking in\ncrowds. In Proceedings of the 2011 International Conference on Computer Vision, Barcelona, Spain,\n6\u201313November2011;pp.2423\u20132430.\n28. Mehran, R.; Oyama, A.; Shah, M. Abnormal crowd behavior detection using social force model.\nInProceedingsoftheIEEEConferenceonComputerVisionandPatternRecognition, Miami, FL,USA,\n20\u201325June2009;pp.935\u2013942.\n29. Yuan,Y.;Fang,J.;Wang,Q.Onlineanomalydetectionincrowdscenesviastructureanalysis.IEEETrans.\nCybern.2014,45,548\u2013561.[CrossRef][PubMed]\n30. Fagette,A.;Courty,N.;Racoceanu,D.;Dufour,J.-Y.Unsuperviseddensecrowddetectionbymultiscale\ntextureanalysis.PatternRecognit.Lett.2014,44,126\u2013133.[CrossRef]\n31. Kumar,N.; Vaish,A.DominantFlowbasedAttributeGroupingforIndifferentMovementDetectionin\nCrowd.Int.J.Comput.Appl.2014,88,1\u20136.[CrossRef]\n32. Chan,A.B.;Liang,Z.-S.J.;Vasconcelos,N.Privacypreservingcrowdmonitoring:Countingpeoplewithout\npeople models or tracking. In Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition,Anchorage,AK,USA,23\u201328June2008;pp.1\u20137.\n33. Chen,K.;Gong,S.;Xiang,T.;Loy,C.C.Cumulativeattributespaceforageandcrowddensityestimation.\nInProceedingsoftheIEEEConferenceonComputerVisionandPatternRecognition,Portland,OR,USA,\n23\u201328June2013;pp.2467\u20132474.\nAppl.Sci.2020,10,4781 14of17\n34. Zhang,C.;Li,H.;Wang,X.;Yang,X.Cross-scenecrowdcountingviadeepconvolutionalneuralnetworks.\nInProceedingsoftheIEEEConferenceonComputerVisionandPatternRecognition,Boston,MA,USA,\n7\u201312June2015.\n35. Zhang,Y.;Zhou,D.;Chen,S.;Gao,S.;Ma,Y. Single-imagecrowdcountingviamulti-columnconvolutional\nneural network. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,\nLas Vegas,NV,USA,27\u201330June2016.\n36. Shao, J.; Kang, K.; Loy, C.C.; Wang, X. Deeply learned attributes for crowded scene understanding.\nInProceedingsoftheIEEEConferenceonComputerVisionandPatternRecognition,Boston,MA,USA,\n7\u201312June2015;pp.4657\u20134666.\n37. Mohamed,S.A.E.;Parvez,M.T.CrowdModelingBasedAutoActivatedBarriersforManagementofPilgrims\ninMataf.InProceedingsoftheInternationalConferenceonInnovativeTrendsinComputerEngineering\n(ITCE),Aswan,Egypt,19\u201321Februry2019;pp.260\u2013265.\n38. Nasser,N.;Anan,M.;Awad,M.F.C.;Bin-Abbas,H.;Karim,L.Anexpertcrowdmonitoringandmanagement\nframeworkforHajj. InProceedingsoftheInternationalConferenceonWirelessNetworksandMobile\nCommunications(WINCOM),Rabat,Morocco,1\u20134November2017;pp.1\u20138.\n39. Fadhlullah,S.Y.;Ismail,W.Pathlossmodelforcrowdcounting.InProceedingsofthe7thIEEEInternational\nConferenceonControlSystem,ComputingandEngineering(ICCSCE),Penang,Malaysia,24\u201326November\n2017;pp.45\u201348.\n40. Khozium,M.O.;Abuarafah,A.G.;AbdRabou,E.Aproposedcomputer-basedsystemarchitectureforcrowd\nmanagementofpilgrimsusingthermography.LifeSci.J.2012,9,377\u2013383.\n41. Shehzed,A.;Jalal,A.;Kim,K.Multi-PersonTrackinginSmartSurveillanceSystemforCrowdCounting\nandNormal/AbnormalEventsDetection.InProceedingsoftheInternationalConferenceonAppliedand\nEngineeringMathematics(ICAEM),Taxila,Pakistan,27\u201329August2019;pp.163\u2013168.\n42. Sabokrou,M.;Fathy,M.;Hoseini,M.;Klette,R.Real-timeanomalydetectionandlocalizationincrowded\nscenes. InProceedingsoftheIEEEConferenceonComputerVisionandPatternRecognitionworkshops,\nBoston,MA,USA,7\u201312June2015;pp.56\u201362.\n43. Khan,S.D.;Ullah,H.;Uzair,M.;Ullah,M.;Ullah,R.;Cheikh,F.A.Disam:DensityIndependentandScale\nAwareModelforCrowdCountingandLocalization.InProceedingsoftheIEEEInternationalConferenceon\nImageProcessing(ICIP),Taipei,Taiwan,22\u201325September2019;pp.4474\u20134478\n44. Sadiq,F.I.;Selamat,A.;Ibrahim,R.;Krejcar,O.EnhancedApproachUsingReducedSBTFDFeaturesand\nModifiedIndividualBehaviorEstimationforCrowdConditionPrediction.Entropy2019,21,487.[CrossRef]\n45. Rao, A.S.; Gubbi, J.; Palaniswami, M. Anomalous Crowd Event Analysis Using Isometric Mapping.\nInAdvancesinSignalProcessingandIntelligentRecognitionSystems;Springer:Berlin/Heidelberg,Germany,\n2016;pp.407\u2013418.\n46. Fradi, H.; Luvison, B.; Pham, Q.C. Crowd behavior analysis using local mid-level visual descriptors.\nIEEE Trans.CircuitsSyst.VideoTechnol.2016,27,589\u2013602.[CrossRef]\n47. Alginahi,Y.M.;Mudassar,M.;Kabir,M.N.;Tayan,O.AnalyzingtheCrowdEvacuationPatternofaLarge\nDenselyPopulatedBuilding.Arab.J.Sci.Eng.2019,44,3289\u20133304.[CrossRef]\n48. Palanisamy, G.; Manikandan, T. Group Behaviour Profiling for Detection of Anomaly in Crowd.\nIn Proceedings of the International Conference on Technical Advancements in Computers and\nCommunications(ICTACC),Melmaurvathur,India,10\u201311April2017;pp.11\u201315.\n49. Wang,J.; Xu,Z.Crowdanomalydetectionforautomatedvideosurveillance. InProceedingsofthe6th\nInternationalConferenceonImagingforCrimePreventionandDetection(ICDP-15),London,UK,15\u201317\nJuly2015.\n50. Xu,F.;Rao,Y.;Wang,Q.Anunsupervisedabnormalcrowdbehaviordetectionalgorithm.InProceedingsof\ntheInternationalConferenceonSecurity,PatternAnalysis,andCybernetics(SPAC),Shenzhen,China,15\u201317\nDecember2017;pp.219\u2013223.\n51. Gao,M.;Jiang,J.;Ma,L.;Zhou,S.;Zou,G.;Pan,J.;Liu,Z.Violentcrowdbehaviordetectionusingdeep\nlearningandcompressivesensing.InProceedingsoftheChineseControlAndDecisionConference(CCDC),\nNanchang,China,3\u20135June2019;pp.5329\u20135333\n52. Chibloun,A.;Fkihi,S.E.;Mliki,H.;Hammami,M.;Thami,R.O.H.AbnormalCrowdBehaviorDetection\nUsingSpeedandDirectionModels.InProceedingsofthe9thInternationalSymposiumonSignal,Image,\nVideoandCommunications(ISIVC),Rabat,Morocco,27\u201330November2018;pp.197\u2013202.\nAppl.Sci.2020,10,4781 15of17\n53. Yang, M.; Rashidi, L.; Rao, A.S.; Rajasegarar, S.; Ganji, M.; Palaniswami, M.; Leckie, C. Cluster-based\nCrowdMovementBehaviorDetection. InProceedingsoftheDigitalImageComputing: Techniquesand\nApplications(DICTA),Canberra,Australia,10\u201313December2018;pp.1\u20138.\n54. Yimin,D.;Fudong,C.;Jinping,L.;Wei,C.AbnormalBehaviorDetectionBasedonOpticalFlowTrajectoryof\nHumanJointPoints.InProceedingsoftheChineseControlAndDecisionConference(CCDC),Nanchang,\nChina,3\u20135June2019;pp.653\u2013658.\n55. Shri,S.J.;Jothilakshmi,S.VideoAnalysisforCrowdandTrafficManagement.InProceedingsoftheIEEE\nInternationalConferenceonSystem,Computation,AutomationandNetworking(ICSCA),Pondicherry,\nIndia,6\u20137July2018;pp.1\u20136.\n56. Xu,M.;Ge,Z.;Jiang,X.;Cui,G.;Lv,P.;Zhou,B.;Xu,C.Depthinformationguidedcrowdcountingfor\ncomplexcrowdscenes.PatternRecognit.Lett.2019,125,563\u2013569.[CrossRef]\n57. Xu,C.;Qiu,K.;Fu,J.;Bai,S.;Xu,Y.;Bai,X.LearntoScale:GeneratingMultipolarNormalizedDensityMaps\nforCrowdCounting.InProceedingsoftheIEEEInternationalConferenceonComputerVision,Seoul,Korea,\n27October\u20132November2019;pp.8382\u20138390.\n58. Anees,M.V.;Kumar,S.G.DeepLearningFrameworkforDensityEstimationofCrowdVideos.InProceedings\nofthe8thInternationalSymposiumonEmbeddedComputingandSystemDesign(ISED),Cochin,India,\n13\u201315December2018;pp.16\u201320.\n59. Shang,C.;Ai,H.;Bai,B.End-to-endcrowdcountingviajointlearninglocalandglobalcount.InProceedings\noftheIEEEInternationalConferenceonImageProcessing(ICIP),Phoenix,AZ,USA,25\u201328September2016;\npp.1215\u20131219.\n60. Pu, S.; Song, T.; Zhang, Y.; Xie, D. Estimation of crowd density in surveillance scenes based on deep\nconvolutionalneuralnetwork.ProcediaComput.Sci.2017,111,154\u2013159.[CrossRef]\n61. Khan,S.D.; Bandini,S.; Basalamah,S.; Vizzari,G.Analyzingcrowdbehaviorinnaturalisticconditions:\nIdentifyingsourcesandsinksandcharacterizingmainflows.Neurocomputing2016,177,543\u2013563.[CrossRef]\n62. Lian, D.; Li, J.; Zheng, J.; Luo, W.; Gao, S.Densitymapregressionguideddetectionnetworkforrgb-d\ncrowdcountingandlocalization.InProceedingsoftheIEEEConferenceonComputerVisionandPattern\nRecognition,LongBeach,CA,USA,15\u201320June2019;pp.1821\u20131830.\n63. Sam,D.B.;Peri,S.V.;Kamath,A.;Babu,R.V.Locate,SizeandCount:AccuratelyResolvingPeopleinDense\nCrowdsviaDetection.arXiv2019,arXiv:1906.07538.\n64. Xue,Y.;Liu,S.;Li,Y.;Qian,X.CrowdSceneAnalysisbyOutputEncoding.arXiv2020,arXiv:2001.09556.\n65. Tripathi,G.;Singh,K.;Vishwakarma,D.K.Convolutionalneuralnetworksforcrowdbehaviouranalysis:A\nsurvey.Vis.Comput.2019,35,753\u2013776.[CrossRef]\n66. Rohit, K.; Mistree, K.; Lavji, J.Areviewonabnormalcrowdbehaviordetection. InProceedingsofthe\nInternationalConferenceonInnovationsinInformation,EmbeddedandCommunicationSystems(ICIIECS),\nCoimbatore,India,17\u201318March2017;pp.1\u20133.\n67. Lahiri, S.; Jyoti, N.; Pyati, S.; Dewan, J. Abnormal Crowd Behavior Detection Using Image Processing.\nIn Proceedings of the Fourth International Conference on Computing Communication Control and\nAutomation(ICCUBEA),Pune,India,16\u201318August2018;pp.1\u20135.\n68. Wang,T.;Qiao,M.;Zhu,A.;Shan,G.;Snoussi,H.Abnormaleventdetectionviatheanalysisofmulti-frame\nopticalflowinformation.Front.Comput.Sci.2020,14,304\u2013313.[CrossRef]\n69. Krizhevsky,A.;Sutskever,I.;Hinton,G.E.Imagenetclassificationwithdeepconvolutionalneuralnetworks.\nInProceedingsoftheAdvancesinNeuralInformationProcessingSystems, LakeTahoe, ND,USA,3\u20136\nDecember2012.\n70. Simonyan,K.;Zisserman,A.Verydeepconvolutionalnetworksforlarge-scaleimagerecognition. arXiv\n2014,arXiv:1409.1556.\n71. Szegedy,C.;Liu,W.;Jia,Y.;Sermanet,P.;Reed,S.;Anguelov,D.;Erhan,D.;Vanhoucke,V.;Rabinovich,A.\nGoingdeeperwithconvolutions.InProceedingsoftheIEEEConferenceonComputerVisionandPattern\nRecognition,Boston,MA,USA,7\u201312June2015.\n72. Saqib,M.;Khan,S.D.;Sharma,N.;Blumenstein,M.Crowdcountinginlow-resolutioncrowdedscenesusing\nregion-baseddeepconvolutionalneuralnetworks.IEEEAccess2019,7,35317\u201335329.[CrossRef]\n73. Shaban, M.; Mahmood, A.; Al-Maadeed, S.; Rajpoot, N. An information fusion framework for person\nlocalizationviabodyposeinspectatorcrowds.Inf.Fusion2019, 51,178\u2013188.[CrossRef]\nAppl.Sci.2020,10,4781 16of17\n74. Albelwi, S.; Mahmood, A. A framework for designing the architectures of deep convolutional neural\nnetworks.Entropy2017,19,242.[CrossRef]\n75. Lamba, S.; Nain, N.Atexturebasedmani-foldapproachforcrowddensityestimationusingGaussian\nMarkovRandomField.Multimed.ToolsAppl.2019,78,5645\u20135664.[CrossRef]\n76. Ren,S.;He,K.;Girshick,R.;Sun,J.FasterR-CNN:Towardsreal-timeobjectdetectionwithregionproposal\nnetworks.InProceedingsoftheAdvancesinNeuralInformationProcessingSystems,Montreal,QC,Canada,\n7\u201312December2015.\n77. Kumar,S.;Datta,D.;Singh,S.K.;Sangaiah,A.K.Anintelligentdecisioncomputingparadigmforcrowd\nmonitoringinthesmartcity.J.ParallelDistrib.Comput.2018,118,344\u2013358.[CrossRef]\n78. Hu,X.;Zheng,H.;Chen,Y.;Chen,L.Densecrowdcountingbasedonperspectiveweightmodelusinga\nfisheyecamera.Optik2015,126,123\u2013130.[CrossRef]\n79. Karpagavalli,P.;Ramprasad,A.Estimatingthedensityofthepeopleandcountingthenumberofpeoplein\nacrowdenvironmentforhumansafety.InProceedingsoftheInternationalConferenceonCommunication\nandSignalProcessing,Melmaruvathur,India,3\u20135April2013;pp.663\u2013667.\n80. Saeed,S.N.;Abid,A.;Waraich,E.U.;Atta,S.;Naseer,A.;Sheikh,A.A.;Felemban,E.iCrowd\u2014Aframework\nformonitoringofidentifiablecrowd.InProceedingsofthe12thInternationalConferenceonInnovationsin\nInformationTechnology(IIT),AlAin,UAE,28\u201330November2016;pp.1\u20137\n81. Almagbile,A.EstimationofcrowddensityfromUAVsimagesbasedoncornerdetectionproceduresand\nclusteringanalysis.Geo-Spat.Inf.Sci.2019,22,23\u201334.[CrossRef]\n82. Karpagavalli,P.;Ramprasad,A.Anadaptivehybridgmmformultiplehumandetectionincrowdscenario.\nMultimed.ToolsAppl.2017,76,14129\u201314149.[CrossRef]\n83. Yuan,Y.Crowdmonitoringusingmobilephones.InProceedingsoftheSixthInternationalConferenceon\nIntelligentHuman-MachineSystemsandCybernetics,Hangzhou,China,26\u201327August2014;pp.261\u2013264.\n84. Meynberg,O.;Kuschk,G.Airbornecrowddensityestimation.ISPRSAnn.Photogramm.RemoteSens.Spat.\nInf.Sci.2013,49\u201354.[CrossRef]\n85. Rao,A.S.;Gubbi,J.;Marusic,S.;Palaniswami,M.Crowdeventdetectiononopticalflowmanifolds.IEEE\nTrans.Cybern.2015,46,1524\u20131537.[CrossRef][PubMed]\n86. Kang, D.; Ma, Z.; Chan, A.B. Beyond Counting: Comparisons of Density Maps for Crowd Analysis\nTasks\u2014Counting,Detection,andTracking. IEEETrans. CircuitsSyst. VideoTechnol. 2018,29,1408\u20131422.\n[CrossRef]\n87. Sam,D.B.;Surya,S.;Babu,R.V.Switchingconvolutionalneuralnetworkforcrowdcounting.InProceedings\noftheIEEEConferenceonComputerVisionandPatternRecognition(CVPR),Honolulu,HI,USA,21\u201326\nJuly2017.\n88. Sindagi, V.A.; Patel, V.M.Generatinghigh-qualitycrowddensitymapsusingcontextualpyramidcnns.\nInProceedingsoftheIEEEInternationalConferenceonComputerVision,Venice,Italy,22\u201329October2017.\n89. Sindagi,V.A.;Patel,V.M.Cnn-basedcascadedmulti-tasklearningofhigh-levelprioranddensityestimation\nforcrowdcounting. InProceedingsofthe14thIEEEInternationalConferenceonAdvancedVideoand\nSignalBasedSurveillance(AVSS),Lecce,Italy,29August\u20131September2017.\n90. Shen,Z.;Xu,Y.;Ni,B.;Wang,M.;Hu,J.;Yang,X.Crowdcountingviaadversarialcross-scaleconsistency\npursuit.InProceedingsoftheIEEEConferenceonComputerVisionandPatternRecognition,SaltLakeCity,\nUT,USA,18\u201323June2018.\n91. Liu, X.; Weijer, J.; Bagdanov, A.D. Leveraging unlabeled data for crowd counting by learning to rank.\nInProceedingsoftheIEEEConferenceonComputerVisionandPatternRecognition,SaltLakeCity,UT,\nUSA,18\u201323June2018.\n92. Shi,Z.;Zhang,L.;Liu,Y.;Cao,X.;Ye,Y.;Cheng,M.M.;Zheng,G.Crowdcountingwithdeepnegative\ncorrelationlearning.InProceedingsoftheIEEEConferenceonComputerVisionandPatternRecognition,\nSaltLakeCity,UT,USA,18\u201323June2018.\n93. Li,Y.;Zhang,X.;Chen,D.Csrnet: Dilatedconvolutionalneuralnetworksforunderstandingthehighly\ncongestedscenes. InProceedingsoftheIEEEConferenceonComputerVisionandPatternRecognition,\nSalt LakeCity,UT,USA,18\u201323June2018.\n94. Ranjan, V.; Le, H.; Hoai, M. Iterative crowd counting. In Proceedings of the European Conference on\nComputerVision(ECCV),Munich,Germany,8\u201314September2018.\nAppl.Sci.2020,10,4781 17of17\n95. Cao, X.; Wang, Z.; Zhao, Y.; Su, F. Scale aggregation network for accurate and efficient crowd counting.\nInProceedingsoftheEuropeanConferenceonComputerVision(ECCV),Munich,Germany,8\u201314September2018.\n96. Badrinarayanan,V.;Kendall,A.;SegNet,R.C.Adeepconvolutionalencoder-decoderarchitectureforimage\nsegmentation.arXiv 2015,arXiv:1511.00561.\n97. He,K.;Zhang,X.;Ren,S.;Sun,J.Deepresiduallearningforimagerecognition.InProceedingsoftheIEEE\nConferenceonComputerVisionandPatternRecognition,LasVegas,NV,USA,27\u201330June2016.\n98. Huang,G.;Liu,Z.;Maaten,L.v.;Weinberger,K.Q.Denselyconnectedconvolutionalnetworks.InProceedings\noftheIEEEConferenceonComputerVisionandPatternRecognition,Honolulu,HI,USA,21\u201326July2017.\n(cid:13)c 2020bytheauthors.LicenseeMDPI,Basel,Switzerland.Thisarticleisanopenaccess\narticledistributedunderthetermsandconditionsoftheCreativeCommonsAttribution\n(CCBY)license(http://creativecommons.org/licenses/by/4.0/).",
    "metadata": {
      "CreationDate": "D:20200712175102+08'00'",
      "ModDate": "D:20200712175102+08'00'",
      "Producer": "iText\u00ae 7.1.1 \u00a92000-2018 iText Group NV (AGPL-version)"
    },
    "error_message": null,
    "pages": [
      {
        "page_number": 0,
        "text_content": "applied\nsciences\nReview\nCrowd Monitoring and Localization Using Deep\nConvolutional Neural Network: A Review\nAkbarKhan 1,JawadAliShah1,* ,KushsairyKadir1,WaleedAlbattah2\nandFaizullahKhan3\n1 ElectronicSection,UniversitiKualaLumpurBritishMalaysianInstitute,Selangor53100, Malaysia;\nakbar.khan@s.unikl.edu.my(A.K.);kushsairy@unikl.edu.my(K.K.)\n2 DepartmentofInformationTechnology,CollegeofComputer,QassimUniversity,\n51921Buraydah,SaudiArabia;w.albattah@qu.edu.sa\n3 DepartmentofTelecommunicationEngineering,BalochistanUniversityofInformationTechnology,\nEngineeringandManagementSciences,Quetta87300,Pakistan;faizullah.khan@buitms.edu.pk\n* Correspondence:jawad@unikl.edu.my\nReceived:18May2020;Accepted:26June2020;Published:11July2020\nAbstract: Crowd management and monitoring is crucial for maintaining public safety and is an\nimportant research topic. Developing a robust crowd monitoring system (CMS) is a challenging\ntask as it involves addressing many key issues such as density variation, irregular distribution\nof objects, occlusions, pose estimation, etc. Crowd gathering at various places like hospitals,\nparks, stadiums, airports, cultural and religious points are usually monitored by Close Circuit\nTelevision(CCTV)cameras.ThedrawbacksofCCTVcamerasare: limitedareacoverage,installation\nproblems, movability, high power consumption and constant monitoring by the operators.\nTherefore,manyresearchershaveturnedtowardscomputervisionandmachinelearningthathave\novercome these issues by minimizing the need of human involvement. This review is aimed to\ncategorize,analyzeaswellasprovidethelatestdevelopmentandperformanceevolutionincrowd\nmonitoringusingdifferentmachinelearningtechniquesandmethodsthatarepublishedinjournals\nandconferencesoverthepastfiveyears.\nKeywords: crowdmonitoring;crowdcounting;crowddensityestimation;deepconvolutionalneural\nnetworks(DCNN);crowdbehavior\n1. Introduction\nCrowdissameordifferentsetofpeoplearrangedinonegroupandmotivatedbycommongoals.\nTherearetwotypesofcrowdnamelystructuredcrowdandunstructuredcrowd.Intheformer,thedirection\nofthemovementistowardsacommonpointandpeoplearenotinscatteredformwhileinthelatertypethe\ndirectionofthepeopleisnottowardsacommonpointandtheyareusuallyinscatteredform[1].\nCrowdmonitoringhasawiderangeofapplicationssuchas,safetymonitoring,disastermanagement,\ntrafficmonitoringanddesignofpublicspaces.Thisvarietyofapplicationshaveencouragedresearchers\nthroughout numerous fields to develop models for crowd monitoring and associated tasks such as\ncounting [2\u20135], density estimation [6\u20138], tracking [9], scene understanding [10], localization [11] and\nbehaviordetection[12,13].Amongthese,thecrowdcountinganddensityestimationareimportanttasks\nand represent fundamental building blocks for several other applications [14]. There are three most\ncommonlyusedmethodsforcrowdcountingnamely,objectdetectionbasedcounting[15\u201317],clustered\nbasedcounting[18]andregressionbasedcounting[19,20].Inobjectdetectionbasedcounting;theobject\ndetectorsaretrainedtolocalizethepositionofeverypersoninthecrowdforcounting.Clusterbasedcrowd\ncountingconsistsofidentifyingandtrackingvisualfeatures.Featuretrajectoriesthatshowcoherentmotion\nareclusteredandthenumbersofclusterisregardedasaestimationofmovingobjects[21]. Regression\nAppl.Sci.2020,10,4781;doi:10.3390/app10144781 www.mdpi.com/journal/applsci",
        "tables": [
          [
            [
              ""
            ],
            [
              ""
            ],
            [
              ""
            ]
          ]
        ],
        "forms": {},
        "confidence_score": 1.0
      },
      {
        "page_number": 1,
        "text_content": "Appl.Sci.2020,10,4781 2of17\nbasedcountingestimatesthecrowdcountbycarryingoutregressionbetweentheimagefeaturesandcrowd\nsize.Thoughtheregressionbasedmethodsareworkinggoodinsituationsofhighdensityastheycapture\ngeneralizeddensityinformationfromtheimageofcrowd,stillithastwomainlimitationsi.e.,performance\ndegradationduetooverestimatingofcountinlowdensitysituationsandimproperdistributionofcrowdin\nthescene[22].Onemayconsiderthatconventionalcrowdcountingmethodssuchaspatchbasedapproach,\nobjectbasedapproachandrichfeatureapproach[8,23,24],whichdependoneitherdetectionorregression,\narelimitedwhenhandlingrealscenewithunavoidabledensityvariations.However,thesemethodscan\nonlybeusedtoestimateandcountthepeopleinlowdensitysituationsinwhichallpartsofthepeople\narefullyvisible.Theperformanceofthesemethodsdeteriorateswhenappliedinhighdensitysituations.\nAnidealcountingmethodshouldhavetheadaptiveabilitytochoosetheappropriatecountingmode\naccordingtocrowddensity[20]. Exactcrowdcountingandlocalizationareindispensableforhandling\nhighdensitycrowds. Localizationmeanstogettheaccuratelocationoftheheadsinanimage. Head\nis the only visible part through which localization can be found in highly dense crowd images or\nvideos[25]. Althoughnumerousstepshavebeentakeninthedetectionofhumanheads[16,17,26],\nheaddetectionisstillachallengingtask. Asaresultofthevariationinthescaleappearanceofheads,\nitisstillabigproblemtoexactlydiscriminatehumanheadsfromthebackground. Forlocalizationin\ncrowdedscene,densitymaphasbeenusedasaregularizerduringthedetection[27]. Incomputer\nvision,crowdbehaviordetectioninvideosurveillanceisoneofthelatestresearchareas[1]. Crowd\nbehavior detection has many application domains such as automatic detection of riots or chaotic\nactsincrowdandlocalizationofabnormalregions[28]. Detectionofcrowdbehaviorisextensively\nusedinordertomonitorandmaintainthesurveillanceofpublicplaceslikesportsevents,markets,\nreligiousandpoliticalgatherings,etc. Therearetwocategoriesofcrowdbehaviordetectionnamely\nglobalcrowdbehaviordetectionandlocalcrowdbehaviordetection[29]. Inglobalcrowdbehavior\ndetection a large area is affected, while in the local crowd behavior detection it affects the limited\nareaofthecrowdandthebehaviorofanindividualisquitedifferentfromitsneighbor. Multiscale\ntextureanalysisisusedforassessingthebehaviorofcrowdinvideosequences[30]. Theaimofvideo\nsurveillanceisusedtodetectabnormalhumanbehavior[31].\n1.1. Rational\nReviewingtheliteratureisoneofthemostimportantactivitiesinresearch.Thispaperisthefirstpaper\ninaseriesofresearchpapersinthefieldofcrowdmonitoring.Accordingtoourplanofcurrentworking\nonasubstantialcrowdmonitoringprojectfundedbyMinistryofEducationinSaudiArabia,itisavery\ncrucialphasetostudytheliteratureandanalyzeitinordertoaddressdifferentaspectsofthesubjectunder\ninvestigations.Anycontributioninanysubjectorfieldcannotbeachievedwithaconsiderableknowledge\nofthestateoftheart.Webelievethispaperwillprovideusaswellasinterestedresearcherswithoverview\nofexistingstudiesinthefieldofcrowdmonitoringandmanagement.Wealsoexpectthisreviewprovides\nanovelsynthesisofthecurrentresearchworks,whichwehopecanleadtoanewmeansofconsidering\ncrowdmonitoringaswellasfindinganyavailablegaps.\n1.2. Datasets\nVariousdatasetscontainingcrowdvideosandimagesarepubliclyavailableandarebeingusedto\nvalidatetheexperimentalresults. Someofthepubliclyavailabledatasetsalongwithitsdescriptionare\nshowninTable1.\nUCSDdatasetwasthefirstdatasetusedforpeoplecounting[32]. Thedatahasobtainedthrough\na camera installed on a pedestrian pathway. The dataset includes 2000 frames 238 \u00d7 158 of video\nsequences,alongwithgroundtruthannotationofeachpedestrianineveryfifthframehaving49,885\npedestrianintotal. TheMalldatasethasbeencollectedbymeansofsurveillancecamerasinstalled\ninashoppingmall[33]. Ithasatotalof2000frameswithsizeof320\u00d7240. UCF_CC_50dataset[2]\nisachallengingdatasetcomprisingofawidevarietyofdensitiesandvariousscenes. Thisdataset\nhas been obtained from different places like concerts, political protests, stadiums and marathons.",
        "tables": [],
        "forms": {},
        "confidence_score": 1.0
      },
      {
        "page_number": 2,
        "text_content": "Appl.Sci.2020,10,4781 3of17\nTheentirenumbersofannotatedimagesare50containing1279individualsperimage. Thisdataset\nhasavaryingresolutionandtheindividualsdifferfrom94to4543representingalargevariationinthe\nimage. Thedrawbackofthistypeofdatasetisthat,thereisonlylimitednumberofimagesavailablefor\ntrainingandevaluation. WorldExpo\u201910datasetintroducedin[34]hasbeenusedforcrossscenecrowd\ncounting. Thedatasetcomprisesof3980framesofsize576\u00d7720with199,923labeledpedestrians.\nThemaximumcrowdcountthroughthisdatasetislimitedto220andisinsufficientforevaluating\nextremelydensecrowdscounting. TheShanghaiTechdataset[35]hasbeenintroducedforlargescale\ncrowdcountingcontainingof1198imageswith330,165annotatedheads. Intermsofannotatedheads,\nthisdatasetisoneofthelargest. Thedatasetcontainstwotypes,namelyPartA,PartB.PartAismade\nof482imagestakenfromtheinternetrandomly.WhereasPartBcomprisesof716imagescollectedfrom\nthemetropolitanstreetinshanghai. ThemostrecentdatasetisUCF-QNRF[11]having1535images.\nWithinthisdataset,thenumberofpeoplefluctuatesfrom49to12,865makingamassivevariation\nindensity. Furthermore,ithasahugeimageresolutionrangingfrom400\u00d7300to9000\u00d76000and\nconsists of crowd videos with varying densities and perspective scales. CUHK dataset has been\ncollected from diverse locations namely, street, shopping malls, airports and parks. The dataset\ncomprisesof474videosclipsfrom215scenes[36].\nTable1.Descriptionofdatasets.\nOverall\nDatasets Description No.ofImages Resolution Min Ave Max Accessibility\nCount\nUCSD Peoplecounting 2000 238\u00d7158 11 25 46 49,885 Yes\nMALL Peoplecounting 2000 320\u00d7240 13 - 53 62,325 Yes\nUCF_CC_50 Densityestimation 50 Variable 94 1279 4543 63,974 Yes\nCrossscene\nWorldExpo\u201910 3980 576\u00d7720 1 50 253 199,923 Yes\ncrowdcounting\nVariable,\nShanghaiTechA,B Crowdcounting 482 33 501 3139 241,677 Yes\n768\u00d71024\nCrowdcounting 400\u00d7300to\nUCF-QNRF 716 9 123 578 88,488 Yes\nandlocalization 9000\u00d76000\nCUHK Crowdbehavior 1535 Variable 49 815 12,865 - Yes\nThis review paper is mainly focusing on the crowd monitoring (crowd counting,\ncrowdlocalizationandbehaviordetection). Therestofthepaperisorganizedasfollows. Section2\nshowssearchmethodologyandTaxonomyLevel. InSection3,crowdmonitoringapproachestaken\nfromthepreviousliteraturearesummarizedintabularform. Section4showscrowdrelatedresearch\napproaches. Section5showstheconvolutionalneuralnetworkanddeepCNNframeworks. Section6\nrepresentsthediscussionandSection7elaboratesconclusions.\n2. SearchMethodologyandTaxonomyLevel\nFigure 1 shows the growth of published papers on crowd monitoring (Crowd counting,\nlocalization and behavior) using different machine learning methods and techniques. The graphs\nshowthetwomaindatabasesofscopusandwebofscienceinwhichtheresearchpapershavebeen\npublishedfromyear2014to2019. Wehavesearchedcrowdmonitoringkeywordinbothdatabases\nandfounddifferentpaperswithdifferentmethodsandtechniques. Thedetailsofthepaperspublished\ninbothdatabasesareshowningraphicalviewasshowninFigure1.\nTaxonomyLevel\nTheentiretaxonomylevelofcrowdmonitoringhasbeenshownintheformofflowchart.Basically\nthecrowdrelatedresearchapproacheshavebeencategorizedintotwodomainsbasedontheliterature\nreview namely crowd management and crowd monitoring. Then we have made categories of crowd\nmonitoringi.e.,counting,localizationandbehavior. Focusingoncrowdmonitoring,wejustreviewthe",
        "tables": [],
        "forms": {},
        "confidence_score": 1.0
      },
      {
        "page_number": 3,
        "text_content": "Appl.Sci.2020,10,4781 4of17\ncrowdmanagementanddidnotcategorizeitfurther. Afterthat,thecountingsectionhasbeendivided\nintotwopartsnamelydensityestimationandpeoplecounting. Inlocalizationtherearethreemainsub\ncategoriesi.e.,countingandlocalization,estimationandlocalizationandanomalydetectionandlocalization.\nFinally,thebehaviorcategoryhasbeendividedintothreesubcategoriesi.e.,individualbehaviorestimation,\nanomalousbehaviordetectionandnormalandabnormalbehaviordetection.Thetaxonomylevelofcrowd\nmonitoring(crowdcounting,localizationandbehavior)hasbeenshowninFigure2.\nWeb of science Scopus No.of papers\n30 27\n2019 8\n25\n19 20 2018 7\n20 18\n16\n14 2017 10\n15 12 12 13\n10 2016 6\n10 7 8\n2015 5\n5\n2014 2\n0\n2014 2015 2016 2017 2018 2019 0 2 4 6 8 10 12\na) Web of science and scopus b) Machine learning on crowd monitoring\nFigure1.(a)Thelistofresearchpaperspublishedinwebofscienceandscopusoncrowdmonitoring\nfrom2014\u20132019while(b)showsmachinelearningoncrowdmonitoring\nCrowd Related\nResearch\nApproaches\nCrowd Crowd\nManagement Monitoring\nCounting Localization Behavior\nCounting Individual\nE D si e t n m s a it t y io n c P o e u o n p t l i e n g loca a li n z d at ion es b t e im ha a v t i i o o r n\nEstimation Anomalous\nand behavior\nlocalization detection\nAnomaly Normal and\ndetection abnormal\nand behavior\nLocalization detection\nFigure2.Crowdrelatedresearchapproaches.\n3. CrowdMonitoring(CM)Approaches\nTable2showscompletelyaboutalltheresearchpapersaboutcrowdcounting,localizationand\nbehaviordetection. Differentresearchpaperswithakeyword\u201ccrowdmonitoring\u201dhavebeensearched\nwhileusingtwomainlyuseddatabasesi.e.,webofscienceandscopusfrom2014\u20132019,respectively.\nWehavefounddifferentmethodsandtechniquesrelatedtocrowdmonitoringandtriedtoreview\nitcompletely. Furthertheelaborationofthecrowdcounting, localizationandbehaviorhavebeen\ndeliberatelydiscussedintabularformasshowninTable2.",
        "tables": [
          [
            [
              "Web of science Scopus\n30 27\n25\n20\n19\n20 18\n16\n14\n15 12 12 13\n10\n10 7 8",
              "No.of papers\n2019 8\n2018 7\n2017 10\n2016 6"
            ],
            [
              "5\n0\n2014 2015 2016 2017 2018 2019",
              "2015 5\n2014 2\n0 2 4 6 8 10 12"
            ]
          ],
          [
            [
              "8\n7\n10\n6"
            ],
            [
              "5\n2"
            ]
          ],
          [
            [
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              ""
            ],
            [
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              ""
            ],
            [
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              ""
            ],
            [
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              ""
            ],
            [
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              ""
            ],
            [
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              ""
            ],
            [
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              ""
            ]
          ]
        ],
        "forms": {},
        "confidence_score": 1.0
      },
      {
        "page_number": 4,
        "text_content": "Appl.Sci.2020,10,4781 5of17\nTable2.CrowdMonitoring(CM)approaches.\nRef Process Frameworks/Methods Performance Conclusion/Result\nCombinationofcrowdsizeestimationand Classificationwith\n[4] Crowdmonitoring(Counting) Accuracy90%\ncounting MSE0.0081\nNeuralnetworkandregression BPNNcandeal9\n[37] Crowdmonitoring(Counting) BPNNprovidesthebestestimation\ntreesusingfisheyecamera framesinsecond\nICrowdframeworkwasdesigned\nonathree-layerapproach,device\n[38] Crowdmonitoring(Estimation) Noexperimentalresults Capableoflocationupdates\nlayer,middlewarelayerandthe\napplicationlayer\nAirbornecamerasystems, Dependsongoodtraining Gaborfilterplaysaprominent\n[39] Densityestimation(Estimation)\nsupportvectormachineandGaborfilter samplesandsimilarimages roleinrealscenesofimages\nReal-timecrowddensity\nCrowdmonitoring Informationmanagement Expertsystemmodule\n[40] measurementsand\n(Controllingcrowdmovement) moduleanddecisionsupportsystem performswell\ncommunicationsduringhajj\nCrowdCounting\n[41] MedianfilterandKalmanfilter Accuracy95.5% Robustsmartsurveillancesystem\n(Normal/abnormalevent)\nEstimationand\n[11] DeepCNNnetworks Specificity75.8% Decreaseerrorrate\nlocalization(Densitymap)\nDetectionandlocalization Globalandlocaldescriptors, Achievedgoodandcompetingmethods\n[42] Accuracy99.6%\n(Anomalydetection) withtwoclassifierswereproposed withlowcomputationalcomplexity\nDISAMoutperformsforUCSD\nCountingandLocalization andWorldExpo\u201910datasetswith\n[43] CNN Reductionofclassificationtime\n(Humanheads) thelowestMAEof1.01and8.65,\nrespectively\nAveragePrecisionandaverage Reductionofclassificationtimeand\n[25] Countingandlocalization SD-CNNModel\nrecallrate73.58and71.68 improvementindetectionaccuracy\n99.1%accuracyand\n[44] Crowdmonitoring(Behavior) EHCAF HighlyaccurateandlowFNR\nFNRof2.8%,\nCrowdmonitoring\n[45] Isometricmapping(ISOMAP) Reducedfeaturespace Reductionofcomputationtime\n(Behaviordetection)",
        "tables": [],
        "forms": {},
        "confidence_score": 1.0
      },
      {
        "page_number": 5,
        "text_content": "Appl.Sci.2020,10,4781 6of17\nTable2.Cont.\nRef Process Frameworks/Methods Performance Conclusion/Result\nVisualdescriptorshaveextractedand\nCrowdbehavior\n[46] Spatio-temporalmodel Accuracy98%and88% consideredforbothindividual\nanalysis(Behavior)\nandinteractivebehaviors\nCrowdevacuation Correlationscoreswere\n[47] LegionEvacsoftware Reducedevacuationtime\n(Evacuationbehavior) positive\nDetectionofanomaly OpticalflowandHorn Computationofdistance Anovelapproachofabnormal\n[48]\n(Normalandabnormal) Schunckalgorithm betweencentroids eventdetectionhasproposed\nTheSTTmethoddemonstrates\ncomparableresultsof\nCrowdbehaviordetection Spatio-Temporal Crowdanomalydetection\n[49] Spatio-temporalCompositions\n(Identifybehavior) Texturemodel frameworkwasintroduced\n(STC)andInferenceby\nComposition(IBC)\nApproximatemedianfilter\nCrowdbehavior Arobustunsupervisedabnormal\n[50] andforeground Lowerfalserate\ndetection(Behavior) crowdbehaviordetectionhasachieved\nsegmentationalgorithm\nCombiningcompressivesensingand\nViolentbehavior Hybridrandommatrix(HRM)\n[51] Accuracy90.17%and91.61% deeplearningtoidentifyviolentcrowd\ndetection anddeepneuralnetwork\nbehavior\nTheperformanceofthis Holisticapproachforabnormal\n[52] Crowdbehaviordetection Holisticapproach\nmethodsyieldsbetterresults crowdbehaviordetectionhasproposed\nThecombinationofSIFTandgenetic\nCrowdbehavior Scale-invariantfeature\n[1] Accuracy95% algorithmhasachievedbettersimulation\ndetection(Realtime) transform(SIFT)\nresults\nCrowdbehaviormonitoring Fixed-widthclustering Accuracyisbetween Theapproachhasasuperior\n[53]\n(Eventdetection) algorithmandYOLO 80%-95.7% performanceonsixvideos\nAbnormalbehavior Opticalflowmethod\n[54] 87.4%accuracy Higherdetectionrateforanomaly\ndetection(Abnormality) andSVM",
        "tables": [],
        "forms": {},
        "confidence_score": 1.0
      },
      {
        "page_number": 6,
        "text_content": "Appl.Sci.2020,10,4781 7of17\n4. CrowdRelatedResearchApproaches\nAfter going through an important number of papers, it has been determined that the overall\nworkcanbedividedintotwocategories,namelycrowdmanagingandcrowdmonitoring(counting,\nlocalizationandbehavior). Hereisabriefdescription:\n4.1. CrowdManagement\nCrowdmanagementhasmadeenormousprogressoverthelastfewyears. Withintheliterature,\nvariousmodelshavebeenproposed. Likein[37],theauthorshaveproposedaFiniteStateMachine\n(FSM)modeltosimulatethemovementofcrowdduringTawaf(tomovearoundtheKaabaseven\ntimesaspartofthehajjinMecca). ThemodelcanbeusedtomonitorandmanagecrowdinMataf\n(place of Tawaf) in real time. Similarly in [38], the authors have proposed a framework weighted\nroundrobintoovercomethecongestionandovercrowdedduringhajj(pilgrimage). Theframework\nwasdesignedtobeproactiveinaccuratelypredictingpotentialproblemsthroughtheuseofsmart\nmonitoringofeachpathofritualslocations. Amodelhasplannedtocountthenumberofpeople\nbyusingnon-participatory(non-intrusive)techniquesupportedbystatisticalapproachbyDesign\nofExperiments(DOE)forcrowdsafetyandmanagement[39]. In[40],aninformationmanagement\nmodule and decision support system was used to monitor and manage the crowd. The proposed\nframework provides an automated approach for detecting and evaluating the video scene and\nclassifyingcrowdsandtrafficmanagement[55].\n4.2. CrowdMonitoring\nCrowd monitoring can further be categorized into crowd counting, crowd localization and\ncrowdbehavior.\n4.2.1. CrowdCounting\nCountingmeansspecificallycountthenumberofpeopleinthecrowd. Thecrowdcountinghas\nbeendiscussedbymanyauthorsintheliterature. Khanetal. [43]proposesanovelheadcountingand\nlocalizationtechnique,DensityIndependentandScaleAwareModel(DISAM),thatperformswellfor\nhighdensitycrowedwherehumanheadistheonlyvisiblepartintheimages. CNNisfirstusedas\nheaddetectorandlaterforcomputingresponsematrixfromthescaleawareheadproposalstoobtain\ntheprobabilitiesofheadintheimages. In[56],youonlylookonce(YOLO)isadetectiontechnique\nwhichisbroadlyusedforthedetectionofobjectsinanimagewithhighlevelofperspectivevalues\ni.e.,maximumthresholdvalue. Xuetal. [57]recommendedCNNandlearntoscalethatgenerate\nmultipolarnormalizeddensitymapsforcrowdcounting. Itextractsapatch-leveldensitymapby\naprocessofdensityestimationandclustersthenintomultiplelevelsofdensity. Eachpatchdensity\nmap is normalized via an online learning strategy for the center with multi polar loss. In [58] the\ncrowddensityofsurveillancevideosismeasuredusingCNNandshorttermmemory. Twoclassical\ndeepconvolutionalnetworksnamelyGooglenet[59]andVGGnet[5]wereusedforestimatingcrowd\ndensity [60]. Similarly, [4] first approximate crowd size estimation and secondly count the exact\nnumberofpeopleinthecrowd. Theefficiencyremainsunchangedinthetermsofitsaccuracyof(90%).\n4.2.2. CrowdLocalization\nLocalizationofcrowdsincrowdedimagesreceivedlessattentionfromtheresearchcommunity.\nWithlocalizationinformation,onecanfigureouthowpeoplearedistributedinthearea,whichisvery\nimportantforcrowdmanagers[43]. Informationaboutlocalizationcanbeusedtodetectandmonitor\nanindividualindensecrowds [61]. Inordertoidentifythelocationofheadinanimagearegression\nguideddetectionnetwork(RDNet)hasproposedforRGB-Datasetsthatcansimultaneouslyestimate\nheadcountsandlocalizeheadswithboundingboxes[62]. Similarlyin[27],adensitymaphasbeen\nusedtolocalizetheheadsindenseimagewithaccurateresults. In[63],localizationhasbeenidentified",
        "tables": [],
        "forms": {},
        "confidence_score": 1.0
      },
      {
        "page_number": 7,
        "text_content": "Appl.Sci.2020,10,4781 8of17\nwhileusingLSC-CNNwiththehelpofametricnamedasMeanLocalizationError(MLE).Thismodel\nhasachievedaremarkableperformanceintermsofitsaccuracy. CompressedSensingbasedOutput\nEncoding(CSOE)hasbeenproposedwhichcanhelptoimprovetheefficiencyoflocalizationinhighly\ndensecrowdedsituation[64].\n4.2.3. CrowdBehavior\nCrowdbehavioranalysisanddetectionhavebecomeaprimaryparteverywhereforpeaceful\neventorganization[65]. Thedifficultiesofbehavioridentificationandabnormalbehaviordetecting\nareveryimportantissuesinvideoprocessing[66]. Theresearchershaveproposeddifferentmethods\nandtechniquesforthecrowdbehaviordetection. Someofthecloserelatedworksareelaboratedhere.\nIn[66,67],imageprocessingwithopticalflowandmotionhistoryimagetechniqueswereusedtodetect\nthebehaviorofcrowd. Similarlyin[54],anopticalflowmethodwithSupportVectorMachine(SVM)\nwasusedforabnormalbehaviordetection. In[68],CascadeDeepAutoEncoder(CDA)andwiththe\ncombinationofmulti-frameopticalflowinformationhavebeenproposedforthedetectionofcrowd\nbehavior. IsometricMapping(ISOMAP)[45],spatio-temporal[46]andspatio-temporaltexture[49]\nmodelswereusedtodetecttheanomalouscrowddetection. In[51]HybridRandomMatrix(HRM)\nanddeepneuralnetworkwereusedforthedetectionofviolentbehaviordetection. OneusesSIFT\nfeatureextractiontechnique[1]andFixed-widthclusteringalgorithmwithYOLOwereusedtodetect\ncrowdbehavior[53].\n5. CNNandDeepCNNFrameworks\nDeep CNNs are special types of Artificial Neural Networks (ANNs) that learn hierarchical\nrepresentationfromthespatialinformationcontainedindigitalimages. Itwasoriginallydesignto\nprocessmultidimensional(2Dand3D)arraysofhighresolutioninputdatasetssuchasimagesand\nvideos[69\u201371]. ThefirstdeepCNNarchitecturewasAlexNet[69]havingsevenhiddenlayerswith\nmillionsofparameters. Deepconvolutionalneuralnetworkshaveachievedgreatsuccessonimage\nclassification[70],objectdetection[71],crowdcounting[72]andpeoplelocalization[73]. Thebrief\nstructureofdeepCNNisshowninFigure3.\nFigure3.Structureofdeepconvolutionalneuralnetworks(CNN)[74].\nThesuccessofconvolutionalneuralnetworks(CNN)anddeepconvolutionalneuralnetworks\n(DCNN)invariouscomputervisiontaskshasinspiredresearcherstoleveragetheirabilitytolearn",
        "tables": [],
        "forms": {},
        "confidence_score": 1.0
      },
      {
        "page_number": 8,
        "text_content": "Appl.Sci.2020,10,4781 9of17\nnonlinearfunctionsfromcrowdimagestotheirrespectivedensitymapsorcounts[75]. Avarietyof\nCNNsandDCNNshavebeenproposedintheliteraturewhichareusedforcrowdmonitoring. Recent\nstudiesareincludedinTable3whichrepresentsdifferentmethodsandtechniquesusedforcrowd\nmonitoringi.e.,crowdcounting,densityestimationandlocalization.Theselectionofresearchpapersis\nfrom2017-2019respectivelyinwhichdeepconvolutionalneuralnetworks,scaledrivenconvolutional\nnetworksandsimpleconvolutionalneuralnetworkshavebeenused. Theexperimentalresultswere\nevaluatedusingdifferentdatasetsnamelyUCSD[32],worldexpo\u201910[19],UCF-CC-50[2],Shanghai\nTechPartA,B[35],UCF-QNRF[11]andsubway-carriage[60]datasets. Furtherdetailsarepresentedin\nTable3.\nTable3.CNNanddeepCNN\nMethods/\nRef CrowdMonitoring Datasets Researchfocus Accuracy\nTechniques\nUCSD,world\nPeoplecounting SD-CNN Reducedthe\n[25] Expo\u201910and Countandlocalize\nandlocalization model classificationtime\nUCF-CC-50\nCrowdcounting UCSDand Detection,estimation Reducedthe\n[43] DISAM\nandLocalization Worldexpo\u201910 andlocalization classificationtime\nShanghaiTechA,B, Largevariationin\n4.2%,14.3%,27.1%and\n[57] Crowdcounting SPN+L2SM UCF-CC-50 densityforcrowd\n20.1%ratesofMAE\nandUCF-QNRF counting\nShanghaiTechPartA, Decreasedthe\nCounting,estimation\nEstimationand ShanghaiTechB, errorrateof\n[11] Compositionloss ofdensitymap\nlocalization UCF-CC-50and compositional\nandlocalization.\nUCF-QNRFdatasets loss.\nCrowdmanaging\n[60] DeepCNN Subway-carriagescenes Crowddensityestimation 91.73%\nandmonitoring\n6. Discussion\nThis section describes the pairwise comparison of various methods and test datasets.\nFor comparison we have selected some state of the art models which are widely used for crowd\nmonitoring. Table 4 contains the summary of pairwise comparison using MAE and MSE as\nbenchmarkingparameters. TheCNNbasedcrowdcountingandlocalizationalgorithmspresented\nin[8,22,34,35,76]arecomparedwithScaleDrivenConvolutionalNeuralNetwork(SD-CNN)using\nUCSD, UCF-CC-50, WorldExpo\u201910 and ShanghaiTech Part A, B datasets. The MAE and MSE of\nSD-CNN are lesser than those of other models on UCSD, UCF-CC-50 and WorldExpo\u201910 datasets,\nrespectively. Comparingthesemodels,SD-CNNhastheabilitytocountandlocalizethehumanheads\ninbothlowandhighleveldensitycrowdimages. Similarly,DensityIndependentandScaleAware\nModel(DISAM)hasthelowestMAEascomparedtoothermodelstestedonUCSDandWorldExpo\u201910\ndatasets. Unlikepreviousmodelswhichcanonlycountthepeopleindensecrowd,DISAMhasthe\nabilitytohandlebothcountingandlocalizingpeopleinthedensecrowd. Finally,wehavecompared\nSD-CNNwithDISAMondifferentdatasetsandconcludedthatSD-CNNhaslowerrateofMSEon\nUCSDdatasetandMAEonWorldExpo\u201910,respectively.\nIncrowdmonitoringandcountingproblems,researcherstrytoexplorethedomainandhave\napplieddifferentmachinelearningtechniquesandmethodologiestocountandlocalizethecrowdas\nwellastheiranomalousbehavior. In[4]theauthorshavepresentedtheapproximatesizeestimation\nandcountingtheaccuratenumberofpeopleinthecrowd. Themodelhasachieved90%accuracyand\nithasbeenshownthattheefficiencyisnotaffectedbyincreasingthenumberofpeople. TheKalman\nfilteringapproachandKL-divergencetechniquehavebeenused[77]tomonitorandcountthecrowd\ninsmartcity. Similarlytomonitorandestimatecrowddensity,deepconvolutionalnetworkhasbeen\nusedin[60]. Therearethreedifferentclassifiersnamelymultiplelinearregressions,backpropagation\nneuralnetworkandregressiontreeswhichhavebeenappliedandusedin[78]tomonitorandcount\nthenumberofpeopleindensecrowd. In[79]theproposedmodelhasbeendividedintotwofolds;",
        "tables": [],
        "forms": {},
        "confidence_score": 1.0
      },
      {
        "page_number": 9,
        "text_content": "Appl.Sci.2020,10,4781 10of17\nfirstly,toproposedensityestimationofthecrowdandsecondly,tocountthenumberofpeopleusing\nK-Gaussian Mixture Model (GMM). A new model has been proposed namely identifiable crowd\nmonitoring(iCrowd)usingthreelayersapproachi.e.,devicelayer,middlewarelayerandapplication\nlayer to identify and monitor the crowd [80]. A Feature From Accelerated Segment Test (FAST)\nalgorithmisintroducedin[81]todetectandestimatethenumberofpeopleinacrowd. Thedeep\nlearning frameworks like CNN and Long Short Term Memory (LSTM) have been used in [58] for\ncrowd density estimation. In [38] an expert crowd control and management system for hajj has\nbeen used with three strategies i.e., to address congestion and overcrowded situation using; First\nIn First Out (FIFO), priority queuing and Weighted Round Robin (WRR). An automatic multiple\nhumandetectionmethodusinghybridadaptiveGaussianmixturemodelwasintroducedin[82]for\nhumandetection. Theefficiencyofproposedmethodhasfurtherevaluatedandanalyzedbyusing\nReceiverOperating/OutputCharacteristics(ROC),MeanAbsoluteError(MAE)andMeanRelative\nError (MRE). The proposed method has shown better results. The crowd can also be monitored\nandcountedbyusingmobilephonesandadoptingclusteringmethods. Throughthesemethodsthe\nmodelhasachieved92%accuracy[83]. TheairbornecamerasystemswiththetechniquesofSupport\nVectorMachine(SVM)andGaborfiltershavealsobeenusedin[84]forcrowdmonitoringanddensity\nestimation. Thequalityofresultsclearlydependsupongoodtrainingsamplesandsimilarimages.\nIn[40],the authorshavedevelopedadecisionsupportsystemandinformationmanagementmodule\nfortherealtimecrowddensitymeasurements. Thismodelhasalsobeenimplementedforthecrowd\nmonitoringduringHajj. In[44],theauthorshavepresentedaframeworkforcrowdbehaviorusingan\nEnhancedContext-AwareFrameworkandachievedexperimentalresultswiththeaccuracyof99.1%\nand2.8%ofFalseNegativeRate(FNR)indicatingasignificantimprovementoverthe92.0%accuracy\nandFNRof31.3%oftheBasicContext-AwareFramework(BCF).Thedetectionofanomalouscrowd\nbehaviorhasbeenmonitoredin[45]usingIsometricMapping(ISOMAP).Duringthemonitoringand\ndetectingofcrowdbehaviortheISOMAPhasreducedthecomputationaltimesignificantly.Toquantify\nthecrowdbehavioranalysisaspatio-temporalmodelhasbeenproposedin[46]onCUHKandUMN\ndatasets. Theaccuracyachievedforbothdatasetsare98%and88%,respectively. Asoftwarenamedas\nlegionEvachasbeenusedin[47]forthebehaviorofcrowdevacuation,duringsimulatinglegionEvac\ncalculatesvariousmetricsthatreflectaholisticpatternofcrowdevacuation,whichcapturethebehavior\nofcrowd. In[85],authorshavedevelopedaprobabilisticdetectionofcrowdevents(running,walking,\nsplitting,merging,localdispersionandevacuation)onOpticalFlowManifolds(OFM)usingOptical\nFlowVector(OFV)andOpticalFlowBundles(OFB).Dealingwiththeissueofdifferentbehaviors\ncapturedinsurveillancevideofortheuseofnormalandabnormalbehavioraldetection,clustering\nbasedgroupanalysishasbeenusedin[48]anddescribedcertaingroupbehavior,suchascollectivity,\nuniformityandconflict. In[49],theauthorshaveproposedaSpatialTemporalTexture(STT)model\nwhichcanautomateandidentifycrowdbehaviorundercomplexreallifesituation. Theanticipated\nSTTmethoddemonstratessimilarresultsofSpatio-TemporalComposition(STC)andInferenceby\nComposition(IBC)andusedlesstimeandasmalleramountofsystemmemoryresources. In[50],\ntheauthorshavepresentedanunsupervisedabnormalcrowdbehaviordetectionusingapproximate\nmedianfilterandforegroundsegmentationalgorithms. In[51]theauthorshaveproposedamodel\nthatmaydetectandidentifytheviolentbehaviorofcrowdusinghybridrandommatrixanddeep\nneuralnetwork. In[52],theauthorspresentedanovelmethodfordetectingcrowdbehaviorinvideo\nsequencesusingprobabilitymodelofspeedanddirection. Thismethodcomprisesoftwomainphases;\nbuilding the motion model (speed and direction) and comparing the model of different frames to\ndetectanomalies. AScaleInvariantFeatureTransform(SIFT)techniqueisusedin[1]fordetection\nofcrowdbehaviorinrealtimevideosequences. Similarly, afixedwidthclusteringalgorithmand\nYOLOhavebeenusedin[53]todetectthecrowdbehaviorinvideosurveillance. In[54]theauthors\nhavesuggestedaneffectiveandconcretemethodfordetectingabnormalitiesonthebasisofoptical\nflowpathofthejointpointsforeachhumanbody. Themethodhasanexpressivelyhigherdetection\nrateonthepublicdatasetwith87.4%accuracy. In[41],theauthorshavepresentedauniquemulti",
        "tables": [],
        "forms": {},
        "confidence_score": 1.0
      },
      {
        "page_number": 10,
        "text_content": "Appl.Sci.2020,10,4781 11of17\npersontrackingsystemforcrowdcountingandnormal/abnormalindoorandoutdoormonitoring\nsystemusingmedianandKalmanfilters,andhaveobtained95.5%accuracyineventdetection. In[42]\ntheauthorsproposedasystemforidentificationandlocalizationofanomaliesincrowdedsensein\nrealtime. Theperformancewascalculatedonthebasisofitsaccuracyi.e.,99.6%. Similarly,in[43]\ntheauthorshaveproposedanovelmodelnamelyDensityIndependentandScaleAwaremodelfor\ncrowdcountingandlocalizationinhighlydensecrowdandevaluatedthemodelonMeanAbsolute\nError(MAE).In[25]theScaleDrivenConvolutionalNeuralNetworkmodelhasproposedtocount\nandlocalizethecrowd. Thisstrategyreducedtheclassificationtimesignificantlyandimprovedthe\ndetectionaccuracy. Therearemanycommonproblemsinresearchrelatedtocrowdmonitoringsuch\nasscalevariation,complexbackground,localization,etc. whichneedtobesolvedbyusingdifferent\ntechniques. Forscalevariation,SD-CNNhasbeenproposedinliteraturewiththeassumptionthat\ntheheadistheonlyvisiblefeatureinthecrowd. Inadensecrowd,theissueofscalevariationhas\nbeenaddressedbygeneratingascaleawareobjectproposal. Similarly, forlargedensityvariation,\nlearningtoscalemodelhasalsobeenproposed. Localizationofobjectsincomplexbackgroundisstill\nachallengingtask. DISAMcanbeusedthathastheabilitytopreciselylocalizetheheadsincomplex\nscenes. Localizationperformanceisprimarilyaffectedbychangingthethresholdvalue,sofindingan\noptimumstrategyforthisissueisanewdirectionofresearch.\nTable4.Comparisonofsurveyedmethodsandtestdatasets.\nRef UCSD UCF-CC-50 WorldExpo\u201910 ShanghaiTechPartA,B UCF-QNRF\nMAE MSE MAE MSE MAE MAE MSE MAE MSE\n[34] 1.6 3.31 467 498.5 12.9 - - - -\n[8] 1.61 4.4 235.74 345.6 - - - - -\n[22] 1.03 1.37 302.3 411.6 - 49.25 76.25 - -\n[35] 1.07 1.35 377.6 509.1 11.6 68.3 107.25 - -\n[76] 2.89 9.25 - - 26.87 - - - -\n[25] 1.01 1.28 235.74 345.6 7.42 - - - -\n[34] 1.6 3.31 467 498.5 12.9 - - - -\n[22] 1.03 1.37 302.3 411.6 - - - - -\n[35] 1.07 1.35 377.6 509.1 11.6 68.3 107.25 - -\n[86] 1.17 2.15 406.2 404 14.7 - - - -\n[87] 1.62 2.1 318.1 439.2 9.4 60.65 91.75 - -\n[88] - - 295.8 320.9 8.86 46.85 68.25 - -\n[20] 1.03 - - - 9.23 20.75 29.42 - -\n[43] 1.01 - - - 8.65 - - - -\n[35] - - 377.6 509.1 - 68.3 107.25 277 -\n[87] 1.62 2.1 318.1 439.2 - 60.65 91.75 252 514\n[88] - - 295.8 320.9 8.86 46.85 68.25 - -\n[89] - - 322.8 397.9 - 46.85 71.05 - -\n[90] 1.04 1.35 291 404.6 7.5 46.45 65.05 - -\n[91] - - 279.6 388.9 - 43.65 66.7 - -\n[92] - - 288.4 404.7 9.1 46.1 69.15 - -\n[93] 1.16 1.47 266.1 397.5 8.6 39.4 65.5 - -\n[94] - - 260.9 365.5 10.3 40.25 66.65 - -\n[95] 1.02 1.29 258.4 334.9 8.2 37.7 59.05 - -\n[11] - - - - - - - 132 191\n[57] - - 188.4 315.3 - 35.7 54.75 104 173.6\n[35] - - - - - - - 315 508\n[2] - - - - - - - 277 426\n[87] 1.62 2.1 318.1 439.2 9.4 60.65 91.75 270 478\n[89] - - 322.8 397.9 9.23 46.85 71.05 252 514\n[96] - - - - - - - 228 445\n[97] - - - - - - - 190 277\n[98] - - - - - - - 163 226\n7. Conclusions\nInconclusion,thisreviewpaperprovidesacomprehensiveliteraturereviewoncrowdmonitoring\nusingdifferentmachinelearningtechniquesandmethods. Existingapproachesoncrowdmonitoring\nwerethoroughlyreviewed. Fromthisreview,weconcludedthatScaleDrivenConvolutionalNeural",
        "tables": [],
        "forms": {},
        "confidence_score": 1.0
      },
      {
        "page_number": 11,
        "text_content": "Appl.Sci.2020,10,4781 12of17\nNetwork(SD-CNN)andDISAMmodelsaretobeconsideredasnovelmodelsforcrowdcounting\nandlocalizationindensecrowdimageswithhighestaccuracyondifferentdatasets. These models\nhavetheapplicationstodetectthevisibleheadsinanimagewithrespecttoitsscaleanddensitymap.\nExtensiveexperimentsondifferentdatasetsdemonstratethatthesemodelshaveachievedasignificant\nimprovement over the previous models as explained in the literature review section. The future\ndevelopment of deep CNN on crowd monitoring and localization has different opportunities\nandchallenges.\nAuthorContributions:A.K.andJ.A.S.havecollectedandpreparedthedata.A.K.,K.K.andF.K.havecontributed\ntoreviewandanalysis.W.A.hassupervisedtheprocessofthisreview.ThemanuscriptwaswrittenbyA.K.and\nJ.A.S.Allauthorshavereadandagreedtothepublishedversionofthemanuscript.\nFunding: ThisresearchwasfundedbyMinistryofEducationinSaudiArabiathroughprojectnumberQURDO001.\nAcknowledgments: The authors extend their appreciation to the Deputyship for Research and Innovation,\nMinistryofEducationinSaudiArabiaforfundingthisresearchworkthroughtheprojectnumberQURDO001.\nProjecttitle:IntelligentReal-TimeCrowdMonitoringSystemUsingUnmannedAerialVehicle(UAV)Videoand\nGlobalPositioningSystems(GPS)Data.\nConflictsofInterest: Theauthorsdeclarenoconflictofinterest.\nReferences\n1. Choudhary,S.;Ojha,N.;Singh,V.Real-timecrowdbehaviordetectionusingSIFTfeatureextractiontechnique\ninvideosequences.InProceedingsoftheInternationalConferenceonIntelligentComputingandControl\nSystems(ICICCS),Madurai,India,15\u201316June2017;pp.936\u2013940.\n2. Idrees,H.;Saleemi,I.;Seibert,C.;Shah,M.Multi-sourcemulti-scalecountinginextremelydensecrowd\nimages.InProceedingsoftheIEEEConferenceonComputerVisionandPatternRecognition,Portland,OR,\nUSA,23\u201328June2013;pp.2547\u20132554.\n3. Chan,A.B.;Vasconcelos,N.Countingpeoplewithlow-levelfeaturesandBayesianregression.IEEETrans.\nImageProcess.2011,21,2160\u20132177[CrossRef][PubMed]\n4. Bharti,Y.;Saharan,R.;Saxena,A.CountingtheNumberofPeopleinCrowdasaPartofAutomaticCrowd\nMonitoring: ACombinedApproach. InInformationandCommunicationTechnologyforIntelligentSystems;\nSpringer:Berlin/Heidelberg,Germany, 2019;pp.545\u2013552.\n5. Boominathan, L.; Kruthiventi, S.S.; Babu, R.V. Crowdnet: A deep convolutional network for dense\ncrowdcounting. InProceedingsofthe24thACMInternationalConferenceonMultimedia,Amsterdam,\nThe Netherlands,12\u201316October2016;pp.640\u2013644.\n6. Wang, Y.; Zou, Y. Fast visual object countingvia example-based density estimation. InProceedings of\ntheIEEEInternationalConferenceonImageProcessing(ICIP),Phoenix,AZ,USA,25\u201328September2016;\npp.3653\u20133657.\n7. Chen,K.;Loy,C.C.;Gong,S.;Xiang,T.Featureminingforlocalisedcrowdcounting.InProceedingsofthe\nBritishMachineVisionConference2012,Surrey,UK,3\u20137September2012;p.3.\n8. Pham,V.-Q.;Kozakaya,T.;Yamaguchi,O.;Okada,R.Countforest:Co-votinguncertainnumberoftargets\nusingrandomforestforcrowddensityestimation.InProceedingsoftheIEEEInternationalConferenceon\nComputerVision,Santiago,Chile,18February2016.\n9. Zhu,F.;Wang,X.;Yu,N.Crowdtrackingwithdynamicevolutionofgroupstructures.InProceedingsofthe\nEuropeanConferenceonComputerVision,Zurich,Switzerland,6\u201312September2014;pp.139\u2013154.\n10. Shao, J.; Loy, C.C.; Wang, X. Scene-independent group profiling in crowd. In Proceedings of the\nIEEEConferenceonComputerVisionandPatternRecognition, Columbus, OH,USA,23\u201328June2014;\npp.2219\u20132226.\n11. Idrees,H.;Tayyab,M.;Athrey,K.;Zhang,D.;Al-Maadeed,S.;Rajpoot,N.Compositionlossforcounting,\ndensitymapestimationandlocalizationindensecrowds.InProceedingsoftheEuropeanConferenceon\nComputerVision(ECCV),Munich,Germany,8\u201314September2018;pp.532\u2013546.\n12. Deep,S.;Zheng,X.;Karmakar,C.;Yu,D.;Hamey,L.;Jin,J.ASurveyonAnomalousBehaviorDetectionfor\nElderlyCareusingDense-sensingNetworks.IEEECommun.Surv.Tutor.2020,22,352\u2013370.[CrossRef]",
        "tables": [],
        "forms": {},
        "confidence_score": 1.0
      },
      {
        "page_number": 12,
        "text_content": "Appl.Sci.2020,10,4781 13of17\n13. Ito,R.;Tsukada,M.;Kondo,M.;Matsutani,H.AnAdaptiveAbnormalBehaviorDetectionusingOnline\nSequentialLearning.InProceedingsofthe2019IEEEInternationalConferenceonComputationalScience\nandEngineering(CSE)andIEEEInternationalConferenceonEmbeddedandUbiquitousComputing(EUC),\nNewYork,NY,USA,1\u20133August2019;pp.436\u2013440.\n14. Sindagi,V.A.;Patel,V.M.Asurveyofrecentadvancesincnn-basedsingleimagecrowdcountinganddensity\nestimation.PatternRecognit.Lett.2018,107,3\u201316.[CrossRef]\n15. Zeng,L.;Xu,X.;Cai,B.;Qiu,S.;Zhang,T.Multi-scaleconvolutionalneuralnetworksforcrowdcounting.\nInProceedingsoftheIEEEInternationalConferenceonImageProcessing(ICIP),Beijing, China, 17\u201320\nSeptember2017;pp.465\u2013469.\n16. Saqib,M.;Khan,S.D.;Sharma,N.;Blumenstein,M.Personheaddetectioninmultiplescalesusingdeep\nconvolutionalneuralnetworks.InProceedingsoftheInternationalJointConferenceonNeuralNetworks\n(IJCNN),RiodeJaneiro,Brazil,8\u201313July2018;pp.1\u20137.\n17. Shami,M.B.;Maqbool,S.;Sajid,H.;Ayaz,Y.;Cheung,S.-C.S.Peoplecountingindensecrowdimagesusing\nsparseheaddetections.IEEETrans.CircuitsSyst.VideoTechnol.2018,29,2627\u20132636.[CrossRef]\n18. Saleh,S.A.M.;Suandi,S.A.;Ibrahim,H.Recentsurveyoncrowddensityestimationandcountingforvisual\nsurveillance.Eng.Appl.Artif.Intell.2015,41,103\u2013114.[CrossRef]\n19. Zhang,Y.;Zhou,C.;Chang,F.;Kot,A.C.Multi-resolutionattentionconvolutionalneuralnetworkforcrowd\ncounting.Neurocomputing2019,329,144\u2013152.[CrossRef]\n20. Liu,J.;Gao,C.;Meng,D.;Hauptmann,A.G.Decidenet:Countingvaryingdensitycrowdsthroughattention\nguideddetectionanddensityestimation.InProceedingsoftheIEEEConferenceonComputerVisionand\nPatternRecognition,SaltLakeCity,UT,USA,18\u201323June2018.\n21. Luo,J.;Wang,J.;Xu,H.;Lu,H.Real-timepeoplecountingforindoorscenes.SignalProcess.2016,124,27\u201335.\n[CrossRef]\n22. Zhu,L.;Li,C.;Yang,Z.;Yuan,K.;Wang,S.Crowddensityestimationbasedonclassificationactivationmap\nandpatchdensitylevel.NeuralComput.Appl.2019.[CrossRef]\n23. Lempitsky,V.;Zisserman,A.Learningtocountobjectsinimages.InProceedingsoftheAdvancesinNeural\nInformationProcessingSystems,Vancouver,BC,Canada,6\u20139December2010;pp.1324\u20131332.\n24. Xu,B.;Qiu,G.Crowddensityestimationbasedonrichfeaturesandrandomprojectionforest.InProceedings\noftheIEEEWinterConferenceonApplicationsofComputerVision(WACV),LakePlacid,NY,USA,7\u201310\nMarch2016;pp.1\u20138.\n25. Basalamah,S.;Khan,S.D.;Ullah,H.ScaleDrivenConvolutionalNeuralNetworkModelForPeopleCounting\nandLocalizationinCrowdScenes.IEEEAccess2019,7,71576\u201371584.[CrossRef]\n26. Li,W.;Li,H.;Wu,Q.;Meng,F.;Xu,L.;Ngan,K.N.Headnet:Anend-to-endadaptiverelationalnetworkfor\nheaddetection.IEEETrans.CircuitsSyst.VideoTechnol.2020,30,482\u2013494.[CrossRef]\n27. Rodriguez, M.; Laptev, I.; Sivic, J.; Audibert, J.-Y. Density-aware person detection and tracking in\ncrowds. In Proceedings of the 2011 International Conference on Computer Vision, Barcelona, Spain,\n6\u201313November2011;pp.2423\u20132430.\n28. Mehran, R.; Oyama, A.; Shah, M. Abnormal crowd behavior detection using social force model.\nInProceedingsoftheIEEEConferenceonComputerVisionandPatternRecognition, Miami, FL,USA,\n20\u201325June2009;pp.935\u2013942.\n29. Yuan,Y.;Fang,J.;Wang,Q.Onlineanomalydetectionincrowdscenesviastructureanalysis.IEEETrans.\nCybern.2014,45,548\u2013561.[CrossRef][PubMed]\n30. Fagette,A.;Courty,N.;Racoceanu,D.;Dufour,J.-Y.Unsuperviseddensecrowddetectionbymultiscale\ntextureanalysis.PatternRecognit.Lett.2014,44,126\u2013133.[CrossRef]\n31. Kumar,N.; Vaish,A.DominantFlowbasedAttributeGroupingforIndifferentMovementDetectionin\nCrowd.Int.J.Comput.Appl.2014,88,1\u20136.[CrossRef]\n32. Chan,A.B.;Liang,Z.-S.J.;Vasconcelos,N.Privacypreservingcrowdmonitoring:Countingpeoplewithout\npeople models or tracking. In Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition,Anchorage,AK,USA,23\u201328June2008;pp.1\u20137.\n33. Chen,K.;Gong,S.;Xiang,T.;Loy,C.C.Cumulativeattributespaceforageandcrowddensityestimation.\nInProceedingsoftheIEEEConferenceonComputerVisionandPatternRecognition,Portland,OR,USA,\n23\u201328June2013;pp.2467\u20132474.",
        "tables": [],
        "forms": {},
        "confidence_score": 1.0
      },
      {
        "page_number": 13,
        "text_content": "Appl.Sci.2020,10,4781 14of17\n34. Zhang,C.;Li,H.;Wang,X.;Yang,X.Cross-scenecrowdcountingviadeepconvolutionalneuralnetworks.\nInProceedingsoftheIEEEConferenceonComputerVisionandPatternRecognition,Boston,MA,USA,\n7\u201312June2015.\n35. Zhang,Y.;Zhou,D.;Chen,S.;Gao,S.;Ma,Y. Single-imagecrowdcountingviamulti-columnconvolutional\nneural network. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,\nLas Vegas,NV,USA,27\u201330June2016.\n36. Shao, J.; Kang, K.; Loy, C.C.; Wang, X. Deeply learned attributes for crowded scene understanding.\nInProceedingsoftheIEEEConferenceonComputerVisionandPatternRecognition,Boston,MA,USA,\n7\u201312June2015;pp.4657\u20134666.\n37. Mohamed,S.A.E.;Parvez,M.T.CrowdModelingBasedAutoActivatedBarriersforManagementofPilgrims\ninMataf.InProceedingsoftheInternationalConferenceonInnovativeTrendsinComputerEngineering\n(ITCE),Aswan,Egypt,19\u201321Februry2019;pp.260\u2013265.\n38. Nasser,N.;Anan,M.;Awad,M.F.C.;Bin-Abbas,H.;Karim,L.Anexpertcrowdmonitoringandmanagement\nframeworkforHajj. InProceedingsoftheInternationalConferenceonWirelessNetworksandMobile\nCommunications(WINCOM),Rabat,Morocco,1\u20134November2017;pp.1\u20138.\n39. Fadhlullah,S.Y.;Ismail,W.Pathlossmodelforcrowdcounting.InProceedingsofthe7thIEEEInternational\nConferenceonControlSystem,ComputingandEngineering(ICCSCE),Penang,Malaysia,24\u201326November\n2017;pp.45\u201348.\n40. Khozium,M.O.;Abuarafah,A.G.;AbdRabou,E.Aproposedcomputer-basedsystemarchitectureforcrowd\nmanagementofpilgrimsusingthermography.LifeSci.J.2012,9,377\u2013383.\n41. Shehzed,A.;Jalal,A.;Kim,K.Multi-PersonTrackinginSmartSurveillanceSystemforCrowdCounting\nandNormal/AbnormalEventsDetection.InProceedingsoftheInternationalConferenceonAppliedand\nEngineeringMathematics(ICAEM),Taxila,Pakistan,27\u201329August2019;pp.163\u2013168.\n42. Sabokrou,M.;Fathy,M.;Hoseini,M.;Klette,R.Real-timeanomalydetectionandlocalizationincrowded\nscenes. InProceedingsoftheIEEEConferenceonComputerVisionandPatternRecognitionworkshops,\nBoston,MA,USA,7\u201312June2015;pp.56\u201362.\n43. Khan,S.D.;Ullah,H.;Uzair,M.;Ullah,M.;Ullah,R.;Cheikh,F.A.Disam:DensityIndependentandScale\nAwareModelforCrowdCountingandLocalization.InProceedingsoftheIEEEInternationalConferenceon\nImageProcessing(ICIP),Taipei,Taiwan,22\u201325September2019;pp.4474\u20134478\n44. Sadiq,F.I.;Selamat,A.;Ibrahim,R.;Krejcar,O.EnhancedApproachUsingReducedSBTFDFeaturesand\nModifiedIndividualBehaviorEstimationforCrowdConditionPrediction.Entropy2019,21,487.[CrossRef]\n45. Rao, A.S.; Gubbi, J.; Palaniswami, M. Anomalous Crowd Event Analysis Using Isometric Mapping.\nInAdvancesinSignalProcessingandIntelligentRecognitionSystems;Springer:Berlin/Heidelberg,Germany,\n2016;pp.407\u2013418.\n46. Fradi, H.; Luvison, B.; Pham, Q.C. Crowd behavior analysis using local mid-level visual descriptors.\nIEEE Trans.CircuitsSyst.VideoTechnol.2016,27,589\u2013602.[CrossRef]\n47. Alginahi,Y.M.;Mudassar,M.;Kabir,M.N.;Tayan,O.AnalyzingtheCrowdEvacuationPatternofaLarge\nDenselyPopulatedBuilding.Arab.J.Sci.Eng.2019,44,3289\u20133304.[CrossRef]\n48. Palanisamy, G.; Manikandan, T. Group Behaviour Profiling for Detection of Anomaly in Crowd.\nIn Proceedings of the International Conference on Technical Advancements in Computers and\nCommunications(ICTACC),Melmaurvathur,India,10\u201311April2017;pp.11\u201315.\n49. Wang,J.; Xu,Z.Crowdanomalydetectionforautomatedvideosurveillance. InProceedingsofthe6th\nInternationalConferenceonImagingforCrimePreventionandDetection(ICDP-15),London,UK,15\u201317\nJuly2015.\n50. Xu,F.;Rao,Y.;Wang,Q.Anunsupervisedabnormalcrowdbehaviordetectionalgorithm.InProceedingsof\ntheInternationalConferenceonSecurity,PatternAnalysis,andCybernetics(SPAC),Shenzhen,China,15\u201317\nDecember2017;pp.219\u2013223.\n51. Gao,M.;Jiang,J.;Ma,L.;Zhou,S.;Zou,G.;Pan,J.;Liu,Z.Violentcrowdbehaviordetectionusingdeep\nlearningandcompressivesensing.InProceedingsoftheChineseControlAndDecisionConference(CCDC),\nNanchang,China,3\u20135June2019;pp.5329\u20135333\n52. Chibloun,A.;Fkihi,S.E.;Mliki,H.;Hammami,M.;Thami,R.O.H.AbnormalCrowdBehaviorDetection\nUsingSpeedandDirectionModels.InProceedingsofthe9thInternationalSymposiumonSignal,Image,\nVideoandCommunications(ISIVC),Rabat,Morocco,27\u201330November2018;pp.197\u2013202.",
        "tables": [],
        "forms": {},
        "confidence_score": 1.0
      },
      {
        "page_number": 14,
        "text_content": "Appl.Sci.2020,10,4781 15of17\n53. Yang, M.; Rashidi, L.; Rao, A.S.; Rajasegarar, S.; Ganji, M.; Palaniswami, M.; Leckie, C. Cluster-based\nCrowdMovementBehaviorDetection. InProceedingsoftheDigitalImageComputing: Techniquesand\nApplications(DICTA),Canberra,Australia,10\u201313December2018;pp.1\u20138.\n54. Yimin,D.;Fudong,C.;Jinping,L.;Wei,C.AbnormalBehaviorDetectionBasedonOpticalFlowTrajectoryof\nHumanJointPoints.InProceedingsoftheChineseControlAndDecisionConference(CCDC),Nanchang,\nChina,3\u20135June2019;pp.653\u2013658.\n55. Shri,S.J.;Jothilakshmi,S.VideoAnalysisforCrowdandTrafficManagement.InProceedingsoftheIEEE\nInternationalConferenceonSystem,Computation,AutomationandNetworking(ICSCA),Pondicherry,\nIndia,6\u20137July2018;pp.1\u20136.\n56. Xu,M.;Ge,Z.;Jiang,X.;Cui,G.;Lv,P.;Zhou,B.;Xu,C.Depthinformationguidedcrowdcountingfor\ncomplexcrowdscenes.PatternRecognit.Lett.2019,125,563\u2013569.[CrossRef]\n57. Xu,C.;Qiu,K.;Fu,J.;Bai,S.;Xu,Y.;Bai,X.LearntoScale:GeneratingMultipolarNormalizedDensityMaps\nforCrowdCounting.InProceedingsoftheIEEEInternationalConferenceonComputerVision,Seoul,Korea,\n27October\u20132November2019;pp.8382\u20138390.\n58. Anees,M.V.;Kumar,S.G.DeepLearningFrameworkforDensityEstimationofCrowdVideos.InProceedings\nofthe8thInternationalSymposiumonEmbeddedComputingandSystemDesign(ISED),Cochin,India,\n13\u201315December2018;pp.16\u201320.\n59. Shang,C.;Ai,H.;Bai,B.End-to-endcrowdcountingviajointlearninglocalandglobalcount.InProceedings\noftheIEEEInternationalConferenceonImageProcessing(ICIP),Phoenix,AZ,USA,25\u201328September2016;\npp.1215\u20131219.\n60. Pu, S.; Song, T.; Zhang, Y.; Xie, D. Estimation of crowd density in surveillance scenes based on deep\nconvolutionalneuralnetwork.ProcediaComput.Sci.2017,111,154\u2013159.[CrossRef]\n61. Khan,S.D.; Bandini,S.; Basalamah,S.; Vizzari,G.Analyzingcrowdbehaviorinnaturalisticconditions:\nIdentifyingsourcesandsinksandcharacterizingmainflows.Neurocomputing2016,177,543\u2013563.[CrossRef]\n62. Lian, D.; Li, J.; Zheng, J.; Luo, W.; Gao, S.Densitymapregressionguideddetectionnetworkforrgb-d\ncrowdcountingandlocalization.InProceedingsoftheIEEEConferenceonComputerVisionandPattern\nRecognition,LongBeach,CA,USA,15\u201320June2019;pp.1821\u20131830.\n63. Sam,D.B.;Peri,S.V.;Kamath,A.;Babu,R.V.Locate,SizeandCount:AccuratelyResolvingPeopleinDense\nCrowdsviaDetection.arXiv2019,arXiv:1906.07538.\n64. Xue,Y.;Liu,S.;Li,Y.;Qian,X.CrowdSceneAnalysisbyOutputEncoding.arXiv2020,arXiv:2001.09556.\n65. Tripathi,G.;Singh,K.;Vishwakarma,D.K.Convolutionalneuralnetworksforcrowdbehaviouranalysis:A\nsurvey.Vis.Comput.2019,35,753\u2013776.[CrossRef]\n66. Rohit, K.; Mistree, K.; Lavji, J.Areviewonabnormalcrowdbehaviordetection. InProceedingsofthe\nInternationalConferenceonInnovationsinInformation,EmbeddedandCommunicationSystems(ICIIECS),\nCoimbatore,India,17\u201318March2017;pp.1\u20133.\n67. Lahiri, S.; Jyoti, N.; Pyati, S.; Dewan, J. Abnormal Crowd Behavior Detection Using Image Processing.\nIn Proceedings of the Fourth International Conference on Computing Communication Control and\nAutomation(ICCUBEA),Pune,India,16\u201318August2018;pp.1\u20135.\n68. Wang,T.;Qiao,M.;Zhu,A.;Shan,G.;Snoussi,H.Abnormaleventdetectionviatheanalysisofmulti-frame\nopticalflowinformation.Front.Comput.Sci.2020,14,304\u2013313.[CrossRef]\n69. Krizhevsky,A.;Sutskever,I.;Hinton,G.E.Imagenetclassificationwithdeepconvolutionalneuralnetworks.\nInProceedingsoftheAdvancesinNeuralInformationProcessingSystems, LakeTahoe, ND,USA,3\u20136\nDecember2012.\n70. Simonyan,K.;Zisserman,A.Verydeepconvolutionalnetworksforlarge-scaleimagerecognition. arXiv\n2014,arXiv:1409.1556.\n71. Szegedy,C.;Liu,W.;Jia,Y.;Sermanet,P.;Reed,S.;Anguelov,D.;Erhan,D.;Vanhoucke,V.;Rabinovich,A.\nGoingdeeperwithconvolutions.InProceedingsoftheIEEEConferenceonComputerVisionandPattern\nRecognition,Boston,MA,USA,7\u201312June2015.\n72. Saqib,M.;Khan,S.D.;Sharma,N.;Blumenstein,M.Crowdcountinginlow-resolutioncrowdedscenesusing\nregion-baseddeepconvolutionalneuralnetworks.IEEEAccess2019,7,35317\u201335329.[CrossRef]\n73. Shaban, M.; Mahmood, A.; Al-Maadeed, S.; Rajpoot, N. An information fusion framework for person\nlocalizationviabodyposeinspectatorcrowds.Inf.Fusion2019, 51,178\u2013188.[CrossRef]",
        "tables": [],
        "forms": {},
        "confidence_score": 1.0
      },
      {
        "page_number": 15,
        "text_content": "Appl.Sci.2020,10,4781 16of17\n74. Albelwi, S.; Mahmood, A. A framework for designing the architectures of deep convolutional neural\nnetworks.Entropy2017,19,242.[CrossRef]\n75. Lamba, S.; Nain, N.Atexturebasedmani-foldapproachforcrowddensityestimationusingGaussian\nMarkovRandomField.Multimed.ToolsAppl.2019,78,5645\u20135664.[CrossRef]\n76. Ren,S.;He,K.;Girshick,R.;Sun,J.FasterR-CNN:Towardsreal-timeobjectdetectionwithregionproposal\nnetworks.InProceedingsoftheAdvancesinNeuralInformationProcessingSystems,Montreal,QC,Canada,\n7\u201312December2015.\n77. Kumar,S.;Datta,D.;Singh,S.K.;Sangaiah,A.K.Anintelligentdecisioncomputingparadigmforcrowd\nmonitoringinthesmartcity.J.ParallelDistrib.Comput.2018,118,344\u2013358.[CrossRef]\n78. Hu,X.;Zheng,H.;Chen,Y.;Chen,L.Densecrowdcountingbasedonperspectiveweightmodelusinga\nfisheyecamera.Optik2015,126,123\u2013130.[CrossRef]\n79. Karpagavalli,P.;Ramprasad,A.Estimatingthedensityofthepeopleandcountingthenumberofpeoplein\nacrowdenvironmentforhumansafety.InProceedingsoftheInternationalConferenceonCommunication\nandSignalProcessing,Melmaruvathur,India,3\u20135April2013;pp.663\u2013667.\n80. Saeed,S.N.;Abid,A.;Waraich,E.U.;Atta,S.;Naseer,A.;Sheikh,A.A.;Felemban,E.iCrowd\u2014Aframework\nformonitoringofidentifiablecrowd.InProceedingsofthe12thInternationalConferenceonInnovationsin\nInformationTechnology(IIT),AlAin,UAE,28\u201330November2016;pp.1\u20137\n81. Almagbile,A.EstimationofcrowddensityfromUAVsimagesbasedoncornerdetectionproceduresand\nclusteringanalysis.Geo-Spat.Inf.Sci.2019,22,23\u201334.[CrossRef]\n82. Karpagavalli,P.;Ramprasad,A.Anadaptivehybridgmmformultiplehumandetectionincrowdscenario.\nMultimed.ToolsAppl.2017,76,14129\u201314149.[CrossRef]\n83. Yuan,Y.Crowdmonitoringusingmobilephones.InProceedingsoftheSixthInternationalConferenceon\nIntelligentHuman-MachineSystemsandCybernetics,Hangzhou,China,26\u201327August2014;pp.261\u2013264.\n84. Meynberg,O.;Kuschk,G.Airbornecrowddensityestimation.ISPRSAnn.Photogramm.RemoteSens.Spat.\nInf.Sci.2013,49\u201354.[CrossRef]\n85. Rao,A.S.;Gubbi,J.;Marusic,S.;Palaniswami,M.Crowdeventdetectiononopticalflowmanifolds.IEEE\nTrans.Cybern.2015,46,1524\u20131537.[CrossRef][PubMed]\n86. Kang, D.; Ma, Z.; Chan, A.B. Beyond Counting: Comparisons of Density Maps for Crowd Analysis\nTasks\u2014Counting,Detection,andTracking. IEEETrans. CircuitsSyst. VideoTechnol. 2018,29,1408\u20131422.\n[CrossRef]\n87. Sam,D.B.;Surya,S.;Babu,R.V.Switchingconvolutionalneuralnetworkforcrowdcounting.InProceedings\noftheIEEEConferenceonComputerVisionandPatternRecognition(CVPR),Honolulu,HI,USA,21\u201326\nJuly2017.\n88. Sindagi, V.A.; Patel, V.M.Generatinghigh-qualitycrowddensitymapsusingcontextualpyramidcnns.\nInProceedingsoftheIEEEInternationalConferenceonComputerVision,Venice,Italy,22\u201329October2017.\n89. Sindagi,V.A.;Patel,V.M.Cnn-basedcascadedmulti-tasklearningofhigh-levelprioranddensityestimation\nforcrowdcounting. InProceedingsofthe14thIEEEInternationalConferenceonAdvancedVideoand\nSignalBasedSurveillance(AVSS),Lecce,Italy,29August\u20131September2017.\n90. Shen,Z.;Xu,Y.;Ni,B.;Wang,M.;Hu,J.;Yang,X.Crowdcountingviaadversarialcross-scaleconsistency\npursuit.InProceedingsoftheIEEEConferenceonComputerVisionandPatternRecognition,SaltLakeCity,\nUT,USA,18\u201323June2018.\n91. Liu, X.; Weijer, J.; Bagdanov, A.D. Leveraging unlabeled data for crowd counting by learning to rank.\nInProceedingsoftheIEEEConferenceonComputerVisionandPatternRecognition,SaltLakeCity,UT,\nUSA,18\u201323June2018.\n92. Shi,Z.;Zhang,L.;Liu,Y.;Cao,X.;Ye,Y.;Cheng,M.M.;Zheng,G.Crowdcountingwithdeepnegative\ncorrelationlearning.InProceedingsoftheIEEEConferenceonComputerVisionandPatternRecognition,\nSaltLakeCity,UT,USA,18\u201323June2018.\n93. Li,Y.;Zhang,X.;Chen,D.Csrnet: Dilatedconvolutionalneuralnetworksforunderstandingthehighly\ncongestedscenes. InProceedingsoftheIEEEConferenceonComputerVisionandPatternRecognition,\nSalt LakeCity,UT,USA,18\u201323June2018.\n94. Ranjan, V.; Le, H.; Hoai, M. Iterative crowd counting. In Proceedings of the European Conference on\nComputerVision(ECCV),Munich,Germany,8\u201314September2018.",
        "tables": [],
        "forms": {},
        "confidence_score": 1.0
      },
      {
        "page_number": 16,
        "text_content": "Appl.Sci.2020,10,4781 17of17\n95. Cao, X.; Wang, Z.; Zhao, Y.; Su, F. Scale aggregation network for accurate and efficient crowd counting.\nInProceedingsoftheEuropeanConferenceonComputerVision(ECCV),Munich,Germany,8\u201314September2018.\n96. Badrinarayanan,V.;Kendall,A.;SegNet,R.C.Adeepconvolutionalencoder-decoderarchitectureforimage\nsegmentation.arXiv 2015,arXiv:1511.00561.\n97. He,K.;Zhang,X.;Ren,S.;Sun,J.Deepresiduallearningforimagerecognition.InProceedingsoftheIEEE\nConferenceonComputerVisionandPatternRecognition,LasVegas,NV,USA,27\u201330June2016.\n98. Huang,G.;Liu,Z.;Maaten,L.v.;Weinberger,K.Q.Denselyconnectedconvolutionalnetworks.InProceedings\noftheIEEEConferenceonComputerVisionandPatternRecognition,Honolulu,HI,USA,21\u201326July2017.\n(cid:13)c 2020bytheauthors.LicenseeMDPI,Basel,Switzerland.Thisarticleisanopenaccess\narticledistributedunderthetermsandconditionsoftheCreativeCommonsAttribution\n(CCBY)license(http://creativecommons.org/licenses/by/4.0/).",
        "tables": [],
        "forms": {},
        "confidence_score": 1.0
      }
    ]
  },
  {
    "file_path": "pdfs\\6th paper.pdf",
    "total_pages": 9,
    "combined_text": "Hindawi\nMathematical Problems in Engineering\nVolume 2021, Article ID 9975700, 9 pages\nhttps://doi.org/10.1155/2021/9975700\nResearch Article\nWeapon Detection Using YOLO V3 for Smart Surveillance System\nSanam Narejo ,1 Bishwajeet Pandey ,2 Doris Esenarro vargas ,3 Ciro Rodriguez ,4\nand M. Rizwan Anjum 5\n1DepartmentofComputerSystemsEngineering,MehranUniversityofEngineeringandTechnology(MUET),Jamshoro,Pakistan\n2GranSassoScience Institute,L\u2019Aquila,Italy\n3UniversidadNacional FedericoVillarreal,Lima,Peru\n4UniversidadNacional MayordeSanMarcos,Lima,Peru\n5Departmentof ElectronicEngineering,TheIslamiaUniversityofBahawalpur,Bahawalpur 63100,Pakistan\nCorrespondenceshouldbeaddressedtoBishwajeetPandey;dr.pandey@ieee.org\nReceived 4 March 2021; Revised 15 April 2021; Accepted 3 May 2021; Published 12 May 2021\nAcademicEditor:ZainAnwarAli\nCopyright\u00a92021SanamNarejoetal.ThisisanopenaccessarticledistributedundertheCreativeCommonsAttributionLicense,\nwhichpermitsunrestricteduse,distribution,andreproductioninanymedium,providedtheoriginalworkisproperlycited.\nEveryyear,alargeamountofpopulationreconcilesgun-relatedviolenceallovertheworld.Inthiswork,wedevelopacomputer-\nbased fully automated system to identify basic armaments, particularly handguns and rifles. Recent work in the field of deep\nlearningandtransferlearninghasdemonstratedsignificantprogressintheareasofobjectdetectionandrecognition.Wehave\nimplementedYOLOV3\u201cYouOnlyLookOnce\u201dobjectdetectionmodelbytrainingitonourcustomizeddataset.Thetraining\nresults confirm that YOLO V3 outperforms YOLO V2 and traditional convolutional neural network (CNN). Additionally,\nintensiveGPUsorhighcomputationresourceswerenotrequiredinourapproachasweusedtransferlearningfortrainingour\nmodel.Applyingthismodelinoursurveillancesystem,wecanattempttosavehumanlifeandaccomplishreductionintherateof\nmanslaughterormasskilling.Additionally,ourproposedsystemcanalsobeimplementedinhigh-endsurveillanceandsecurity\nrobots todetecta weaponorunsafe assets toavoidany kindofassaultorriskto human life.\n1.Introduction According to statistics, 4.2 in 100000 people are killed in\nPakistaneveryyearinmassshootings.Fromstreetcrimesto\nViolence committed with guns puts significant impact on an individual institution attack, many precious lives suf-\npublic, health, psychological, and economic cost. Many fered.Thisfurtherindicatesthatmanualsurveillancesystem\npeople die each year from gun-related violence. Psycho- stillneedshumaneyetodetecttheabnormalactivitiesandit\nlogicaltraumaisfrequentamongchildrenwhoareexposed takes a sufficient amount of time reporting to security of-\nto high levels of violence in their communities or through ficials to tackle the situation.\nthe media. Children exposed to gun-related violence, Although the human visual framework is quick and\nwhether they are victims, perpetrators, or witnesses, can preciseandcanlikewiseperformcomplexundertakingslike\nexperiencenegativepsychologicaleffectsovertheshortand distinguishing different items and recognizing snags with\nlongterms.Numberofstudiesshowthathandheldgunisthe minimalcognizantidea,however,itiscommontruththatif\nprimary weapon used for various crimes like break-in, anindividualwatchessomethingverysimilarforquitealong\nrobbery,shoplifting,andrape.Thesecrimescanbereduced time, there is an opportunity of sluggishness and lack of\nby identifying the disruptive behavior at early stage and regard.\nmonitoring the suspicious activities carefully so that law Nowadays, with the accessibility of huge datasets,\nenforcementagenciescanfurthertakeimmediateaction[1]. quicker GPUs, advanced machine learning algorithms, and\nLevels of gun-related violence vary greatly among geo- bettercalculations,wecannoweffectivelypreparePCsand\ngraphicallocationsandcountries.Theglobaldeathtollfrom develop automated computer-based system to distinguish\nuse of guns may be as high as 1,000 dead each day [2]. and identify numerous items on a site with high accuracy.\n2 MathematicalProblemsinEngineering\nRecent developments indicate that machine learning [3\u20136] makinglikeabnormaleventdetectionoranyanomaly.The\nand advance image processing algorithms have played latestanomalydetectiontechniquescanbedividedintotwo\ndominant role in smart surveillances and security systems groups,whichareobject-centeredtechniquesandintegrated\n[7, 8]. Apart from this, popularity of smart devices and methods.Theconvolutionalneuralnetwork(CNN)spatial-\nnetworked cameras has also empowered this domain. temporalsystemisonlyappliedtospatial-temporalvolumes\nHowever,humanobjectsorweapondetectionandtracking of interest (SVOI), reducing the cost of processing. In\nare still conducted at cloud centers, as real-time, online surveillance videos of complex scenes, researchers in [14]\ntracking is computationally costly. Significant efforts have proposed a tool for detecting and finding anomalous ac-\nbeen made in recent years to monitor robot manipulators tivities. By conducting spatial-temporal convolution layer,\nthatneedhighcontrolperformanceinreliabilityandspeed thisarchitecturehelpsonetocaptureobjectsfrombothtime\n[9, 10]. The researchers have attempted to improve the domainandfrequencydomain,therebyextractingboththe\nresponse characteristics of the robotic system and to at- presenceandmotiondataencodedincontinuousframes.To\ntenuate the uncertainties in [11]. The proposed developed dotraditionalfunctionstolocalnoiseandimprovedetection\nrobustmodel-freecontrollerincorporatestimedelaycontrol precision, spatial-temporal convolution layers are only\n(TDC) and adaptive terminal sliding mode control implementedwithinspatial-temporalquantitiesofchanging\n(ATSMC) methods. pixels. Researchers proposed anomaly-introduced learning\nIn this research work, we aim to develop a smart sur- method for detecting anomalous activities by developing\nveillance security system detecting weapons specifically multi-instance learning graph-based model with abnormal\nguns.Forthispurpose,wehaveappliedfewcomputevision andnormalbimodaldata,highlightingthepositiveinstances\nmethods and deep learning for identification of a weapon by training coarse filter using kernel-SVM classifier and\nfrom captured image. Recent work in the field of machine generating improved dictionary learning known as anchor\nlearninganddeeplearningparticularlyconvolutionalneural dictionary learning. Thus, abnormality is measure by\nnetworks has shown considerable progress in the areas of selecting the sparse reconstruction cost which yields the\nobject detection and recognition, exclusively in images. As comparison with other techniques including utilizing ab-\nthe first step for any video surveillance application, object normal information and reducing time and cost for SRC.\ndetection and classification are essential for further object Hu et al. [15] have contributed in detecting various\ntracking tasks. For this purpose, we trained the classifier objects in traffic scenes by presenting a method which de-\nmodelofYOLOv3,i.e.,\u201cYouOnlyLookOnce\u201d[12,13].This tectstheobjectsinthreesteps.Initially,itdetectstheobjects,\nmodel is a state-of-the-art real-time object detection clas- recognizes the objects, and finally tracks the objects in\nsifier.Furthermore,wearenotjustdetectingtheguns,rifles, motionbymainlytargetingthreeclassesofdifferentobjects\nand fire but also getting the location of the incident and including cars, cyclists, and traffic signs. Therefore, all the\nstoring the data for future use. We have connected three objects are detected using single learning-based detection\nsystemsusingsocketprogramingasademonstrationforthe framework consisting of dense feature extractor and tri-\nreal-life scenario as camera, CCTV operator, and security modal class detection. Additionally, dense features are\npanels. extractedandsharedwiththerestofdetectorswhichheads\nThisworkisanattempttodesignanddevelopasystem to be faster in speed that further needs to be evaluated in\nwhichcandetecttheguns,rifles,andfireinnotimewithless testing phase. Therefore, intraclass variation of objects is\ncomputational resources. It is evident from technological proposed for object subcategorization with competitive\nadvancementsthatmostofthehumanassistedapplications performance on several datasets.\nare now automated and computer-based. Eventually, in Gregaetal.presentedanalgorithmwhichautomatically\nfuture these computer-based systems will be replaced by detects knives and firearms in CCTV image and alerts the\nmoresmartmachines,robots,orhumanoidrobots.Inorder security guard or operator [16]. Majorly, focusing on lim-\nto provide visionary sense to robots, object detection plays iting false alarms and providing a real-time application\nfundamental part for understanding the objects and its wherespecificityofthealgorithmis94.93%andsensitivityis\ninterpretation. Thus, our proposed system can also be 81.18% for knife detection. Moreover, specificity for fire\nimplemented in surveillance and security robots to detect alarmsystemis96.69%andsensitivityis35.98%fordifferent\nany weapon or unsafe assets. objectsinthevideo.Mousavietal.in[17]carriedoutvideo\nclassifier also referred to as the Histogram of Directed\n2.Literature Review Tracklets which identifies irregular conditions in complex\nscenes. In comparison to traditional approaches using op-\nReducing the life-threatening acts and providing high se- tical flow which only measure edge features from two\ncurityarechallengingateveryplace.Therefore,anumberof subsequent frames, descriptors have been developing over\nresearchers have contributed to monitoring various activi- long-range motion projections called tracklets. Spatiotem-\nties and behaviors using object detection. In general, a poralcuboidfootagesequencesarestatisticallygatheredon\nframework of smart surveillance system is developed on the tracklets that move through them.\nthree levels: firstly, to extract low-level information like Ji et al. developed a system for security footage which\nfeatures engineering and object tracking; secondly, to automatically identifies the human behavior using con-\nidentifyunusualhumanactivities,behavior,ordetectionof volutional neural nets (CNNs) by forming deep learning\nany weapon; and finally, the high level is about decision model which operates directly on the raw inputs [18].\nMathematicalProblemsinEngineering 3\nTherefore, 3D CNN model for classification requires the are transformed into the same width and height 416\u00d7416\nregularization of outputs with high-level characteristics to pixels.\nincrease efficiency and integrating the observations of a Objectdetectionisprimarilyrelatedtocomputervision\nvariety of various models. thatincludesdistinguishingobjectsincomputerizedimages.\nPangetal.presentedreal-timeconcealedvariousobject Object detection is a domain that has benefited immensely\ndetection under human dress in [19]. Metallic guns on fromtherecentadvancementsintherealmofdeeplearning.\nhuman skeleton were used for passive millimeter wave YOLOisbasicallyapretrainedobjectdetector.ItisaCNN\nimagerywhichreliesonYOLOalgorithmondatasetofsmall model.ACNNisadeeplearningalgorithmwhichcantake\nscale. Subsequently, comparison is undertaken between inarawinputimageandassignlearnableweightsandbiases\nSingle MultiBox Detector algorithm, YOLOv3-13, SSD- to various aspects/objects in the image. A convolutional\nVGG16,andYOLOv3-53onPMMWdataset.Moreover,the layer in CNN model is responsible of extracting the high-\nweapondetectionaccuracycomputed36framespersecond level features such as edges, from the input image. This\nofdetectionspeedand95%meanaverageprecision.WarsiA worksbyapplyingkxkfilterknownaskernelrepeatedlyover\net al. have contributed to automatically detecting the rawimage.Thisfurtherresultsinactivationmapsorfeature\nhandgun invisual surveillance by implementing YOLO V3 maps. These feature maps are the presence of detected\nalgorithm with Faster Region-Based CNN (RCNN) by features from the given input. Thus, the preprocessing re-\ndifferentiating the number of false negatives and false quired is much lower as compared to other classification\npositives [20], thus, taking real-time images and incorpo- algorithms,whereasinstandardapproach,filtersarehand-\nrating with ImageNet dataset then training it using YOLO engineeredandinCNNthesearelearnedthroughanumber\nV3algorithm.TheyhavecomparedFasterRCNNtoYOLO of iterations and training. Figure 3 indicates a basic CNN\nV3 using four different videos and as a result YOLO V3 architectureasclassificationmodelfor10differentweapons.\nimparted faster speed in real-time environment. Subsequently,thenextlayerisMax-PoolingorSubsampling\nlayer,whichisresponsibleforreducingthespatialsizeofthe\nconvolved features. This is to decrease the computational\n3.Methodology\npower required toprocess the data throughdimensionality\nreduction. ReLU is a rectified linear unit activation\nIn this work, we have attempted to develop an integrated\nexpressed in (1), which is related to the feature of non-\nframeworkforreconnaissancesecuritythatdistinguishesthe\nsaturating activation. It eliminates undesirable values from\nweapons progressively, if identification is positively true it\nanactivationmapeffectivelybysettingthemtonil.Finally,\nwill caution/brief the security personals to handle the cir-\nthe last layers are fully connected layers transforming the\ncumstancebyarrivingattheplaceoftheincidentthroughIP\ndataintoa1-dimensionalarray.Tocreateaparticularlong\ncameras.Weproposeamodelthatprovidesavisionarysense\nfeature vector, the flattened output is fed to a feedforward\ntoamachinetoidentifytheunsafeweaponandcanalsoalert\nneural network and backpropagation is applied to every\nthehumanadministratorwhenagunorfirearmisobvious\niterationoftraining.Theselayersareliabletolearnnonlinear\nin the edge. Moreover, we have programmed entryways\ncombinationsofthehigh-levelfeaturesasrepresentedbythe\nlocking framework when the shooter seems to carry ap-\noutput of the convolutional layer.\npalling weapon. On the off chance conceivable, through IP\nwebcams we can likewise share the live photo to approach ReLU: f(x)\ufffdmax(0,x). (1)\nsecuritypersonalstomakethemoveinmeantime.Also,we\nhave constructed the information system for recording all As mentioned earlier that YOLO is a pretrained object\ntheexercisestoconveyimpactactivitiesinthemetropolitan detector, a pretrained model simply means that another\nterritories for a future crisis. This further ends up in de- dataset has been trained on it. It is extremely time con-\nsigningthedatabaseforrecordingalltheactivitiesinorder sumingtotrainamodelfromscratch;itcantakeweeksora\nto take prompt actions for future emergency. Figure 1 monthtocompletethetrainingstep.Apretrainedmodelhas\npresents the overall generalized approach of our research already seen tons of objects and knows how each of them\nwork divided into three parts. must be classified. The weights in the abovementioned\nThemostimportantandcrucialpartofanyapplicationis pretrained model have been obtained by training the net-\nto have a desired and suitable dataset in order to train the work on COCO and Imagenet dataset. Thus, it can only\nmachinelearningmodels.Therefore,wemanuallycollected detectobjectsbelongingtotheclassespresentinthedataset\nhuge amount of images from Google. A few of the image used to train the network. It uses Darknet-53 as the back-\nsamples are shown in Figure 2. For each weapon class, we bone network for feature extraction and uses three scale\ncollectedatleast50images.Usinggoogle-images-download predictions. The DarkNet-53 is again convolutional neural\nis one of the best ways to collect images for constructing network that has 53 layers as elucidated in Figure 4. Dar-\none\u2019sowndataset.Wefurthersavedthoseimagestoafolder kNet-53 is a fully convolutional neural network. Pooling\ncalled\u201cimages.\u201dOnemustsaveimagesin\u201c.jpg\u201dform;ifthe layerisreplacedwithaconvolutionoperationwithstride2.\nimages are in different extensions, it will be a little trou- Furthermore,residualunitsareappliedtoavoidthegradient\nblesomeandwillgenerateerrorswhenprovidedfortraining. dispersion.\nAlternatively, since the images are processed in terms of Initially,CNNarchitectureswerequitelinear.Recently,\nbatches,thereforepriortotraining,thesizesofalltheimages numerous variations are introduced, for example, middle\n4 MathematicalProblemsinEngineering\nInput video Detected guns\nDetected frame\nFrame conversion\nTraining data Get location\nImage preprocessing\nClassification\nAlert\nObject detection\nDatabase\nObject identification\nObject detection Analysis Action\nFigure1:Theflowof researchmethodology.\n(a) (b) (c) (d)\n(e) (f) (g) (h)\nFigure2:Sample imagesfrom collecteddataset.\nblocks, skip connections, and aggregations of data between stacked over it, accumulating to a total of a 106-layer fully\nlayers. These network models have already acquired rich convolutional architecture. Due to its multiscale feature\nfeaturerepresentationsbygettingtrainedoverawiderange fusion layers, YOLO V3 uses 3 feature maps of different\nofimages.Thus,selectingapretrainednetworkandusingit scales for target detection as shown in Figure 5.\nas a starting point to learn a new task is a concept behind\ntransferlearning.Inordertorecognizetheweapons,wetook\n4.Experimental Results\nthe weights of a pretrained model and trained another\nYOLO V3 model. Image classification includes, for example, the class of one\nYOLOV3isdesignedtobeamultiscaleddetectorrather object in a picture. However, object localization is to rec-\nthan image classifier. Therefore, for object detection, clas- ognizetheareaofatleastonearticleinapictureanddrawing\nsificationheadisreplacedbyappendingadetectionheadto aproliferatingboxaroundtheirdegreeasshowninFigure6.\nthis architecture. Henceforth, the output is vector with the Moreover,Figure7illustratesthedetectionofriflefroman\nboundingboxcoordinatesandprobabilityclasses.YOLOV3 animated video. The shape of the detection kernel is com-\ninherits Darknet-53 as its backbone, a framework to train putedby1\u00d71\u00d7(bbx(4+1+nc)).Hence,bbisthenumber\nneural networks with 53 layers as indicated in Figure 4. ofboundingboxes,\u201c4\u201disforthe4boundingboxcoordinate\nMoreover,forobjectdetectiontaskadditional53layersare positionsand1isobjectconfidence,andncisthenumberof\nMathematicalProblemsinEngineering 5\nfc_3\nfc_4\nFully connected Fully connected\nConv_1 Conv_2 neural network neural network\nconvolution convolution ReLU activation\n(5 \u00d7 5) kernel Max-pooling (5 \u00d7 5) kernel Max-pooling\nvalid padding (2 \u00d7 2) valid padding (2 \u00d7 2) (With\ndropout)\n0\n1\nFlatte\nne 2\nd\n9\nInput n1 channels n1 channels n2 channels n2 channels\n(28 \u00d7 28 \u00d7 1) (24 \u00d7 24 \u00d7 n1) (12 \u00d7 12 \u00d7 n1) (8 \u00d7 8 \u00d7 n2) (4 \u00d7 2 \u00d7 n2) Output\nn3 units\nFigure3: Feedforwardconvolutionalneural network(CNN).\nclasses. The downsampling of the input image is for three Type Filters Size Output\nscale predictions and is computed by strides 32, 16, and 8.\nConvolutional 32 3 \u00d7 3 256 \u00d7 256\nThe loss function over here is comprisedon three sections,\nConvolutional 64 3 \u00d7 3/2 128 \u00d7 128\nlocation error (L ), confidence error (L ), and classifi-\nbox cls Convolutional 32 1 \u00d7 1\ncation error (L ), as presented in (2).\nobj 1\u00d7 Convolutional 64 3 \u00d7 3\nLoss\ufffdL +L +L . (2) Residual 128 \u00d7 128\nbox cls obj\nConvolutional 128 3 \u00d7 3/2 64 \u00d7 64\nLiterature suggests that YOLO v2 often struggled with Convolutional 64 1 \u00d7 1\nsmall object detections. This happened due to loss of fine- 2\u00d7 Convolutional 128 3 \u00d7 3\ngrained features as the layers downsampled the input. In Residual 64 \u00d7 64\nconclusion, YOLO v2 applies an identity mapping, con-\nConvolutional 256 3 \u00d7 3/2 32 \u00d7 32\ncatenating feature maps from a previous layer to capture\nConvolutional 128 1 \u00d7 1\nlow-level features. However, YOLO v2\u2019s architecture was 8\u00d7 Convolutional 256 3 \u00d7 3\nlacking some of the influential essentials that are encapsu-\nResidual 32 \u00d7 32\nlatedinmostofstate-of-the-artalgorithms.Theearlymodels\nConvolutional 512 3 \u00d7 3/2 16 \u00d7 16\nwere lacking in the residual blocks, skip connections, and\nConvolutional 256 1 \u00d7 1\nupsampling.Ontheotherhand,YOLOv3incorporatesallof\n8\u00d7 Convolutional 512 3 \u00d7 3\nthese. The detection of smaller objects can be seen from\nResidual 16 \u00d7 16\ncumulative results demonstrated in Figure 8. We retrained\nConvolutional 1024 3 \u00d7 3/2 8 \u00d7 8\nboth YOLO V2 and YOLO V3. Alternatively, we also\nConvolutional 512 1 \u00d7 1\nconducted comparative analysis of the models with tradi-\n4\u00d7 Convolutional 1024 3 \u00d7 3\ntional CNN which was trained from the very scratch with\nResidual 8 \u00d7 8\nnullweights.TheobtainedresultsaresummarizedinTable1.\nAvgpool Global\nThe subsequent part of our research is based on the\nConnected 1000\nrecordingoflocationwheretheweaponwasdetectedsothat\nSoftmax\nthealarmisgenerated.Forthispurpose,atbackendwehave\nalso created a Database. A desktop application is also de- Figure4: Architecturaldetails ofDARKNET-53layers[10].\nveloped in order to provide connectivity with the database\nsystem.Therearefourattributesthatarecollectedfromthe map their positions. As it can be seen from the relational\nsitewhereanobjectlikeweaponwasdetected.Thecollected table provided in Figure 9, the attributes are latitude, lon-\ninformation needs to be translated into a geographical gitude, time, and location where weapons were seen or\nformat of longitude and latitude. For this purpose, geo- identified. At backend DAO (Data Access Object) layer is\ncoding was performed. It is the method of translating ad- alsoavailabletoshowtheuserthedatafromthedatabase.It\ndresses to geographical details, longitude, and latitude, to is component of Java Foundation Classes (JFC), which is a\n6 MathematicalProblemsinEngineering\nType Filters Size Output\nConvolutional 32 3 \u00d7 3 256 \u00d7 256\nConvolutional 64 3 \u00d7 3/2 128 \u00d7 128\nConvolutional 32 1 \u00d7 1\nConvolutional 64 3 \u00d7 3\n1\u00d7 Residual 128 \u00d7 128\nConvolutional 128 3 \u00d7 3/2 64 \u00d7 64\nConvolutional 64 1 \u00d7 1\nConvolutional 128 3 \u00d7 3\nResidual 64 \u00d7 64\n2\u00d7\nConvolutional 256 3 \u00d7 3/2 32 \u00d7 32\nConvolutional 128 1 \u00d7 1 Scale 3\nConvolutional 256 3 \u00d7 3\nResidual 32 \u00d7 32\n3\u00d7\nConvolutional 512 3 \u00d7 3/2 16 \u00d7 16\nConvolutional 256 1 \u00d7 1 Scale 2\nConvolutional 512 3 \u00d7 3\nResidual 16 \u00d7 16\n4\u00d7\nConvolutional 1024 3 \u00d7 3/2 8 \u00d7 8\nConvolutional 512 1 \u00d7 1 Scale 1\nConvolutional 1024 3 \u00d7 3\nResidual 8 \u00d7 8\nAvgpool Global Convs Convs Convs\nConnected 1000\nSoftmax\nYOLO detection layer\nFigure5: ArchitecturaldescriptionofYOLO V3.\nFigure6: Boundingboxaround detectedobject; weapon categoryGUN.\nGUI-providing API for Java programs. Swing provides Our proposed system is further compared with the\npackages that let us render our Java programs a complex existing literature in Table 2. In [21], the proposed system\ncollection of GUI components and it really is platform includes CNN-based VGG-16 architecture as feature ex-\nindependent. Figure 10 presents the class diagram and tractor, followed by state-of-the-art classifiers which are\nimplementation of DAO layer. implemented on a standard gun database. The researchers\nMathematicalProblemsinEngineering 7\nFigure7:Real-time weapondetected froma videosurrounded bybounding box.Weaponcategory rifle.\nFigure8:Cumulativeresultofdetectingweapon withprecisionvalue.\nTable 1:Experimental resultsfortrained deep learningmodels.\nS. no Models Accuracy\n1 TraditionalCNN 95\n2 YOLOV2 96.76\n3 YOLOV3 98.89\nFigure9: Imagepresentingthe recordeddatabase.\n8 MathematicalProblemsinEngineering\n<<Interface>>\nDAO\n+getLocation()\nConnectivity\n\u2013Connection con Database\n+connect()\nDAO [mp]\n+getLocation()\nImplemented by Implemented by\nGunDetection\nOfficialPanel +setRecord()\n+getLoaction()\nLocation\n\u2013float lat\n\u2013float lon\nUsed by \u2013strib = ng loc\nUsed by\n+getFloat lat()\n+setFloat lat (float lat) : void\n+getFloat lon()\n+setFloat lon (float lon) : void\n+getString loc()\n+setString loc (String loc) : void\nFigure10:Class diagramforDOA layer.\nTable 2:Comparison withthe existingwork.\nS. no Models Dataset Accuracy(%)\n1 Our trained modelYOLOV3 Imagedatasetcollectedforcurrentresearch 98.89\n2 Alexnet+SVM [22] Gunvideo database[24] 95\n4 FasterRCNN[23] Streaming video 95.4\n5 CNNVGG-16[21] IMDB 93.1\ninvestigated four machine learning models, namely, BoW, surveillance systems with the growing availability of cheap\nHOG+SVM, CNN, and Alexnet+SVM, to recognize the computing, video infrastructure, high-end technology, and\nfirearmsandknifesfromadatasetofimages[22].Theirwork better video processing.\nsuggeststhatpretrainedAlexnet+SVMperformedthebest.\nAs it is evident from the previous studies, researchers have Data Availability\nwidely applied CNN and its variant for weapon or knife\nidentification from CCTV videos [23]. It is obvious from The data are available on request.\nTable2thattheimplementedYOLOv3outperformstherest\nof the other models. Conflicts of Interest\nThe authors declare that they have no conflicts of interest.\n5.Conclusion and Future Work\nReferences\nInthisstudy,thestate-of-the-artYOLOV3objectdetection\nmodel was implemented and trained over our collected\n[1] S.A.Velastin,B.A.Boghossian,andM.A.Vicencio-Silva,\u201cA\ndataset for weapon detection. We propose a model that\nmotion-based image processing system for detecting poten-\nprovidesavisionarysensetoamachineorrobottoidentify tiallydangeroussituationsinundergroundrailwaystations,\u201d\nthe unsafe weapon and can also alert the human adminis- Transportation Research Part C: Emerging Technologies,\ntrator when a gun or a firearm is obvious in the edge. The vol.14,no.2, pp.96\u2013113, 2006.\nexperimental results show that the trained YOLO V3 has [2] UnitedNations,OfficeonDrugsandCrime,Reporton\u201cGlobal\nbetterperformancecomparedtotheYOLOV2modelandis StudyofHomicide\u201d,https://www.unodc.org/documents/data-\nlessexpensivecomputationally.Thereisanimmediateneed and-analysis/gsh/Booklet1.pdf.\n[3] P. M. Kumar, U. Gandhi, R. Varatharajan, G. Manogaran,\ntoupdatethecurrentsurveillancecapabilitieswithimproved\nR.Jidhesh,and T.Vadivel, \u201cIntelligentfacerecognitionand\nresourcestosupportmonitoringtheeffectivenessofhuman\nnavigationsystemusingneurallearningforsmartsecurityin\noperators. Smart surveillance systems would fully replace\ninternet of things,\u201d Cluster Computing, vol. 22, no. S4,\ncurrent infrastructure with the growing availability of low-\npp. 7733\u20137744,2019.\ncost storage, video infrastructure, and better video pro-\n[4] V.Babanne,N.S.Mahajan,R.L.Sharma,andP.P.Gargate,\ncessing technologies. Eventually, the digital monitoring \u201cMachine learning based smart surveillance system,\u201d in\nsystems in terms of robots would fully replace current Proceedings of the 2019 Third International Conference on\nMathematicalProblemsinEngineering 9\nI-SMAC (IoT in Social, Mobile, Analytics and Cloud)(I- [19] L.Pang,H.Liu,Y.Chen,andJ.Miao,\u201cReal-timeconcealed\nSMAC),pp. 84\u201386,IEEE, Palladam,India,December2019. object detection from passive millimeter wave images based\n[5] A. Joshi, N. Jagdale, R. Gandhi, and S. Chaudhari, \u201cSmart on the YOLOv3 algorithm,\u201d Sensors, vol. 20, no. 6, p.1678,\nsurveillance system for detection of suspicious behaviour 2020.\nusing machinelearning,\u201din IntelligentComputing, Informa- [20] A.Warsi,M.Abdullah,M.N.Husen,M.Yahya,S.Khan,and\ntionandControlSystems.ICICCS2019.AdvancesinIntelligent N. Jawaid, \u201cGun detection system using YOLOv3,\u201d in Pro-\nSystems and Computing, A. Pandian, K. Ntalianis, and ceedingsofthe2019IEEEInternationalConferenceonSmart\nR. Palanisamy, Eds., vol. 1039, Berlin, Germany, Springer, Instrumentation, Measurement and Application (ICSIMA),\nCham,2020. pp.1\u20134,IEEE, KualaLumpur,Malaysia, August2019.\n[6] K.-E.KoandK.-B.Sim,\u201cDeepconvolutionalframeworkfor [21] G.K.VermaandA.Dhillon,\u201cAhandheldgundetectionusing\nabnormalbehaviordetectioninasmartsurveillancesystem,\u201d faster r-cnn deep learning,\u201d in Proceedings of the 7th Inter-\nEngineering Applications of Artificial Intelligence, vol. 67, national Conference on Computer and Communication\npp.226\u2013234, 2018. Technology, pp. 84\u201388, Kurukshetra, Haryana, November\n[7] S. Y. Nikouei, Y. Chen, S. Song, R. Xu, B.-Y. Choi, and 2017.\nT.Faughnan,\u201cSmartsurveillanceasanedgenetworkservice: [22] S.B.KibriaandM.S.Hasan,\u201cAnanalysisoffeatureextraction\nfrom harr-cascade, SVM to a lightweight CNN,\u201d in Pro- andclassificationalgorithmsfordangerousobjectdetection,\u201d\nceedings of the 2018 IEEE 4th International Conference on in Proceedings of the 2017 2nd International Conference on\nCollaboration and Internet Computing (CIC), pp. 256\u2013265, Electrical & Electronic Engineering (ICEEE), pp. 1\u20134, IEEE,\nPhiladelphia,PA,USA,April2018. Rajshahi, Bangladesh,December2017.\n[8] R.Xu,S.Y.Nikouei,Y.Chenetal.,\u201cReal-timehumanobjects [23] A. Castillo, S. Tabik, F. Pe\u00b4rez, R. Olmos, and F. Herrera,\ntrackingforsmartsurveillanceattheedge,\u201dinProceedingsof \u201cBrightness guided preprocessing for automatic cold steel\nthe 2018 IEEE International Conference on Communications weapondetectioninsurveillancevideoswithdeeplearning,\u201d\n(ICC), pp.1\u20136,KansasCity,MO,USA, May2018. Neurocomputing, vol. 330,pp.151\u2013161, 2019.\n[9] S. Ahmed, A. Ahmed, I. Mansoor, F. Junejo, and A. Saeed, [24] V.Gun, \u201cDatabase,\u201dhttp://kt.agh.edu.pl/grega/guns/.\n\u201cOutput feedback adaptive fractional-order super-twisting\nslidingmodecontrolofroboticmanipulator,\u201dIranianJournal\nof Science and Technology, Transactions of Electrical Engi-\nneering,vol. 45,no.1, pp.335\u2013347, 2021.\n[10] S.Ahmed,H.Wang,andY.Tian,\u201cAdaptivefractionalhigh-\norder terminal sliding mode control for nonlinear robotic\nmanipulator under alternating loads,\u201d Asian Journal of\nControl,2020.\n[11] S. Ahmed, H. Wang, and Y. Tian, \u201cAdaptive high-order\nterminalslidingmodecontrolbasedontimedelayestimation\nfortheroboticmanipulatorswithbacklashhysteresis,\u201dIEEE\nTransactions on Systems, Man, and Cybernetics: Systems,\nvol. 51, no.2, pp.1128\u20131137, 2021.\n[12] J.Redmon,S.Divvala,R.Girshick,andA.Farhadi,\u201cYouonly\nlookonce:unified,real-timeobjectdetection,\u201dinProceedings\nof the IEEE Conference on Computer Vision and Pattern\nRecognition,pp. 779\u2013788,Las Vegas,NV, USA,June 2016.\n[13] A. Farhadi and R. Joseph, \u201cYolov3: an incremental im-\nprovement,\u201dComputerVisionandPatternRecognition,2018.\n[14] C.He,J.Shao,andJ.Sun,\u201cAnanomaly-introducedlearning\nmethodforabnormaleventdetection,\u201dMultimediaToolsand\nApplications,vol. 77, no.22,pp. 29573\u201329588,2018.\n[15] Q.Hu,S.Paisitkriangkrai,C.Shen,A.vandenHengel,and\nF.Porikli,\u201cFastdetectionofmultipleobjectsintrafficscenes\nwithacommondetectionframework,\u201dIEEETransactionson\nIntelligent Transportation Systems, vol. 17, no. 4, pp. 1002\u2013\n1014, 2015.\n[16] M. Grega, A. Matiolan\u00b4ski, P. Guzik, and M. Leszczuk, \u201cAu-\ntomateddetectionoffirearmsandknivesinaCCTVimage,\u201d\nSensors,vol.16, no.1,p. 47, 2016.\n[17] H. Mousavi, S. Mohammadi, A. Perina, R. Chellali, and\nV.Murino,\u201cAnalyzingtrackletsforthedetectionofabnormal\ncrowd behavior,\u201d in Proceedings of the 2015 IEEE Winter\nConferenceonApplicationsofComputerVision,pp.148\u2013155,\nIEEE,Waikoloa,HI,USA, January2015.\n[18] S.Ji,W.Xu,M.Yang,andK.Yu,\u201c3Dconvolutionalneural\nnetworks for human action recognition,\u201d IEEE Transactions\non Pattern Analysis and Machine Intelligence, vol. 35, no.1,\npp.221\u2013231, 2012.",
    "metadata": {
      "CreationDate": "D:20210511143909+05'30'",
      "Creator": "Aspose Ltd.",
      "ModDate": "D:20210512074854+00'00'",
      "Producer": "PDFsharp 1.32.2608-g (www.pdfsharp.net) (Original: Aspose.Pdf for .NET 10.1.0; modified using iTextSharp\u2122 5.4.0 \u00a92000-2012 1T3XT BVBA (AGPL-version))"
    },
    "error_message": null,
    "pages": [
      {
        "page_number": 0,
        "text_content": "Hindawi\nMathematical Problems in Engineering\nVolume 2021, Article ID 9975700, 9 pages\nhttps://doi.org/10.1155/2021/9975700\nResearch Article\nWeapon Detection Using YOLO V3 for Smart Surveillance System\nSanam Narejo ,1 Bishwajeet Pandey ,2 Doris Esenarro vargas ,3 Ciro Rodriguez ,4\nand M. Rizwan Anjum 5\n1DepartmentofComputerSystemsEngineering,MehranUniversityofEngineeringandTechnology(MUET),Jamshoro,Pakistan\n2GranSassoScience Institute,L\u2019Aquila,Italy\n3UniversidadNacional FedericoVillarreal,Lima,Peru\n4UniversidadNacional MayordeSanMarcos,Lima,Peru\n5Departmentof ElectronicEngineering,TheIslamiaUniversityofBahawalpur,Bahawalpur 63100,Pakistan\nCorrespondenceshouldbeaddressedtoBishwajeetPandey;dr.pandey@ieee.org\nReceived 4 March 2021; Revised 15 April 2021; Accepted 3 May 2021; Published 12 May 2021\nAcademicEditor:ZainAnwarAli\nCopyright\u00a92021SanamNarejoetal.ThisisanopenaccessarticledistributedundertheCreativeCommonsAttributionLicense,\nwhichpermitsunrestricteduse,distribution,andreproductioninanymedium,providedtheoriginalworkisproperlycited.\nEveryyear,alargeamountofpopulationreconcilesgun-relatedviolenceallovertheworld.Inthiswork,wedevelopacomputer-\nbased fully automated system to identify basic armaments, particularly handguns and rifles. Recent work in the field of deep\nlearningandtransferlearninghasdemonstratedsignificantprogressintheareasofobjectdetectionandrecognition.Wehave\nimplementedYOLOV3\u201cYouOnlyLookOnce\u201dobjectdetectionmodelbytrainingitonourcustomizeddataset.Thetraining\nresults confirm that YOLO V3 outperforms YOLO V2 and traditional convolutional neural network (CNN). Additionally,\nintensiveGPUsorhighcomputationresourceswerenotrequiredinourapproachasweusedtransferlearningfortrainingour\nmodel.Applyingthismodelinoursurveillancesystem,wecanattempttosavehumanlifeandaccomplishreductionintherateof\nmanslaughterormasskilling.Additionally,ourproposedsystemcanalsobeimplementedinhigh-endsurveillanceandsecurity\nrobots todetecta weaponorunsafe assets toavoidany kindofassaultorriskto human life.\n1.Introduction According to statistics, 4.2 in 100000 people are killed in\nPakistaneveryyearinmassshootings.Fromstreetcrimesto\nViolence committed with guns puts significant impact on an individual institution attack, many precious lives suf-\npublic, health, psychological, and economic cost. Many fered.Thisfurtherindicatesthatmanualsurveillancesystem\npeople die each year from gun-related violence. Psycho- stillneedshumaneyetodetecttheabnormalactivitiesandit\nlogicaltraumaisfrequentamongchildrenwhoareexposed takes a sufficient amount of time reporting to security of-\nto high levels of violence in their communities or through ficials to tackle the situation.\nthe media. Children exposed to gun-related violence, Although the human visual framework is quick and\nwhether they are victims, perpetrators, or witnesses, can preciseandcanlikewiseperformcomplexundertakingslike\nexperiencenegativepsychologicaleffectsovertheshortand distinguishing different items and recognizing snags with\nlongterms.Numberofstudiesshowthathandheldgunisthe minimalcognizantidea,however,itiscommontruththatif\nprimary weapon used for various crimes like break-in, anindividualwatchessomethingverysimilarforquitealong\nrobbery,shoplifting,andrape.Thesecrimescanbereduced time, there is an opportunity of sluggishness and lack of\nby identifying the disruptive behavior at early stage and regard.\nmonitoring the suspicious activities carefully so that law Nowadays, with the accessibility of huge datasets,\nenforcementagenciescanfurthertakeimmediateaction[1]. quicker GPUs, advanced machine learning algorithms, and\nLevels of gun-related violence vary greatly among geo- bettercalculations,wecannoweffectivelypreparePCsand\ngraphicallocationsandcountries.Theglobaldeathtollfrom develop automated computer-based system to distinguish\nuse of guns may be as high as 1,000 dead each day [2]. and identify numerous items on a site with high accuracy.",
        "tables": [],
        "forms": {},
        "confidence_score": 1.0
      },
      {
        "page_number": 1,
        "text_content": "2 MathematicalProblemsinEngineering\nRecent developments indicate that machine learning [3\u20136] makinglikeabnormaleventdetectionoranyanomaly.The\nand advance image processing algorithms have played latestanomalydetectiontechniquescanbedividedintotwo\ndominant role in smart surveillances and security systems groups,whichareobject-centeredtechniquesandintegrated\n[7, 8]. Apart from this, popularity of smart devices and methods.Theconvolutionalneuralnetwork(CNN)spatial-\nnetworked cameras has also empowered this domain. temporalsystemisonlyappliedtospatial-temporalvolumes\nHowever,humanobjectsorweapondetectionandtracking of interest (SVOI), reducing the cost of processing. In\nare still conducted at cloud centers, as real-time, online surveillance videos of complex scenes, researchers in [14]\ntracking is computationally costly. Significant efforts have proposed a tool for detecting and finding anomalous ac-\nbeen made in recent years to monitor robot manipulators tivities. By conducting spatial-temporal convolution layer,\nthatneedhighcontrolperformanceinreliabilityandspeed thisarchitecturehelpsonetocaptureobjectsfrombothtime\n[9, 10]. The researchers have attempted to improve the domainandfrequencydomain,therebyextractingboththe\nresponse characteristics of the robotic system and to at- presenceandmotiondataencodedincontinuousframes.To\ntenuate the uncertainties in [11]. The proposed developed dotraditionalfunctionstolocalnoiseandimprovedetection\nrobustmodel-freecontrollerincorporatestimedelaycontrol precision, spatial-temporal convolution layers are only\n(TDC) and adaptive terminal sliding mode control implementedwithinspatial-temporalquantitiesofchanging\n(ATSMC) methods. pixels. Researchers proposed anomaly-introduced learning\nIn this research work, we aim to develop a smart sur- method for detecting anomalous activities by developing\nveillance security system detecting weapons specifically multi-instance learning graph-based model with abnormal\nguns.Forthispurpose,wehaveappliedfewcomputevision andnormalbimodaldata,highlightingthepositiveinstances\nmethods and deep learning for identification of a weapon by training coarse filter using kernel-SVM classifier and\nfrom captured image. Recent work in the field of machine generating improved dictionary learning known as anchor\nlearninganddeeplearningparticularlyconvolutionalneural dictionary learning. Thus, abnormality is measure by\nnetworks has shown considerable progress in the areas of selecting the sparse reconstruction cost which yields the\nobject detection and recognition, exclusively in images. As comparison with other techniques including utilizing ab-\nthe first step for any video surveillance application, object normal information and reducing time and cost for SRC.\ndetection and classification are essential for further object Hu et al. [15] have contributed in detecting various\ntracking tasks. For this purpose, we trained the classifier objects in traffic scenes by presenting a method which de-\nmodelofYOLOv3,i.e.,\u201cYouOnlyLookOnce\u201d[12,13].This tectstheobjectsinthreesteps.Initially,itdetectstheobjects,\nmodel is a state-of-the-art real-time object detection clas- recognizes the objects, and finally tracks the objects in\nsifier.Furthermore,wearenotjustdetectingtheguns,rifles, motionbymainlytargetingthreeclassesofdifferentobjects\nand fire but also getting the location of the incident and including cars, cyclists, and traffic signs. Therefore, all the\nstoring the data for future use. We have connected three objects are detected using single learning-based detection\nsystemsusingsocketprogramingasademonstrationforthe framework consisting of dense feature extractor and tri-\nreal-life scenario as camera, CCTV operator, and security modal class detection. Additionally, dense features are\npanels. extractedandsharedwiththerestofdetectorswhichheads\nThisworkisanattempttodesignanddevelopasystem to be faster in speed that further needs to be evaluated in\nwhichcandetecttheguns,rifles,andfireinnotimewithless testing phase. Therefore, intraclass variation of objects is\ncomputational resources. It is evident from technological proposed for object subcategorization with competitive\nadvancementsthatmostofthehumanassistedapplications performance on several datasets.\nare now automated and computer-based. Eventually, in Gregaetal.presentedanalgorithmwhichautomatically\nfuture these computer-based systems will be replaced by detects knives and firearms in CCTV image and alerts the\nmoresmartmachines,robots,orhumanoidrobots.Inorder security guard or operator [16]. Majorly, focusing on lim-\nto provide visionary sense to robots, object detection plays iting false alarms and providing a real-time application\nfundamental part for understanding the objects and its wherespecificityofthealgorithmis94.93%andsensitivityis\ninterpretation. Thus, our proposed system can also be 81.18% for knife detection. Moreover, specificity for fire\nimplemented in surveillance and security robots to detect alarmsystemis96.69%andsensitivityis35.98%fordifferent\nany weapon or unsafe assets. objectsinthevideo.Mousavietal.in[17]carriedoutvideo\nclassifier also referred to as the Histogram of Directed\n2.Literature Review Tracklets which identifies irregular conditions in complex\nscenes. In comparison to traditional approaches using op-\nReducing the life-threatening acts and providing high se- tical flow which only measure edge features from two\ncurityarechallengingateveryplace.Therefore,anumberof subsequent frames, descriptors have been developing over\nresearchers have contributed to monitoring various activi- long-range motion projections called tracklets. Spatiotem-\nties and behaviors using object detection. In general, a poralcuboidfootagesequencesarestatisticallygatheredon\nframework of smart surveillance system is developed on the tracklets that move through them.\nthree levels: firstly, to extract low-level information like Ji et al. developed a system for security footage which\nfeatures engineering and object tracking; secondly, to automatically identifies the human behavior using con-\nidentifyunusualhumanactivities,behavior,ordetectionof volutional neural nets (CNNs) by forming deep learning\nany weapon; and finally, the high level is about decision model which operates directly on the raw inputs [18].",
        "tables": [],
        "forms": {},
        "confidence_score": 1.0
      },
      {
        "page_number": 2,
        "text_content": "MathematicalProblemsinEngineering 3\nTherefore, 3D CNN model for classification requires the are transformed into the same width and height 416\u00d7416\nregularization of outputs with high-level characteristics to pixels.\nincrease efficiency and integrating the observations of a Objectdetectionisprimarilyrelatedtocomputervision\nvariety of various models. thatincludesdistinguishingobjectsincomputerizedimages.\nPangetal.presentedreal-timeconcealedvariousobject Object detection is a domain that has benefited immensely\ndetection under human dress in [19]. Metallic guns on fromtherecentadvancementsintherealmofdeeplearning.\nhuman skeleton were used for passive millimeter wave YOLOisbasicallyapretrainedobjectdetector.ItisaCNN\nimagerywhichreliesonYOLOalgorithmondatasetofsmall model.ACNNisadeeplearningalgorithmwhichcantake\nscale. Subsequently, comparison is undertaken between inarawinputimageandassignlearnableweightsandbiases\nSingle MultiBox Detector algorithm, YOLOv3-13, SSD- to various aspects/objects in the image. A convolutional\nVGG16,andYOLOv3-53onPMMWdataset.Moreover,the layer in CNN model is responsible of extracting the high-\nweapondetectionaccuracycomputed36framespersecond level features such as edges, from the input image. This\nofdetectionspeedand95%meanaverageprecision.WarsiA worksbyapplyingkxkfilterknownaskernelrepeatedlyover\net al. have contributed to automatically detecting the rawimage.Thisfurtherresultsinactivationmapsorfeature\nhandgun invisual surveillance by implementing YOLO V3 maps. These feature maps are the presence of detected\nalgorithm with Faster Region-Based CNN (RCNN) by features from the given input. Thus, the preprocessing re-\ndifferentiating the number of false negatives and false quired is much lower as compared to other classification\npositives [20], thus, taking real-time images and incorpo- algorithms,whereasinstandardapproach,filtersarehand-\nrating with ImageNet dataset then training it using YOLO engineeredandinCNNthesearelearnedthroughanumber\nV3algorithm.TheyhavecomparedFasterRCNNtoYOLO of iterations and training. Figure 3 indicates a basic CNN\nV3 using four different videos and as a result YOLO V3 architectureasclassificationmodelfor10differentweapons.\nimparted faster speed in real-time environment. Subsequently,thenextlayerisMax-PoolingorSubsampling\nlayer,whichisresponsibleforreducingthespatialsizeofthe\nconvolved features. This is to decrease the computational\n3.Methodology\npower required toprocess the data throughdimensionality\nreduction. ReLU is a rectified linear unit activation\nIn this work, we have attempted to develop an integrated\nexpressed in (1), which is related to the feature of non-\nframeworkforreconnaissancesecuritythatdistinguishesthe\nsaturating activation. It eliminates undesirable values from\nweapons progressively, if identification is positively true it\nanactivationmapeffectivelybysettingthemtonil.Finally,\nwill caution/brief the security personals to handle the cir-\nthe last layers are fully connected layers transforming the\ncumstancebyarrivingattheplaceoftheincidentthroughIP\ndataintoa1-dimensionalarray.Tocreateaparticularlong\ncameras.Weproposeamodelthatprovidesavisionarysense\nfeature vector, the flattened output is fed to a feedforward\ntoamachinetoidentifytheunsafeweaponandcanalsoalert\nneural network and backpropagation is applied to every\nthehumanadministratorwhenagunorfirearmisobvious\niterationoftraining.Theselayersareliabletolearnnonlinear\nin the edge. Moreover, we have programmed entryways\ncombinationsofthehigh-levelfeaturesasrepresentedbythe\nlocking framework when the shooter seems to carry ap-\noutput of the convolutional layer.\npalling weapon. On the off chance conceivable, through IP\nwebcams we can likewise share the live photo to approach ReLU: f(x)\ufffdmax(0,x). (1)\nsecuritypersonalstomakethemoveinmeantime.Also,we\nhave constructed the information system for recording all As mentioned earlier that YOLO is a pretrained object\ntheexercisestoconveyimpactactivitiesinthemetropolitan detector, a pretrained model simply means that another\nterritories for a future crisis. This further ends up in de- dataset has been trained on it. It is extremely time con-\nsigningthedatabaseforrecordingalltheactivitiesinorder sumingtotrainamodelfromscratch;itcantakeweeksora\nto take prompt actions for future emergency. Figure 1 monthtocompletethetrainingstep.Apretrainedmodelhas\npresents the overall generalized approach of our research already seen tons of objects and knows how each of them\nwork divided into three parts. must be classified. The weights in the abovementioned\nThemostimportantandcrucialpartofanyapplicationis pretrained model have been obtained by training the net-\nto have a desired and suitable dataset in order to train the work on COCO and Imagenet dataset. Thus, it can only\nmachinelearningmodels.Therefore,wemanuallycollected detectobjectsbelongingtotheclassespresentinthedataset\nhuge amount of images from Google. A few of the image used to train the network. It uses Darknet-53 as the back-\nsamples are shown in Figure 2. For each weapon class, we bone network for feature extraction and uses three scale\ncollectedatleast50images.Usinggoogle-images-download predictions. The DarkNet-53 is again convolutional neural\nis one of the best ways to collect images for constructing network that has 53 layers as elucidated in Figure 4. Dar-\none\u2019sowndataset.Wefurthersavedthoseimagestoafolder kNet-53 is a fully convolutional neural network. Pooling\ncalled\u201cimages.\u201dOnemustsaveimagesin\u201c.jpg\u201dform;ifthe layerisreplacedwithaconvolutionoperationwithstride2.\nimages are in different extensions, it will be a little trou- Furthermore,residualunitsareappliedtoavoidthegradient\nblesomeandwillgenerateerrorswhenprovidedfortraining. dispersion.\nAlternatively, since the images are processed in terms of Initially,CNNarchitectureswerequitelinear.Recently,\nbatches,thereforepriortotraining,thesizesofalltheimages numerous variations are introduced, for example, middle",
        "tables": [],
        "forms": {},
        "confidence_score": 1.0
      },
      {
        "page_number": 3,
        "text_content": "4 MathematicalProblemsinEngineering\nInput video Detected guns\nDetected frame\nFrame conversion\nTraining data Get location\nImage preprocessing\nClassification\nAlert\nObject detection\nDatabase\nObject identification\nObject detection Analysis Action\nFigure1:Theflowof researchmethodology.\n(a) (b) (c) (d)\n(e) (f) (g) (h)\nFigure2:Sample imagesfrom collecteddataset.\nblocks, skip connections, and aggregations of data between stacked over it, accumulating to a total of a 106-layer fully\nlayers. These network models have already acquired rich convolutional architecture. Due to its multiscale feature\nfeaturerepresentationsbygettingtrainedoverawiderange fusion layers, YOLO V3 uses 3 feature maps of different\nofimages.Thus,selectingapretrainednetworkandusingit scales for target detection as shown in Figure 5.\nas a starting point to learn a new task is a concept behind\ntransferlearning.Inordertorecognizetheweapons,wetook\n4.Experimental Results\nthe weights of a pretrained model and trained another\nYOLO V3 model. Image classification includes, for example, the class of one\nYOLOV3isdesignedtobeamultiscaleddetectorrather object in a picture. However, object localization is to rec-\nthan image classifier. Therefore, for object detection, clas- ognizetheareaofatleastonearticleinapictureanddrawing\nsificationheadisreplacedbyappendingadetectionheadto aproliferatingboxaroundtheirdegreeasshowninFigure6.\nthis architecture. Henceforth, the output is vector with the Moreover,Figure7illustratesthedetectionofriflefroman\nboundingboxcoordinatesandprobabilityclasses.YOLOV3 animated video. The shape of the detection kernel is com-\ninherits Darknet-53 as its backbone, a framework to train putedby1\u00d71\u00d7(bbx(4+1+nc)).Hence,bbisthenumber\nneural networks with 53 layers as indicated in Figure 4. ofboundingboxes,\u201c4\u201disforthe4boundingboxcoordinate\nMoreover,forobjectdetectiontaskadditional53layersare positionsand1isobjectconfidence,andncisthenumberof",
        "tables": [],
        "forms": {},
        "confidence_score": 1.0
      },
      {
        "page_number": 4,
        "text_content": "MathematicalProblemsinEngineering 5\nfc_3\nfc_4\nFully connected Fully connected\nConv_1 Conv_2 neural network neural network\nconvolution convolution ReLU activation\n(5 \u00d7 5) kernel Max-pooling (5 \u00d7 5) kernel Max-pooling\nvalid padding (2 \u00d7 2) valid padding (2 \u00d7 2) (With\ndropout)\n0\n1\nFlatte\nne 2\nd\n9\nInput n1 channels n1 channels n2 channels n2 channels\n(28 \u00d7 28 \u00d7 1) (24 \u00d7 24 \u00d7 n1) (12 \u00d7 12 \u00d7 n1) (8 \u00d7 8 \u00d7 n2) (4 \u00d7 2 \u00d7 n2) Output\nn3 units\nFigure3: Feedforwardconvolutionalneural network(CNN).\nclasses. The downsampling of the input image is for three Type Filters Size Output\nscale predictions and is computed by strides 32, 16, and 8.\nConvolutional 32 3 \u00d7 3 256 \u00d7 256\nThe loss function over here is comprisedon three sections,\nConvolutional 64 3 \u00d7 3/2 128 \u00d7 128\nlocation error (L ), confidence error (L ), and classifi-\nbox cls Convolutional 32 1 \u00d7 1\ncation error (L ), as presented in (2).\nobj 1\u00d7 Convolutional 64 3 \u00d7 3\nLoss\ufffdL +L +L . (2) Residual 128 \u00d7 128\nbox cls obj\nConvolutional 128 3 \u00d7 3/2 64 \u00d7 64\nLiterature suggests that YOLO v2 often struggled with Convolutional 64 1 \u00d7 1\nsmall object detections. This happened due to loss of fine- 2\u00d7 Convolutional 128 3 \u00d7 3\ngrained features as the layers downsampled the input. In Residual 64 \u00d7 64\nconclusion, YOLO v2 applies an identity mapping, con-\nConvolutional 256 3 \u00d7 3/2 32 \u00d7 32\ncatenating feature maps from a previous layer to capture\nConvolutional 128 1 \u00d7 1\nlow-level features. However, YOLO v2\u2019s architecture was 8\u00d7 Convolutional 256 3 \u00d7 3\nlacking some of the influential essentials that are encapsu-\nResidual 32 \u00d7 32\nlatedinmostofstate-of-the-artalgorithms.Theearlymodels\nConvolutional 512 3 \u00d7 3/2 16 \u00d7 16\nwere lacking in the residual blocks, skip connections, and\nConvolutional 256 1 \u00d7 1\nupsampling.Ontheotherhand,YOLOv3incorporatesallof\n8\u00d7 Convolutional 512 3 \u00d7 3\nthese. The detection of smaller objects can be seen from\nResidual 16 \u00d7 16\ncumulative results demonstrated in Figure 8. We retrained\nConvolutional 1024 3 \u00d7 3/2 8 \u00d7 8\nboth YOLO V2 and YOLO V3. Alternatively, we also\nConvolutional 512 1 \u00d7 1\nconducted comparative analysis of the models with tradi-\n4\u00d7 Convolutional 1024 3 \u00d7 3\ntional CNN which was trained from the very scratch with\nResidual 8 \u00d7 8\nnullweights.TheobtainedresultsaresummarizedinTable1.\nAvgpool Global\nThe subsequent part of our research is based on the\nConnected 1000\nrecordingoflocationwheretheweaponwasdetectedsothat\nSoftmax\nthealarmisgenerated.Forthispurpose,atbackendwehave\nalso created a Database. A desktop application is also de- Figure4: Architecturaldetails ofDARKNET-53layers[10].\nveloped in order to provide connectivity with the database\nsystem.Therearefourattributesthatarecollectedfromthe map their positions. As it can be seen from the relational\nsitewhereanobjectlikeweaponwasdetected.Thecollected table provided in Figure 9, the attributes are latitude, lon-\ninformation needs to be translated into a geographical gitude, time, and location where weapons were seen or\nformat of longitude and latitude. For this purpose, geo- identified. At backend DAO (Data Access Object) layer is\ncoding was performed. It is the method of translating ad- alsoavailabletoshowtheuserthedatafromthedatabase.It\ndresses to geographical details, longitude, and latitude, to is component of Java Foundation Classes (JFC), which is a",
        "tables": [],
        "forms": {},
        "confidence_score": 1.0
      },
      {
        "page_number": 5,
        "text_content": "6 MathematicalProblemsinEngineering\nType Filters Size Output\nConvolutional 32 3 \u00d7 3 256 \u00d7 256\nConvolutional 64 3 \u00d7 3/2 128 \u00d7 128\nConvolutional 32 1 \u00d7 1\nConvolutional 64 3 \u00d7 3\n1\u00d7 Residual 128 \u00d7 128\nConvolutional 128 3 \u00d7 3/2 64 \u00d7 64\nConvolutional 64 1 \u00d7 1\nConvolutional 128 3 \u00d7 3\nResidual 64 \u00d7 64\n2\u00d7\nConvolutional 256 3 \u00d7 3/2 32 \u00d7 32\nConvolutional 128 1 \u00d7 1 Scale 3\nConvolutional 256 3 \u00d7 3\nResidual 32 \u00d7 32\n3\u00d7\nConvolutional 512 3 \u00d7 3/2 16 \u00d7 16\nConvolutional 256 1 \u00d7 1 Scale 2\nConvolutional 512 3 \u00d7 3\nResidual 16 \u00d7 16\n4\u00d7\nConvolutional 1024 3 \u00d7 3/2 8 \u00d7 8\nConvolutional 512 1 \u00d7 1 Scale 1\nConvolutional 1024 3 \u00d7 3\nResidual 8 \u00d7 8\nAvgpool Global Convs Convs Convs\nConnected 1000\nSoftmax\nYOLO detection layer\nFigure5: ArchitecturaldescriptionofYOLO V3.\nFigure6: Boundingboxaround detectedobject; weapon categoryGUN.\nGUI-providing API for Java programs. Swing provides Our proposed system is further compared with the\npackages that let us render our Java programs a complex existing literature in Table 2. In [21], the proposed system\ncollection of GUI components and it really is platform includes CNN-based VGG-16 architecture as feature ex-\nindependent. Figure 10 presents the class diagram and tractor, followed by state-of-the-art classifiers which are\nimplementation of DAO layer. implemented on a standard gun database. The researchers",
        "tables": [],
        "forms": {},
        "confidence_score": 1.0
      },
      {
        "page_number": 6,
        "text_content": "MathematicalProblemsinEngineering 7\nFigure7:Real-time weapondetected froma videosurrounded bybounding box.Weaponcategory rifle.\nFigure8:Cumulativeresultofdetectingweapon withprecisionvalue.\nTable 1:Experimental resultsfortrained deep learningmodels.\nS. no Models Accuracy\n1 TraditionalCNN 95\n2 YOLOV2 96.76\n3 YOLOV3 98.89\nFigure9: Imagepresentingthe recordeddatabase.",
        "tables": [],
        "forms": {},
        "confidence_score": 1.0
      },
      {
        "page_number": 7,
        "text_content": "8 MathematicalProblemsinEngineering\n<<Interface>>\nDAO\n+getLocation()\nConnectivity\n\u2013Connection con Database\n+connect()\nDAO [mp]\n+getLocation()\nImplemented by Implemented by\nGunDetection\nOfficialPanel +setRecord()\n+getLoaction()\nLocation\n\u2013float lat\n\u2013float lon\nUsed by \u2013strib = ng loc\nUsed by\n+getFloat lat()\n+setFloat lat (float lat) : void\n+getFloat lon()\n+setFloat lon (float lon) : void\n+getString loc()\n+setString loc (String loc) : void\nFigure10:Class diagramforDOA layer.\nTable 2:Comparison withthe existingwork.\nS. no Models Dataset Accuracy(%)\n1 Our trained modelYOLOV3 Imagedatasetcollectedforcurrentresearch 98.89\n2 Alexnet+SVM [22] Gunvideo database[24] 95\n4 FasterRCNN[23] Streaming video 95.4\n5 CNNVGG-16[21] IMDB 93.1\ninvestigated four machine learning models, namely, BoW, surveillance systems with the growing availability of cheap\nHOG+SVM, CNN, and Alexnet+SVM, to recognize the computing, video infrastructure, high-end technology, and\nfirearmsandknifesfromadatasetofimages[22].Theirwork better video processing.\nsuggeststhatpretrainedAlexnet+SVMperformedthebest.\nAs it is evident from the previous studies, researchers have Data Availability\nwidely applied CNN and its variant for weapon or knife\nidentification from CCTV videos [23]. It is obvious from The data are available on request.\nTable2thattheimplementedYOLOv3outperformstherest\nof the other models. Conflicts of Interest\nThe authors declare that they have no conflicts of interest.\n5.Conclusion and Future Work\nReferences\nInthisstudy,thestate-of-the-artYOLOV3objectdetection\nmodel was implemented and trained over our collected\n[1] S.A.Velastin,B.A.Boghossian,andM.A.Vicencio-Silva,\u201cA\ndataset for weapon detection. We propose a model that\nmotion-based image processing system for detecting poten-\nprovidesavisionarysensetoamachineorrobottoidentify tiallydangeroussituationsinundergroundrailwaystations,\u201d\nthe unsafe weapon and can also alert the human adminis- Transportation Research Part C: Emerging Technologies,\ntrator when a gun or a firearm is obvious in the edge. The vol.14,no.2, pp.96\u2013113, 2006.\nexperimental results show that the trained YOLO V3 has [2] UnitedNations,OfficeonDrugsandCrime,Reporton\u201cGlobal\nbetterperformancecomparedtotheYOLOV2modelandis StudyofHomicide\u201d,https://www.unodc.org/documents/data-\nlessexpensivecomputationally.Thereisanimmediateneed and-analysis/gsh/Booklet1.pdf.\n[3] P. M. Kumar, U. Gandhi, R. Varatharajan, G. Manogaran,\ntoupdatethecurrentsurveillancecapabilitieswithimproved\nR.Jidhesh,and T.Vadivel, \u201cIntelligentfacerecognitionand\nresourcestosupportmonitoringtheeffectivenessofhuman\nnavigationsystemusingneurallearningforsmartsecurityin\noperators. Smart surveillance systems would fully replace\ninternet of things,\u201d Cluster Computing, vol. 22, no. S4,\ncurrent infrastructure with the growing availability of low-\npp. 7733\u20137744,2019.\ncost storage, video infrastructure, and better video pro-\n[4] V.Babanne,N.S.Mahajan,R.L.Sharma,andP.P.Gargate,\ncessing technologies. Eventually, the digital monitoring \u201cMachine learning based smart surveillance system,\u201d in\nsystems in terms of robots would fully replace current Proceedings of the 2019 Third International Conference on",
        "tables": [
          [
            [
              "<<Interface>>\nDAO"
            ],
            [
              "+getLocation()"
            ]
          ],
          [
            [
              "Connectivity"
            ],
            [
              "\u2013Connection con"
            ],
            [
              "+connect()"
            ]
          ],
          [
            [
              "DAO [mp]"
            ],
            [
              "+getLocation()"
            ]
          ],
          [
            [
              "GunDetection"
            ],
            [
              "+setRecord()"
            ]
          ],
          [
            [
              "OfficialPanel"
            ],
            [
              "+getLoaction()"
            ]
          ],
          [
            [
              "Location"
            ],
            [
              "\u2013float lat\n\u2013float lon\n\u2013strib = ng loc"
            ],
            [
              "+getFloat lat()\n+setFloat lat (float lat) : void\n+getFloat lon()\n+setFloat lon (float lon) : void\n+getString loc()\n+setString loc (String loc) : void"
            ]
          ]
        ],
        "forms": {},
        "confidence_score": 1.0
      },
      {
        "page_number": 8,
        "text_content": "MathematicalProblemsinEngineering 9\nI-SMAC (IoT in Social, Mobile, Analytics and Cloud)(I- [19] L.Pang,H.Liu,Y.Chen,andJ.Miao,\u201cReal-timeconcealed\nSMAC),pp. 84\u201386,IEEE, Palladam,India,December2019. object detection from passive millimeter wave images based\n[5] A. Joshi, N. Jagdale, R. Gandhi, and S. Chaudhari, \u201cSmart on the YOLOv3 algorithm,\u201d Sensors, vol. 20, no. 6, p.1678,\nsurveillance system for detection of suspicious behaviour 2020.\nusing machinelearning,\u201din IntelligentComputing, Informa- [20] A.Warsi,M.Abdullah,M.N.Husen,M.Yahya,S.Khan,and\ntionandControlSystems.ICICCS2019.AdvancesinIntelligent N. Jawaid, \u201cGun detection system using YOLOv3,\u201d in Pro-\nSystems and Computing, A. Pandian, K. Ntalianis, and ceedingsofthe2019IEEEInternationalConferenceonSmart\nR. Palanisamy, Eds., vol. 1039, Berlin, Germany, Springer, Instrumentation, Measurement and Application (ICSIMA),\nCham,2020. pp.1\u20134,IEEE, KualaLumpur,Malaysia, August2019.\n[6] K.-E.KoandK.-B.Sim,\u201cDeepconvolutionalframeworkfor [21] G.K.VermaandA.Dhillon,\u201cAhandheldgundetectionusing\nabnormalbehaviordetectioninasmartsurveillancesystem,\u201d faster r-cnn deep learning,\u201d in Proceedings of the 7th Inter-\nEngineering Applications of Artificial Intelligence, vol. 67, national Conference on Computer and Communication\npp.226\u2013234, 2018. Technology, pp. 84\u201388, Kurukshetra, Haryana, November\n[7] S. Y. Nikouei, Y. Chen, S. Song, R. Xu, B.-Y. Choi, and 2017.\nT.Faughnan,\u201cSmartsurveillanceasanedgenetworkservice: [22] S.B.KibriaandM.S.Hasan,\u201cAnanalysisoffeatureextraction\nfrom harr-cascade, SVM to a lightweight CNN,\u201d in Pro- andclassificationalgorithmsfordangerousobjectdetection,\u201d\nceedings of the 2018 IEEE 4th International Conference on in Proceedings of the 2017 2nd International Conference on\nCollaboration and Internet Computing (CIC), pp. 256\u2013265, Electrical & Electronic Engineering (ICEEE), pp. 1\u20134, IEEE,\nPhiladelphia,PA,USA,April2018. Rajshahi, Bangladesh,December2017.\n[8] R.Xu,S.Y.Nikouei,Y.Chenetal.,\u201cReal-timehumanobjects [23] A. Castillo, S. Tabik, F. Pe\u00b4rez, R. Olmos, and F. Herrera,\ntrackingforsmartsurveillanceattheedge,\u201dinProceedingsof \u201cBrightness guided preprocessing for automatic cold steel\nthe 2018 IEEE International Conference on Communications weapondetectioninsurveillancevideoswithdeeplearning,\u201d\n(ICC), pp.1\u20136,KansasCity,MO,USA, May2018. Neurocomputing, vol. 330,pp.151\u2013161, 2019.\n[9] S. Ahmed, A. Ahmed, I. Mansoor, F. Junejo, and A. Saeed, [24] V.Gun, \u201cDatabase,\u201dhttp://kt.agh.edu.pl/grega/guns/.\n\u201cOutput feedback adaptive fractional-order super-twisting\nslidingmodecontrolofroboticmanipulator,\u201dIranianJournal\nof Science and Technology, Transactions of Electrical Engi-\nneering,vol. 45,no.1, pp.335\u2013347, 2021.\n[10] S.Ahmed,H.Wang,andY.Tian,\u201cAdaptivefractionalhigh-\norder terminal sliding mode control for nonlinear robotic\nmanipulator under alternating loads,\u201d Asian Journal of\nControl,2020.\n[11] S. Ahmed, H. Wang, and Y. Tian, \u201cAdaptive high-order\nterminalslidingmodecontrolbasedontimedelayestimation\nfortheroboticmanipulatorswithbacklashhysteresis,\u201dIEEE\nTransactions on Systems, Man, and Cybernetics: Systems,\nvol. 51, no.2, pp.1128\u20131137, 2021.\n[12] J.Redmon,S.Divvala,R.Girshick,andA.Farhadi,\u201cYouonly\nlookonce:unified,real-timeobjectdetection,\u201dinProceedings\nof the IEEE Conference on Computer Vision and Pattern\nRecognition,pp. 779\u2013788,Las Vegas,NV, USA,June 2016.\n[13] A. Farhadi and R. Joseph, \u201cYolov3: an incremental im-\nprovement,\u201dComputerVisionandPatternRecognition,2018.\n[14] C.He,J.Shao,andJ.Sun,\u201cAnanomaly-introducedlearning\nmethodforabnormaleventdetection,\u201dMultimediaToolsand\nApplications,vol. 77, no.22,pp. 29573\u201329588,2018.\n[15] Q.Hu,S.Paisitkriangkrai,C.Shen,A.vandenHengel,and\nF.Porikli,\u201cFastdetectionofmultipleobjectsintrafficscenes\nwithacommondetectionframework,\u201dIEEETransactionson\nIntelligent Transportation Systems, vol. 17, no. 4, pp. 1002\u2013\n1014, 2015.\n[16] M. Grega, A. Matiolan\u00b4ski, P. Guzik, and M. Leszczuk, \u201cAu-\ntomateddetectionoffirearmsandknivesinaCCTVimage,\u201d\nSensors,vol.16, no.1,p. 47, 2016.\n[17] H. Mousavi, S. Mohammadi, A. Perina, R. Chellali, and\nV.Murino,\u201cAnalyzingtrackletsforthedetectionofabnormal\ncrowd behavior,\u201d in Proceedings of the 2015 IEEE Winter\nConferenceonApplicationsofComputerVision,pp.148\u2013155,\nIEEE,Waikoloa,HI,USA, January2015.\n[18] S.Ji,W.Xu,M.Yang,andK.Yu,\u201c3Dconvolutionalneural\nnetworks for human action recognition,\u201d IEEE Transactions\non Pattern Analysis and Machine Intelligence, vol. 35, no.1,\npp.221\u2013231, 2012.",
        "tables": [],
        "forms": {},
        "confidence_score": 1.0
      }
    ]
  },
  {
    "file_path": "pdfs\\7th paper.pdf",
    "total_pages": 6,
    "combined_text": "www.ijcrt.org \u00a9 2020 IJCRT | Volume 8, Issue 3 March 2020 | ISSN: 2320-\n2882\nCROWD DETECTION AND MANAGEMENT IN SURVEILLANCE SYSTEM\n1Nidhi Shetty,2Pooja Sankpal,3Priyanka Shripad,4Sandip Zade\n1,2,3Student, 4Assistant Professor\n1Electronics Department,\n1Atharva College of Engineering, Mumbai, India\nAbstract: The steady increase in population and overcrowding has become an unavoidable factor in any public gathering or on the street\nduring any festive occasions. The development in technology has made monitoring smart and methods of tracking humans has advanced.\nIn order to provide an aid to this problem, our project is proposing to provide technical support to manage the crowd by detecting humans\nand keeping in track the count of the people in the scene. In our study, we develop a system using Raspberry Pi 3 board that consists of\nARMv8 CPU that detects the human heads and provide a count of humans in the region using OpenCV-Python. A Haar cascade\nclassifier is trained for human head detection. Human tracking is achieved by indicating the direction of movement of the person. The\nresults of the analysis will be helpful in managing the crowd in any area with high density of crowds.\nIndex Terms \u2013 Haar Cascade Classifier, Adaboost Algorithm, Head Detection\nI. INTRODUCTION\nIndia\u2019s high rate of increase in population, several times makes it difficult to manage it. In several occasions managing the crowd\nbecomes a daunting task such as in temples during special occasions, in railway stations during peak hours, and so on.\nIn this paper, we propose a method to manage the crowd by keeping in track the count of the people in the scene.The crowd can be\nnavigated to disperse from the crowded region by tracking the people which provides the direction of movement of people. The crowded\nregion will be defined by the maximum permissible count of humans in a particular area.\nUp until now different projects were using the tracking methods based on features like motion blobs, texture and color and head\ndetection based on background modelling, head-shoulder detection using omega model and so on. This project focusses on training a\ncascade classifier for human head detection by taking positive samples (images containing human heads) and negative samples (images\ncontaining plain background).\nII. CONCEPT\nIn this project, we develop a system using Raspberry Pi 3 board that consists of ARMv8 CPU that detects the human heads and provide\na count of humans in the region using OpenCV-Python. A Haar cascade classifier is trained for human head detection. The trained\ncascade is then used to process the video frames in which the human heads are detected and the count of the humans in the scene is\nprovided.\nThe detected human heads are then tracked using optical flow algorithm. This tracking provides the direction of motion of the persons\nin the scene.\nIII. LITERATURE SURVEY\nDetection of objects by using a cascade of simple features was mainly introduced by researchers for face detection. [1] The role of Haar\nfeatures extracted from an integral image in object detection is elaborated in their work.\nAnother method of human detection was done by using human detector that was part-based. Parts may not be clearly visible as they\ndepend on factors inter object occlusion, illumination, etc.[2][3][4] Some other studies used foreground segmentation to detect presence of\nhuman. The background is subtracted from the frame and some types of filtering, for example Gaussian filtering, is done to extract human\nmotion blobs out of the frame. This process is usually done in a grayscale image.[5][6] Density based head detection has been demonstrated\nby some researchers. The density of the crowd is initially estimated from which the heads of the individual persons are detected.In certain\nresearches energy coefficients are used in detection crowd.[7] Head detection for tracking passengers in railway stations has also been a\nmajor field of study for many researchers. Many work has been done to look into the direction in which the passengers move through space\ncoordinate information.\nIJCRT2003013 International Journal of Creative Research Thoughts (IJCRT) www.ijcrt.org 84\nwww.ijcrt.org \u00a9 2020 IJCRT | Volume 8, Issue 3 March 2020 | ISSN: 2320-\n2882\nIV. BLOCK DIAGRAM\nCollect training samples using camera\nPositive Negative\nsamples (human samples (other\nheads) than human\nheads)\nTrain Haar Cascade Classifier\nTrained Cascade Classifier Real Time Video feed\nInterface of Raspberry Pi\nDetection and training of human heads\nFig no. 1Working of project\nV. WORKING PRINCIPLE\nThe working principle of this based on three basic concepts:\n5.1.Haar Cascade Classifier\nObject Detection using Haar feature-based cascade classifiers is an effective object detection method proposed by Paul Viola and\nMichael Jones in their paper, \"Rapid Object Detection using a Boosted Cascade of Simple Features\" in 2001. It is a machine learning\nbased approach where a cascade function is trained from a lotof positive and negative images. It is then used to detect objects in other\nimages.[1]\n5.2.Adaboost Algorithm\nAda-boost or Adaptive Boosting is one of ensemble boosting classifier proposed in 1996.Adaboost uses an iterative procedure where\ndifferent classifiers, that when used independently perform poorly, are combined to form a single strong classifier in order to increase the\nefficiency.\nAdaboost should meet two conditions, namely, a)the classifier should be trained interactively on various weighed training examples,\nb)In each iteration, it tries to provide an excellent fit for these examples by minimizing training error.[6]\nThe procedure in which the adaboost works can be listed as follows1)Initially, Adaboost selects a training subset randomly, 2)It\niteratively trains the AdaBoost machine learning model by selecting the training set based on the accurate prediction of the last training,\n3)It assigns the higher weight to wrong classified observations so that in the next iteration these observations will get the high probability\nfor classification, 4)Also, It assigns the weight to the trained classifier in each iteration according to the accuracy of the classifier. The\nmore accurate classifier will get high weight, 5)This process iterate until the complete training data fits without any error or until reached\nto the specified maximum number of estimators, 6)To classify, perform a \"vote\" across all of the learning algorithms you built.\nIJCRT2003013 International Journal of Creative Research Thoughts (IJCRT) www.ijcrt.org 85\nwww.ijcrt.org \u00a9 2020 IJCRT | Volume 8, Issue 3 March 2020 | ISSN: 2320-\n2882\n5.3.Optical Flow Algorithm\nOptical flow algorithm is based on how images in two consecutive frames form a pattern of motion due to the motion of either\nobject or the camera.\nEach pattern acts as a 2D displacement vector field hence the head and tail of the arrow vector shows the direction of movement of\nthe object from one from to another.\nOptical flow algorithm works on several assumptions:\na)The intensities of pixel of a point object does not differ between consecutive frames, b)The pixel being tracked should have motion\nsimilar to its neighbouring pixel.[8]\nThe optical flow diagram not only gives the direction in which the observer is moving and the objects in the background, but also\nthe environment and structure of objects.\nVI. IMPLEMENTATION\n6.1.Training\nIn this project we firs train a haar cascade classifier to detect human heads in a crowd cascade classifier training involves collection of\npositive samples and negative samples.The positive samples are the images which contain human heads and the negative samples\n(background images) are the images that do not contain human heads. Separate videos are taken in an area with and without humans. The\nvideos are then processed and the frames are segmented.[5] Some of the samples we used for our training purpose. The frames with the\nhumans are then subjected to an annotation tool which allows us to mark the human heads. The process of marking human heads\nconsumes time but larger the number of images with marked human heads given as input better is the accuracy of head detection.\nAnnotation tool is an Open CV application which, when executed, opens the images one after the other. A sample of our image with\nhuman heads marked using annotation tool .The tool outputs a text file containing in each line the path of each image followed by the\npixel locations of the heads marked, each separated by a space.\nPositive samples are then created using the annotation tool output containing the pixel locations of marked human heads in the frame\nand the background images. The cascade classifier is then trained and it built the small stages of classifier. A cascade classifier consists of\na cascade of small stages of classifier each classifying the sub-window on the basis of certain features. The stages in the trained cascade\nare sorted using Adaboost algorithm. The classifier uses the Haar features extracted from the positive samples. Some researches suggest\nthat nstages should be ideally 20 and splits should be 2.npos indicates the number of positive samples and -nneg the number of negative\nsamples. This step is extremely CPU intensive and can take several hours/days to complete.\nFig no.2- Positive Samples\nIJCRT2003013 International Journal of Creative Research Thoughts (IJCRT) www.ijcrt.org 86\nwww.ijcrt.org \u00a9 2020 IJCRT | Volume 8, Issue 3 March 2020 | ISSN: 2320-\n2882\nFig no.3-Negative Sample\nFig no. 4-Marking human heads using annotation tool\n6.2.Detection\nWe capture two real time videos in our institution using a 12 mega pixel camera one consisting of humans and the other with no persons\nin the scene. Surveillance cameras in crowd are specifically positioned in a particular angle and hence video is recorded in a specified angle\nand then trained using that video. We then segment each video into frames and save each in separate directories as positive and\nnegative.[3][4] The video is read each frame and each frame is made to pass through all the stages of cascade classifier. A sequence of\nincreasingly complex classifiers is called a cascade. The positive images are then marked for human heads using annotation tool. In the\ntraining of our cascade classifier using real time video frames, we use 1000 positive samples and 4400 negative samples. The training of 20\nstages cascade classifier takes about 5 days. The trained classifier is then used for head detection. Video frames taken in the same scenario\nare tested against the trained cascade classifier. In the frames, after passing through the twenty stages of the classifier, the head regions are\ndetected and results are obtained. Each frame is compare with several sub windows. The compared window is either passed to the next\nstage or refused by that stage. A window must pass through all the stages. Any stage in the classifier may reject it. After a window passes\nthrough all the stages, it is detected as the human head.\nIJCRT2003013 International Journal of Creative Research Thoughts (IJCRT) www.ijcrt.org 87\nwww.ijcrt.org \u00a9 2020 IJCRT | Volume 8, Issue 3 March 2020 | ISSN: 2320-2882\n6.3.Tracking\nThe detected human heads are then tracked for their position in successive frames to compute the direction of motion of any person. The\ntracking is done by optical flow concept. Using optical flow concept, the corners of the bounding box indicating the human heads in a frame\nare fetched and the corresponding pixels in the successive frames are then found.[3] The movement of a person is determined by comparing\nthe pixel values in each frame. Each person is tracked individually.[4]\nVII. APPLICATION\nCrowd detection can be used in local trains, buses and other daily vehicle services. One would be able to organize the frequency of trains as\nwell as the timings of the trains based on live feed of head counts. It will be beneficial for the commuters since they won\u2019t have to face the\nexhaustion of travelling daily alongwith large crowds.\nTracking of human heads can be applied for surveillance purposes. It will be useful in tracking suspicious individuals or to scan a\nparticular area for thieves.\nAnother application of human crowd detection and analysis is in cases of emergency evacuations. They can be planned using by\nstudying about how crowd interacts with each other and how it reacts to certain situations. These are based on biological models and\npatterns, thus the movements predicted are seemingly accurate.The above mentioned model is apparently utilized within film industries to\nproduce realistic and lifelike simulations and scenes which comes under cgi.\nOther applications can be listed as follows:1. Improving a human detector using superpixels. (a) An algorithm to improve a generic\nhuman detector using an unsupervised learning framework. (b) Representation of humans in terms of superpixel-based Bag-of-Words\nrepresentation.\n2. Part-based multiple-human tracking with occlusion handling (a) An algorithm to track multiple humans using online-learned person-\nspecific classifiers. (b) Representation of humans in terms of part-based model to capture the appearance variations. (c) An occlusion\nhandling approach to improve human detection performance in crowded scenes. (d) An approach to overcome partial occlusions for human\ntracking. 3. Multiple-human segmentation leveraging human detector\n(a) An algorithm to automatically segment multiple humans in videos based on human detections. (b) Representation of humans in\nterms of the part-based detection potentials to capture the spatial distribution (c) A new approach to using tracklet-based CRF\noptimization to smooth the segmentation boundaries. 4. NONA: An efficient tracking system (a) A computer vision system for real-time\nhuman tracking in high- resolution videos. (b) A multi-threaded architecture to process video ingestion, tracking and video output\nsimultaneously. (c) A novel approach to using local frame differencing to handle long-term occlusion.\nVIII. FUTURE SCOPE\nTracking a target across different cameras would be a very useful application for wide area surveillance. The challenge is to Crowd\nbehavior analysis is going to be the future talk which is sometimes also reffered as crowd artificial intelligence or swarm intelligence.\nHowever, there are some difficulties in analyzing behavior in a crowd scene. In order to achieve the goal, the analyzing procedure must be\ndone comprehensively through video surveillance. Human instincts are put to test and are applied to biological and artificial models that form\na complex system of multiple agents and their interactions. This section explores some of the possible directions our work may be taken in\nthe future. Motion-based human detection Our proposed human detection method used only static features such as superpixels or HOG,\nwhile in videos the motion features, e.g. Histogram of 124 Optical Flow (HOF), can also provide very discriminative information. Since the\nlocal motion patterns of humans are different from the background or other objects, these correctly associate a person appearing in different\ncameras, each with differing viewpoints and lighting, whilst considering the spatial and temporal constraints. Human pose estimation based\non segmentation. One interesting area to explore is estimating human poses in videos.\nIX. CONCLUSION\nCrowd management using head detection is realized using computer vision in our study, implementing our study using video taken from\nour institution. We use Haar features and Adaboost algorithm to detect the person\u2019s head region. We track the human using optical flow\nconcept. Using increased number of samples, the results are found to be efficient. The human detection and tracking can generally be used in\nsurveillance tasks.\nIJCRT2003013 International Journal of Creative Research Thoughts (IJCRT) www.ijcrt.org 88\nwww.ijcrt.org \u00a9 2020 IJCRT | Volume 8, Issue 3 March 2020 | ISSN: 2320-2882\nX. REFERENCE LIST\n1) Paul Viola and Michael Jones, \u201cRapid Object Detection using a Boosted Cascade of Simple Features\u201d,\nMitsubishi Electric Research Labs, Cambridge, 2001, IEEE.\n2) P.Papageorgiou, Micheal Oren and Tornaso Poggio, \u201cA General Framework for Object Detection\u201d, Center\nfor Biological and Computational Learning Artificial Intelligence Laboratory, Cambridge.\n3) Ashfin Dehghan, Haroon Idrees, Amir Roshan Zamir and Mubrarak Shah, \u201cAutomatic Detection and\nTracking of Pedestrians in Videos with Various Crowd Densities\u201d, Computer Vision Lab, University of\nCentral Florida, Orlando,USA, 2014, Springer International Publishing Switzerland.\n4) Energetic, Electronic and Communication Engineering Vol:2, No:10, 2008.\n5) Songyan Ma and Tiancang Du. \u201cImproved Adoboost face detection,\u201d International Conference on\nMeasuring Technology and Mechatronics Automation, Changsha, 2010.\n6) Mikel Rodriguez, Ivan Laptev, Josef Sivic and Jean- Y ves Audibert, \u201cDensity-aware person detection\nand tracking in crowds,\u201d Imagine, LIGM, Universite Paris-Est.\nIJCRT2003013 International Journal of Creative Research Thoughts (IJCRT) www.ijcrt.org 89",
    "metadata": {
      "Title": "IJRTI",
      "Author": "Hitesh",
      "Creator": "Microsoft\u00ae Word 2016",
      "CreationDate": "D:20200304170148+05'30'",
      "ModDate": "D:20200304170148+05'30'",
      "Producer": "Microsoft\u00ae Word 2016"
    },
    "error_message": null,
    "pages": [
      {
        "page_number": 0,
        "text_content": "www.ijcrt.org \u00a9 2020 IJCRT | Volume 8, Issue 3 March 2020 | ISSN: 2320-\n2882\nCROWD DETECTION AND MANAGEMENT IN SURVEILLANCE SYSTEM\n1Nidhi Shetty,2Pooja Sankpal,3Priyanka Shripad,4Sandip Zade\n1,2,3Student, 4Assistant Professor\n1Electronics Department,\n1Atharva College of Engineering, Mumbai, India\nAbstract: The steady increase in population and overcrowding has become an unavoidable factor in any public gathering or on the street\nduring any festive occasions. The development in technology has made monitoring smart and methods of tracking humans has advanced.\nIn order to provide an aid to this problem, our project is proposing to provide technical support to manage the crowd by detecting humans\nand keeping in track the count of the people in the scene. In our study, we develop a system using Raspberry Pi 3 board that consists of\nARMv8 CPU that detects the human heads and provide a count of humans in the region using OpenCV-Python. A Haar cascade\nclassifier is trained for human head detection. Human tracking is achieved by indicating the direction of movement of the person. The\nresults of the analysis will be helpful in managing the crowd in any area with high density of crowds.\nIndex Terms \u2013 Haar Cascade Classifier, Adaboost Algorithm, Head Detection\nI. INTRODUCTION\nIndia\u2019s high rate of increase in population, several times makes it difficult to manage it. In several occasions managing the crowd\nbecomes a daunting task such as in temples during special occasions, in railway stations during peak hours, and so on.\nIn this paper, we propose a method to manage the crowd by keeping in track the count of the people in the scene.The crowd can be\nnavigated to disperse from the crowded region by tracking the people which provides the direction of movement of people. The crowded\nregion will be defined by the maximum permissible count of humans in a particular area.\nUp until now different projects were using the tracking methods based on features like motion blobs, texture and color and head\ndetection based on background modelling, head-shoulder detection using omega model and so on. This project focusses on training a\ncascade classifier for human head detection by taking positive samples (images containing human heads) and negative samples (images\ncontaining plain background).\nII. CONCEPT\nIn this project, we develop a system using Raspberry Pi 3 board that consists of ARMv8 CPU that detects the human heads and provide\na count of humans in the region using OpenCV-Python. A Haar cascade classifier is trained for human head detection. The trained\ncascade is then used to process the video frames in which the human heads are detected and the count of the humans in the scene is\nprovided.\nThe detected human heads are then tracked using optical flow algorithm. This tracking provides the direction of motion of the persons\nin the scene.\nIII. LITERATURE SURVEY\nDetection of objects by using a cascade of simple features was mainly introduced by researchers for face detection. [1] The role of Haar\nfeatures extracted from an integral image in object detection is elaborated in their work.\nAnother method of human detection was done by using human detector that was part-based. Parts may not be clearly visible as they\ndepend on factors inter object occlusion, illumination, etc.[2][3][4] Some other studies used foreground segmentation to detect presence of\nhuman. The background is subtracted from the frame and some types of filtering, for example Gaussian filtering, is done to extract human\nmotion blobs out of the frame. This process is usually done in a grayscale image.[5][6] Density based head detection has been demonstrated\nby some researchers. The density of the crowd is initially estimated from which the heads of the individual persons are detected.In certain\nresearches energy coefficients are used in detection crowd.[7] Head detection for tracking passengers in railway stations has also been a\nmajor field of study for many researchers. Many work has been done to look into the direction in which the passengers move through space\ncoordinate information.\nIJCRT2003013 International Journal of Creative Research Thoughts (IJCRT) www.ijcrt.org 84",
        "tables": [],
        "forms": {},
        "confidence_score": 1.0
      },
      {
        "page_number": 1,
        "text_content": "www.ijcrt.org \u00a9 2020 IJCRT | Volume 8, Issue 3 March 2020 | ISSN: 2320-\n2882\nIV. BLOCK DIAGRAM\nCollect training samples using camera\nPositive Negative\nsamples (human samples (other\nheads) than human\nheads)\nTrain Haar Cascade Classifier\nTrained Cascade Classifier Real Time Video feed\nInterface of Raspberry Pi\nDetection and training of human heads\nFig no. 1Working of project\nV. WORKING PRINCIPLE\nThe working principle of this based on three basic concepts:\n5.1.Haar Cascade Classifier\nObject Detection using Haar feature-based cascade classifiers is an effective object detection method proposed by Paul Viola and\nMichael Jones in their paper, \"Rapid Object Detection using a Boosted Cascade of Simple Features\" in 2001. It is a machine learning\nbased approach where a cascade function is trained from a lotof positive and negative images. It is then used to detect objects in other\nimages.[1]\n5.2.Adaboost Algorithm\nAda-boost or Adaptive Boosting is one of ensemble boosting classifier proposed in 1996.Adaboost uses an iterative procedure where\ndifferent classifiers, that when used independently perform poorly, are combined to form a single strong classifier in order to increase the\nefficiency.\nAdaboost should meet two conditions, namely, a)the classifier should be trained interactively on various weighed training examples,\nb)In each iteration, it tries to provide an excellent fit for these examples by minimizing training error.[6]\nThe procedure in which the adaboost works can be listed as follows1)Initially, Adaboost selects a training subset randomly, 2)It\niteratively trains the AdaBoost machine learning model by selecting the training set based on the accurate prediction of the last training,\n3)It assigns the higher weight to wrong classified observations so that in the next iteration these observations will get the high probability\nfor classification, 4)Also, It assigns the weight to the trained classifier in each iteration according to the accuracy of the classifier. The\nmore accurate classifier will get high weight, 5)This process iterate until the complete training data fits without any error or until reached\nto the specified maximum number of estimators, 6)To classify, perform a \"vote\" across all of the learning algorithms you built.\nIJCRT2003013 International Journal of Creative Research Thoughts (IJCRT) www.ijcrt.org 85",
        "tables": [],
        "forms": {},
        "confidence_score": 1.0
      },
      {
        "page_number": 2,
        "text_content": "www.ijcrt.org \u00a9 2020 IJCRT | Volume 8, Issue 3 March 2020 | ISSN: 2320-\n2882\n5.3.Optical Flow Algorithm\nOptical flow algorithm is based on how images in two consecutive frames form a pattern of motion due to the motion of either\nobject or the camera.\nEach pattern acts as a 2D displacement vector field hence the head and tail of the arrow vector shows the direction of movement of\nthe object from one from to another.\nOptical flow algorithm works on several assumptions:\na)The intensities of pixel of a point object does not differ between consecutive frames, b)The pixel being tracked should have motion\nsimilar to its neighbouring pixel.[8]\nThe optical flow diagram not only gives the direction in which the observer is moving and the objects in the background, but also\nthe environment and structure of objects.\nVI. IMPLEMENTATION\n6.1.Training\nIn this project we firs train a haar cascade classifier to detect human heads in a crowd cascade classifier training involves collection of\npositive samples and negative samples.The positive samples are the images which contain human heads and the negative samples\n(background images) are the images that do not contain human heads. Separate videos are taken in an area with and without humans. The\nvideos are then processed and the frames are segmented.[5] Some of the samples we used for our training purpose. The frames with the\nhumans are then subjected to an annotation tool which allows us to mark the human heads. The process of marking human heads\nconsumes time but larger the number of images with marked human heads given as input better is the accuracy of head detection.\nAnnotation tool is an Open CV application which, when executed, opens the images one after the other. A sample of our image with\nhuman heads marked using annotation tool .The tool outputs a text file containing in each line the path of each image followed by the\npixel locations of the heads marked, each separated by a space.\nPositive samples are then created using the annotation tool output containing the pixel locations of marked human heads in the frame\nand the background images. The cascade classifier is then trained and it built the small stages of classifier. A cascade classifier consists of\na cascade of small stages of classifier each classifying the sub-window on the basis of certain features. The stages in the trained cascade\nare sorted using Adaboost algorithm. The classifier uses the Haar features extracted from the positive samples. Some researches suggest\nthat nstages should be ideally 20 and splits should be 2.npos indicates the number of positive samples and -nneg the number of negative\nsamples. This step is extremely CPU intensive and can take several hours/days to complete.\nFig no.2- Positive Samples\nIJCRT2003013 International Journal of Creative Research Thoughts (IJCRT) www.ijcrt.org 86",
        "tables": [],
        "forms": {},
        "confidence_score": 1.0
      },
      {
        "page_number": 3,
        "text_content": "www.ijcrt.org \u00a9 2020 IJCRT | Volume 8, Issue 3 March 2020 | ISSN: 2320-\n2882\nFig no.3-Negative Sample\nFig no. 4-Marking human heads using annotation tool\n6.2.Detection\nWe capture two real time videos in our institution using a 12 mega pixel camera one consisting of humans and the other with no persons\nin the scene. Surveillance cameras in crowd are specifically positioned in a particular angle and hence video is recorded in a specified angle\nand then trained using that video. We then segment each video into frames and save each in separate directories as positive and\nnegative.[3][4] The video is read each frame and each frame is made to pass through all the stages of cascade classifier. A sequence of\nincreasingly complex classifiers is called a cascade. The positive images are then marked for human heads using annotation tool. In the\ntraining of our cascade classifier using real time video frames, we use 1000 positive samples and 4400 negative samples. The training of 20\nstages cascade classifier takes about 5 days. The trained classifier is then used for head detection. Video frames taken in the same scenario\nare tested against the trained cascade classifier. In the frames, after passing through the twenty stages of the classifier, the head regions are\ndetected and results are obtained. Each frame is compare with several sub windows. The compared window is either passed to the next\nstage or refused by that stage. A window must pass through all the stages. Any stage in the classifier may reject it. After a window passes\nthrough all the stages, it is detected as the human head.\nIJCRT2003013 International Journal of Creative Research Thoughts (IJCRT) www.ijcrt.org 87",
        "tables": [],
        "forms": {},
        "confidence_score": 1.0
      },
      {
        "page_number": 4,
        "text_content": "www.ijcrt.org \u00a9 2020 IJCRT | Volume 8, Issue 3 March 2020 | ISSN: 2320-2882\n6.3.Tracking\nThe detected human heads are then tracked for their position in successive frames to compute the direction of motion of any person. The\ntracking is done by optical flow concept. Using optical flow concept, the corners of the bounding box indicating the human heads in a frame\nare fetched and the corresponding pixels in the successive frames are then found.[3] The movement of a person is determined by comparing\nthe pixel values in each frame. Each person is tracked individually.[4]\nVII. APPLICATION\nCrowd detection can be used in local trains, buses and other daily vehicle services. One would be able to organize the frequency of trains as\nwell as the timings of the trains based on live feed of head counts. It will be beneficial for the commuters since they won\u2019t have to face the\nexhaustion of travelling daily alongwith large crowds.\nTracking of human heads can be applied for surveillance purposes. It will be useful in tracking suspicious individuals or to scan a\nparticular area for thieves.\nAnother application of human crowd detection and analysis is in cases of emergency evacuations. They can be planned using by\nstudying about how crowd interacts with each other and how it reacts to certain situations. These are based on biological models and\npatterns, thus the movements predicted are seemingly accurate.The above mentioned model is apparently utilized within film industries to\nproduce realistic and lifelike simulations and scenes which comes under cgi.\nOther applications can be listed as follows:1. Improving a human detector using superpixels. (a) An algorithm to improve a generic\nhuman detector using an unsupervised learning framework. (b) Representation of humans in terms of superpixel-based Bag-of-Words\nrepresentation.\n2. Part-based multiple-human tracking with occlusion handling (a) An algorithm to track multiple humans using online-learned person-\nspecific classifiers. (b) Representation of humans in terms of part-based model to capture the appearance variations. (c) An occlusion\nhandling approach to improve human detection performance in crowded scenes. (d) An approach to overcome partial occlusions for human\ntracking. 3. Multiple-human segmentation leveraging human detector\n(a) An algorithm to automatically segment multiple humans in videos based on human detections. (b) Representation of humans in\nterms of the part-based detection potentials to capture the spatial distribution (c) A new approach to using tracklet-based CRF\noptimization to smooth the segmentation boundaries. 4. NONA: An efficient tracking system (a) A computer vision system for real-time\nhuman tracking in high- resolution videos. (b) A multi-threaded architecture to process video ingestion, tracking and video output\nsimultaneously. (c) A novel approach to using local frame differencing to handle long-term occlusion.\nVIII. FUTURE SCOPE\nTracking a target across different cameras would be a very useful application for wide area surveillance. The challenge is to Crowd\nbehavior analysis is going to be the future talk which is sometimes also reffered as crowd artificial intelligence or swarm intelligence.\nHowever, there are some difficulties in analyzing behavior in a crowd scene. In order to achieve the goal, the analyzing procedure must be\ndone comprehensively through video surveillance. Human instincts are put to test and are applied to biological and artificial models that form\na complex system of multiple agents and their interactions. This section explores some of the possible directions our work may be taken in\nthe future. Motion-based human detection Our proposed human detection method used only static features such as superpixels or HOG,\nwhile in videos the motion features, e.g. Histogram of 124 Optical Flow (HOF), can also provide very discriminative information. Since the\nlocal motion patterns of humans are different from the background or other objects, these correctly associate a person appearing in different\ncameras, each with differing viewpoints and lighting, whilst considering the spatial and temporal constraints. Human pose estimation based\non segmentation. One interesting area to explore is estimating human poses in videos.\nIX. CONCLUSION\nCrowd management using head detection is realized using computer vision in our study, implementing our study using video taken from\nour institution. We use Haar features and Adaboost algorithm to detect the person\u2019s head region. We track the human using optical flow\nconcept. Using increased number of samples, the results are found to be efficient. The human detection and tracking can generally be used in\nsurveillance tasks.\nIJCRT2003013 International Journal of Creative Research Thoughts (IJCRT) www.ijcrt.org 88",
        "tables": [],
        "forms": {},
        "confidence_score": 1.0
      },
      {
        "page_number": 5,
        "text_content": "www.ijcrt.org \u00a9 2020 IJCRT | Volume 8, Issue 3 March 2020 | ISSN: 2320-2882\nX. REFERENCE LIST\n1) Paul Viola and Michael Jones, \u201cRapid Object Detection using a Boosted Cascade of Simple Features\u201d,\nMitsubishi Electric Research Labs, Cambridge, 2001, IEEE.\n2) P.Papageorgiou, Micheal Oren and Tornaso Poggio, \u201cA General Framework for Object Detection\u201d, Center\nfor Biological and Computational Learning Artificial Intelligence Laboratory, Cambridge.\n3) Ashfin Dehghan, Haroon Idrees, Amir Roshan Zamir and Mubrarak Shah, \u201cAutomatic Detection and\nTracking of Pedestrians in Videos with Various Crowd Densities\u201d, Computer Vision Lab, University of\nCentral Florida, Orlando,USA, 2014, Springer International Publishing Switzerland.\n4) Energetic, Electronic and Communication Engineering Vol:2, No:10, 2008.\n5) Songyan Ma and Tiancang Du. \u201cImproved Adoboost face detection,\u201d International Conference on\nMeasuring Technology and Mechatronics Automation, Changsha, 2010.\n6) Mikel Rodriguez, Ivan Laptev, Josef Sivic and Jean- Y ves Audibert, \u201cDensity-aware person detection\nand tracking in crowds,\u201d Imagine, LIGM, Universite Paris-Est.\nIJCRT2003013 International Journal of Creative Research Thoughts (IJCRT) www.ijcrt.org 89",
        "tables": [],
        "forms": {},
        "confidence_score": 1.0
      }
    ]
  },
  {
    "file_path": "pdfs\\8thpaper.pdf",
    "total_pages": 17,
    "combined_text": "ReceivedJanuary13,2021,acceptedFebruary1,2021,dateofpublicationFebruary12,2021,dateofcurrentversionMarch4,2021.\nDigitalObjectIdentifier10.1109/ACCESS.2021.3059170\nWeapon Detection in Real-Time CCTV Videos\nUsing Deep Learning\nMUHAMMADTAHIRBHATTI 1,MUHAMMADGUFRANKHAN 1,(SeniorMember,IEEE),\nMASOODASLAM 2,ANDMUHAMMADJUNAIDFIAZ1\n1DepartmentofElectricalEngineering,NationalUniversityofComputerandEmergingSciences,Faisalabad35400,Pakistan\n2DepartmentofElectricalEngineering,ComsatsUniversityIslamabad,Islamabad45550,Pakistan\nCorrespondingauthor:MuhammadTahirBhatti(tahir.bhatti320@gmail.com)\nThisworkwassupportedbytheHigherEducationCommission(HEC)ofPakistanthroughtheTechnologyDevelopmentFund(TDF)\nunderGrantTDF-02-161.\nABSTRACT Securityandsafetyisabigconcernfortoday\u2019smodernworld.Foracountrytobeeconomically\nstrong, it must ensure a safe and secure environment for investors and tourists. Having said that, Closed\nCircuitTelevision(CCTV)camerasarebeingusedforsurveillanceandtomonitoractivitiesi.e.robberies\nbutthesecamerasstillrequirehumansupervisionandintervention.Weneedasystemthatcanautomatically\ndetect these illegal activities. Despite state-of-the-art deep learning algorithms, fast processing hardware,\nand advanced CCTV cameras, weapon detection in real-time is still a serious challenge. Observing angle\ndifferences,occlusionsbythecarrierofthefirearmandpersonsarounditfurtherenhancesthedifficultyofthe\nchallenge.ThisworkfocusesonprovidingasecureplaceusingCCTVfootageasasourcetodetectharmful\nweapons by applying the state of the art open-source deep learning algorithms. We have implemented\nbinary classification assuming pistol class as the reference class and relevant confusion objects inclusion\nconcept is introduced to reduce false positives and false negatives. No standard dataset was available for\nreal-timescenariosowemadeourowndatasetbymakingweaponphotosfromourowncamera,manually\ncollected images from internet, extracted data from YouTube CCTV videos, through GitHub repositories,\ndatabyuniversityofGranadaandInternetMoviesFirearmsDatabase(IMFDB)imfdb.org.Twoapproaches\nare used i.e. sliding window/classification and region proposal/object detection. Some of the algorithms\nusedareVGG16,Inception-V3,Inception-ResnetV2,SSDMobileNetV1,Faster-RCNNInception-ResnetV2\n(FRIRv2), YOLOv3, and YOLOv4. Precision and recall count the most rather than accuracy when object\ndetection is performed so these entire algorithms were tested in terms of them. Yolov4 stands out best\namongstallotheralgorithmsandgaveaF1-scoreof91%alongwithameanaverageprecisionof91.73%\nhigherthanpreviouslyachieved.\nINDEXTERMS Gundetection,deeplearning,objectdetection,artificialintelligence,computervision.\nI. INTRODUCTION what we speak or write has an impact on the people. Even\nThecrimerateacrosstheglobehasincreasedmainlybecause if the news they heard is crafted having no truth but as it\nofthefrequentuseofhandheldweaponsduringviolentactiv- getsviralinafewhoursbecauseofthemediaandespecially\nity. For a country to progress, the law-and-order situation socialmedia,thedamagewillbedone.Peoplenowhavemore\nmustbeincontrol.Whetherwewanttoattractinvestorsfor depression and have less control over their anger, and hate\ninvestmentortogeneraterevenuewiththetourismindustry, speeches can get those people to lose their minds. People\nalltheseneedsisapeacefulandsafeenvironment.Thecrime canbebrainwashedandpsychologicalstudiesshowthatifa\nratio because of guns is very critical in numerous parts of personhasaweaponinthissituation,hemaylosehissenses\nthe world. It includes mainly those countries in which it is andcommitaviolentactivity.\nlegaltokeepafirearm.Theworldisaglobalvillagenowand High incidents were recorded in past few years with the\nuse of harmful weapons in public areas. Starting with the\nThe associate editor coordinating the review of this manuscript and pastyear\u2019sattacksonacoupleofMosquesinNewZealand,\napprovingitforpublicationwasShadiAlawneh . on March 15, 2019 at 1:40 pm, the attacker attacks the\nThisworkislicensedunderaCreativeCommonsAttribution-NonCommercial-NoDerivatives4.0License.\n34366 Formoreinformation,seehttps://creativecommons.org/licenses/by-nc-nd/4.0/ VOLUME9,2021\nM.T.Bhattietal.:WeaponDetectioninReal-TimeCCTVVideosUsingDeepLearning\nChristchurchAL-NoorMosqueduringaFridayprayerkilling of selecting features manually, CNN automatically learns\nalmost 44 innocent and unarmed worshippers. On the same featuresfromgivendata.\nday just after 15 minutes at 1:55 PM, another attack hap- This article presents an automatic detection and classifi-\npened killing seven more civilians [1]. Active shooter inci- cationmethodofweaponsforreal-timescenariousingstate\ndents had also occurred in USA and then in Europe. The of the art deep learning models. For real-time implemen-\nmostsignificantcaseswerethoseatColumbineHighSchool tation relating the problem question of this work \u2018\u2018detect-\n(USA, 37 victims), Andreas Broeivik\u2019s assault on Uotya ingweaponsinreal-timeforpotentialrobbers/terroristusing\nIsland(Norway,179victims)ortheCharlieHebdonewspaper deep learning\u2019\u2019, detection and classification was done for\nattackkilling23.AccordingtostatsprovidedbytheUNODC, pistol, revolver and other shot handheld weapons as in sin-\namong0.1Millionpeopleofacountry,thecrimesinvolving gle class called pistol and related confusion objects such as\ngunsareveryhighi-e.1.6inBelgium,UnitedStateshaving cell phone, metal detector, wallet, selfie stick in not pistol\n4.7andMexicowithanumberof21.5[2]. class. A major reason behind this was our research done on\nCCTV cameras play an important role to overcome this weapons used in robbery cases and it further motivated us\nproblemandareconsideredtobeoneofthemostimportant to choose pistol and revolver as our target object. We go\nrequirementsforthesecurityaspect.[3].CCTVsareinstalled throughseveralCCTVcapturedrobberyvideosonYouTube\nineverypublicplacetodayandaremainlyusedforproviding and found that almost 95% of cases have pistol or revolver\nsafety, crime investigation, and other security measures for astheweaponused.Withtheimplementationofthissystem,\ndetection. CCTV footage is the most important evidence in manyrobberycrimes,andotherincidentslikewhathappened\ncourts.Afteracrimeiscommitted,lawenforcementagencies last year in New Zealand\u2019s Christchurch mosque could be\narriveatthesceneandtaketherecordingoffootagewiththem controlledusingearlyalarmsystembyalertingtheoperator\n[4].Ifwelookatthesurveillancesystemofdifferentcountries andconcernedauthoritiessoactioncanbetakenimmediately.\naroundtheworld,UKhasabout4.5millioncameras,which Gun detection in real-time is a very challenging task.\nare used for surveillance. Sweden has about 50000 cameras As our desired object has a small size so, detecting it in an\ninstalled around 2010. The government of Poland was able image is also very challenging in presence of other objects,\nto reduce drug cases by 60% and street fights by 40% by especially those objects that can be confused with it. Deep\ninstalling just 450 cameras in the city of Poznan [5]. China learning models faced several below mentioned challenges\nhastheworld\u2019sbiggestsurveillancesystemand170million fordetectionandclassificationtask:\ncamerasaroundthenation,andtheseareexpectedtoexpand\n\u2022 The first and main problem is the data through which\nthreetimes,throughanadditional400milliontobeconnected\nCNNlearnitsfeaturestobeusedlaterforclassification\nby 2020. It took only seven minutes for Chinese officials\nanddetection.\nto find and apprehend BBC reporter John Sudworth using\n\u2022 Nostandarddatasetwasavailableforweapons.\ntheir strong CCTV cameras network and facial recognition\n\u2022 For real-time scenarios, making a novel dataset manu-\ntechnologyandputthecriminalbehindthebar[6].\nallywasaverylongandtime-consumingprocess.\nIn previous years, though having surveillance cameras\n\u2022 Labelingthedesireddatabaseisnotaneasytask,asall\ninstalled, to use them for security purposes was not an easy\ndataneedstobelabeledmanually.\nand dependable method. A human has to be there all the\n\u2022 Different detection algorithms were used, so a labeled\ntimetomonitorscreens.CCTVoperatorhastomonitor20-\ndatasetforonealgorithmcannotbeutilizedfortheother\n25screensfor10hours.Hehastolook,observe,identify,and\none.\ncontrolthesituationthatcanbeharmfultotheindividualsand\n\u2022 Every algorithm requires different labeling and pre-\ntheproperty.Asthenumberofscreensincreases,theconcen-\nprocessingoperationsforthesame-labeleddatabase.\ntrationofthepersondecreasesconsiderablytomonitoreach\n\u2022 As for real-time implementation, detection systems\nscreen with time. It is impossible for the person monitoring\nrequire the exact location of the weapon so gun block-\nthescreenstokeepthesamelevelofattentionallthetime[7].\ningorocclusionisalsoaproblemthatarisesfrequently\nThe solution to aforementioned problem is to install\nanditcouldoccurbecauseofself,inter-object,orback-\nsurveillancecameraswiththeabilitytoautomaticallydetect\ngroundblocking.\nweaponsandraisealarmtoalerttheoperatorsorsecurityper-\nsonals.However,thereisnotmuchworkdoneonalgorithms Different approaches are used in this work for weapon\nfor weapon detection in surveillance cameras, and related classificationanddetectionpurposebutallhavedeeplearning\nstudies are often considering concealed weapon detection and CNN architecture behind them because oftheir state of\n(CWD), mostly using X-rays or millimeter waves images the art performance. Training from scratch took very much\nemployingtraditionalmachinelearningtechniques[8]\u2013[12]. time so the Transfer learning approach was used and Ima-\nIn the past few years, deep learning in particular convo- geNet and COCO (common objects in context) pre-trained\nlutional neural network (CNN) has given groundbreaking models are used. Different datasets were made for classifi-\nresults in object categorizing and detection. It has achieved cation and detection. For real-time purposes, we made our\nfinest results thus far in classical problems of image pro- datasetbytakingweaponphotosfromthecamera,datawas\ncessingsuchasgrouping,detectionandlocalization.Instead extractedmanuallyfromrobberyCCTVvideos,downloaded\nVOLUME9,2021 34367\nM.T.Bhattietal.:WeaponDetectioninReal-TimeCCTVVideosUsingDeepLearning\nfromimfdb(internetmoviefirearmdatabase),databyuniver- detect hidden weapons in a situation where the image of\nsityofGranadaandotheronlinerepositories.Alltheworkhas the scene was present over and under exposed area. Their\nbeendonetoachieveresultsinreal-time. methodologywastoapplyahomomorphicfiltercapturedat\nThemaincontributionsofthisworkare:presentationofa distinct exposure conditions to visual and IR pictures [16].\nfirst detailed and comprehensive work on weapon detection Current techniques attain high precision by using various\nthat can achieve detection in videos from real-time CCTV combinations of extractors and detectors, either by using\nandworkswelleveninlowresolutionandbrightnessbecause easy intensity descriptors, boundary detection, and pattern\nmost of the work done earlier is on high definition training matching[9]orbyusingmorecomplicatedtechniquessuch\nimages but realtime scenario needs realtime training data ascascadeclassifierswithboosting.\nas well for better results, finding of the most suitable and CWD though had worked for some sort of cases but it\nappropriateCNNbasedobjectdetectorfortheapplicationof had many limitations. These systems were based on metal\nweapondetectioninreal-timeCCTVvideostreams,making detection; non-metallic guns cannot be detected. They were\nofanewdatasetbecausereal-timedetectionalsoneedsreal- costly to use in many locations because they need to be\ntimetrainingdatasowemadeanewdatabaseof8327images coupledwithX-rayscannersandconveyorbeltsandresponds\nandpreprocesseditusingdifferentOpenCVfiltersi.e.Equal- to all metallic objects, so were not accurate. Economic cost\nized, Grayscale and clahe that helped in detecting images andhealthriskslimitedthepracticalimplementationofsuch\ninlowbrightnessandresolution,introducingtheconceptof methods. Furthermore, video-based firearm detection was a\nrelatedconfusionclassestoreducefalsepositivesandnega- preventivemeasureforacousticdetectionofgunshotandcan\ntives,trainingandtestingofournoveldatabaseonthelatest becombinedwithitforimplementation[17],[18].\nstate of the deep learning based classification and detection Theideaofautomatedimageprocessingforpublicsecurity\nmodelsamongthemYolov4performedbestintermsofboth purposesinmanyfieldshasbeenwellrecognizedandstudied.\nspeed and accuracy and our selected trained model predict CCTVwastheultimateneedforthiskindofworktoprogress.\nimagesatalmosteveryorientation,angle,andview,achieving CCTV was first used back in 1946 in Germany and at that\nthe highest mean average precision of 91.73% along with a time, these cameras were installed to observe the launch of\nF1-scoreof91%onYolov4. a rocket named V2 [19]. Although it had been used earlier,\nTherestofthepaperisorganizedasfollows:relatedwork majorimprovementshappenedinthelasttwodecades.With\nisdiscussedinSectionII.Theimplementationmethodology the advancement in CCTV technology, visual object recog-\nbasedondeeplearningalgorithmsisexplainedinSectionIII. nition and detection for surveillance, control, and security\nThedatasetconstruction,annotation,andpreprocessingusing were performed. In 1973, Charge-Coupled Device (CCD)\ndifferent filters have been discussed in section IV, which was developed, which made the deployment of surveillance\nfollows the experiments and results in Section V. Finally, cameras possible by 1980 [20]. If we go a bit forward in\ntheconclusionandfutureworkisdiscussedinSectionVI. time,acompanynamedAxisCommunicationdevelopedthe\nfirst-evernetworkcamera,whichenabledthetransformation\nII. RELATEDWORK of surveillance cameras from analog to digital [20]. This\nTheproblemofdetectionandclassificationofobjectsinreal- transformation of analog to digital video made it possible\ntime started after major developments in the CCTV field, for everyone to apply image processing, machine learning,\nprocessing hardware, and deep learning models. Very little and computer vision techniques on videos recorded from\nwork has been done in this field before and most of the surveillancecameras.In2003,RoyalPalmMiddleSchoolin\nprevious effort was related to concealed weapon detection Phoenixusedfacialrecognitionforthefirsttimefortracking\n(CWD). missingchildren.\nStartingwithconcealedweapondetection(CWD),before Several object detection algorithms were proposed in the\nits use in weapon detection, it was used for luggage control fieldofcomputervisiontomakesurveillancesystembetter.\nand other security purposes at airports and was based on Objectdetectionalgorithmswereusedinseveralsectorslike\nimagingtechniqueslikemillimeter-waveandinfraredimag- anomaly detection, deterrence, human detection, and traffic\ning[8].Sheenetal.suggestedCWDmethodbasedonathree- monitoring [21]. R. Chellapa et.al. discussed briefly object\ndimensional millimeter (mm) wave imaging method, for tracking and detection in surveillance cameras [22]. The\ndetectinghiddenweaponsatairportsandothersafelocations authors had explained the tracking of an object using mul-\nin the body [13]. Z. Xue et al. suggested a CWD technique tiple surveillance cameras. Another author addressed tech-\nbasedonafusion-basedtechniqueofmulti-scaledecomposi- niques for detecting objects that come into contact with\ntion,whichcombinescolorvisualpicturewithinfrared(IR) another object and are occluded. They also wrote regarding\npicture integration [14]. R. Blum et al. suggested a CWD the segmentation of mean fluctuations. They outlined how\nmethodbasedontheinclusionofvisualpictureandIRormm mean segmentation of shifts can help detect objects. They\nwave picture using a multi-resolution mosaic technique to used a Bayesian Kalman filter with a simplified Gaussian\nhighlightthehiddenweaponofthetargetpicture[15]. blend (BKF-SGM) algorithm to track the detected object\nE.M.Upadhyayet.al.suggestedaCWDtechniqueusing [23].J.SMarquesproposeddistincttechniquesforevaluating\nimage fusion. They used IR image and visual fusion to the efficiency of distinct algorithms for object recognition\n34368 VOLUME9,2021\nM.T.Bhattietal.:WeaponDetectioninReal-TimeCCTVVideosUsingDeepLearning\n[24]. B. Triggs et.al. described histogram oriented gradient learnedthedeeprepresentationofthedataandreducedalot\n(HOG).HOGbecameanovelarchitectureforfeatureextrac- ofmanualwork,andsavedtimeandenergy.\ntion. It was used mostly in applications involved in human RohithVajhalaetal.proposedthetechniqueofknifeand\ndetection [25]. In 2005, the sliding window technique was gun detection in surveillance systems. They had used HOG\nproposedfortherecognitionofnumberplates[26].Theyhad asafeatureextractoralongwithbackpropagationofartificial\nusedaslidingwindowforthepurposeofsegmentationanda neural networks for classification purposes. The detection\nneuralnetworkforcharacterrecognitiononthenumberplate. was performed using different scenarios, first weapon only\nAsdescribedabove,objectiondetectionforthecomputer and then using HOG and background subtraction methods\nvisiontaskswasusedforsomeapplicationswithbigobjects for human before the desired object and claimed to have\nto identity like a person, transport or traffic monitoring, an accuracy of 83% [34]. The aforementioned work uses\netc. Literature review on weapon detection left me with the the CNN along with non-linearity of ReLu, convolutional\nopinionthatregardlessofmanyobjectdetectionalgorithms, neurallayer,fullyconnectedlayer,anddropoutlayerofCNN\nthe algorithms proposed for weapon detection are very few. to reach a result for detection with multiple classes and\nAt last, the idea of firearm detection using the images and implemented their work using the Tensor flow open-source\nvideoswasproposedandfalsealarmswerereducedbyclas- platform. Their system achieved a test accuracy of 90.2 %\nsifying neural networks with region-based descriptors and for their dataset [35]. Micha\u0142Grega et al. proposed knives\ndeterminingregionofinterest(ROI)usingtheslidingwindow and firearm detection in CCTV images. They had applied\ntechniqueandthentrainedtheneuralnetworkclassifierwith MPEG-7 and principle component analysis along with the\nimagepixels[27]. slidingwindowapproach,whichmadetheirworkslowerfor\nWith the development in CCTV\u2019s, object detection for real-time scenarios, although they claimed to achieve good\ndifferent computer vision problems for real-time were per- accuracyontheirtestdataset.[5].\nformedandtheideatodetectfirearmswereintroducedfirst Verma et al. had also used the deep learning technique\nby L. Ward et al. in 2007 [28] and a surveillance system to detect weapons and used the Faster RCNN model. The\nwas also implemented by them a year later in 2008 [29]. work was performed on imfdb, which in my opinion is not\nIntheaforementionedwork,writerscreatedanaccuratepistol suitable to train a model for real-time case. They claimed\ndetection model for RGB pictures. However, in the same to have an accuracy of 93.1% on that dataset but in the\nscene,theirmethoddidnotdetectvariouspistols[11],[10], case of weapon detection, only achieving higher accuracy\n[29]. The approach used comprises of first removing non- is not enough, and precision and recall must the considered\nrelated items from the segmented picture using the K-mean [36]. Siham Tabik et al. work was very much related to\nclustering algorithm and then applying the SURF (Speed the real-time scenario. They used Faster RCNN to detect\nup Robust Features) method to detect points of interest. weapons in real-time using sliding window and region pro-\nDarker gave the concept of SIFT based weapon detection posalmethods.Bestresultswereobtainedbyusingtheregion\nalgorithmandforROIestimation,usedthemotionsegmen- proposaltechnique.Theslidingwindowwasalsoverytime-\ntationmethod[30].SIFTalgorithmispronetofalsealarms, consumingandtook14s/image,ontheotherhand,theregion\nso for estimating ROI, authors used motion segmentation proposal method processed the image in 140ms with 7 fps\nrather than using SIFT on complete image. When ROI was [37]. They trained the network on Faster RCNN using only\ndetermined,thenSIFTwasappliedtodetectfirearmsintheir oneclassfocusingonreducingthefalsepositive.Recentpast\ncase. objectiondetectionworkwiththeapplicationtofirearmswas\nDifferentapproachesthenusedforweapondetectionusing proposedin2019,whereagroupofresearchers,JavedIqbal\nsliding window and region proposal algorithms. HOG (His- etal.proposedorientationawaredetectionoftheobject.This\ntogram of oriented Gradient) models were used to predict system is more suitable for long and thin objects like rifles\nthe objects in the frame. HOG significant work used low- etc. The predicted bounding box in their case was aligned\nlevelfeatures,discriminativelearning,andpictorialstructure with the object and had the less unnecessary area to deal\nalongwithSVM[25],[31],[32].Thesealgorithmswereslow with. Images of very high quality were used for training\nfor real-time scenarios with 14s per image. Although these and testing purposes, which may make it less suitable for\nclassifiersgavegoodaccuracies,theslownessofthesliding real-time scenarios [38]. Jose Luis Salazar Gonz\u2019alez et al.\nwindow method was a big problem, especially for the real- work was very much related to achieve real-time results.\ntimeimplementationpurpose. They did immense experimentation using different datasets\nThisworkfocusesonthestateoftheartdeeplearningnet- and trained Faster\u2013RCNN using Feature Pyramid Network\nwork rather SIFT and HOG features which use handcrafted withResnet50andimprovesthepreviousstateoftheartby\nrules for feature extraction, selection, and detection in real- 3.91%[39].\ntime visual scenario using CCTV cameras. X. Zhang et al.\nconcluded an important finding that helped my work. They III. METHODOLOGY\nconcluded that the automatic feature representation gave Deeplearningisabranchofmachinelearninginspiredbythe\nimproved results rather than manual features [33]. Not only functionalityandstructureofthehumanbrainalsocalledan\nthelearnedfeatureswerebetterinperformance,theyalsohad artificial neural network. The methodology adopted in this\nVOLUME9,2021 34369\nM.T.Bhattietal.:WeaponDetectioninReal-TimeCCTVVideosUsingDeepLearning\nwork features the state of art deep learning, especially the\nconvolutional neural networks due to their exceptional per-\nformanceinthisfield.[40].Theaforementionedtechniques\nare used for both the classification as well as localizing the\nspecific object in a frame so both the object classification\nand detection algorithms were used and because our object\nis small with other object in background so after experi-\nmentationwefoundthebestalgorithmforourcase.Sliding\nwindow/classification and region proposal/object detection\nalgorithmswereused,andthesetechniqueswillbediscussed FIGURE1. ObjectRecognitiontodetectionHierarchy.\nlaterinthissection.\nWe had started by doing the classification using different\nthe feature extracted, it then predicts the label based on the\ndeeplearningmodelsandachievedgoodprecisionbutforthe\nprobability.\nreal-timescenarios,thelowframepersecondsofclassifica-\ntion models were the real issue in implementation. Oxford\nVGG [41], [42], Google Inceptionv3 [43] and Inception- 2) OBJECTLOCALIZATION\nResnetv2 [44], [45] were trained using the aforementioned This method outputs the actual location of an object in an\napproach. imagebygivingtheassociatedheightandwidthalongwith\nTo achieve high precision, increase number of frame itscoordinates.\nper seconds and improve localization, we moved to the\nobject detection and region proposal methods. The differ-\n3) OBJECTDETECTION\nent state of the art deep learning models for object detec-\nThis task uses the properties of the aforementioned algo-\ntion were used and the results were compared in terms of\nrithms. The detection algorithm tells us the bounding box\nprecision, speed, and standard metric of F1 score. State\nhavingxandycoordinateswithassociatedwidthandheight\nof the art deep learning based SSDMobileNetv1 [46]\u2013[48],\nalong with the class label. Non-max suppression is used to\nYOLOv3 [49], FasterRCNN-InceptionResnetv2 [50]\u2013[52],\noutputtheboxwithourdesiredthreshold[54].Thisprocess\nandYOLOv4[53]weretrainedandtested.\ngivesthefollowingresultsaltogether:\nDifferentdatasetsweremadekeepinginmindtheclassifi-\ncationanddetectionproblemasbothhaveaseparaterequire- \u2022 BoundingBox\nmentforperformingthetaskstoachievehighaccuracy,mean \u2022 Probability\naverage precision as well as frame per second for the real-\nIn past object detection was very limited because of less\ntimeimplementation.Tounderstandobjectclassificationand\ndata and low processing power of computers but with the\ndetection let us first briefly understand object recognition\npassageoftimethecomputingpowerofcomputersincreased\nas both the aforementioned types come under the umbrella\nand world moved from CPU\u2019s to Graphic Processing Units\nof this and combined classification and localization make\n(GPU). GPU\u2019s were firstly made for increasing the graphic\ndetection possible for any kind of detection problem giving\nquality of the systems and for gaming but later GPUs were\nclassnameaswellastheregionwhereourdesiredobjectis\nused extensively for deep learning. In ImageNet, competi-\nintheframe.\ntionsstartedandcontainedabout1000classes[55].Thiswas\nthe evolution of machine learning and deep learning. In the\nA. OBJECTRECOGNITION beginning,themodelswerenotverydeep,meanstherewere\nAsthenamesuggests,itistheprocessofpredictingthereal not many layers as they are now in an algorithm. Because\nclassorcategoryofanimagetowhichitbelongsbymaking oftheaforementioneddevelopments,in2012A.Krizhevsky\nprobability high only for that particular class. CNN\u2019s are presentedamodelcalledAlexNettrainedonImageNetand\nused to efficiently perform this process. Many state of the gotthefirstpositioninthatcompetition.Thiswasthebegin-\nart Classification and Detection algorithms uses CNN as a ning of object detection in deep learning. It gave a way to\nbackendtoperformtheirtasks. researchers and then every year the algorithms and models\nFig. 1 depicts that classification and localization come keeponcoming.Allthesealgorithmscontainlayersthatwork\nunderthecategoryofrecognitionandcombinedclassification ontheprincipleoftheconvolutionalneuralnetwork(CNN).\nand localization is performed to do object detection. Let us\nhaveabriefoverviewoftheobjectclassification,localization,\nB. CLASSIFICATIONANDDETECTIONAPPROACH\nanddetection.\nThere are many ways to generate region proposals, but the\nsimplestwayofgeneratingthemisbyusingtheslidingwin-\n1) IMAGECLASSIFICATION dow approach. The sliding window method is slow because\nThe classification model takes an image and slide the ker- filterslidesovertheentireframeandhaslimitations,which\nnel/filteroverthewholeimagetogetthefeaturemaps.From were tackled by the region proposal approach, so we have\n34370 VOLUME9,2021\nM.T.Bhattietal.:WeaponDetectioninReal-TimeCCTVVideosUsingDeepLearning\nthe following two approaches used in our work for both\nclassificationanddetectionmodelsare:\n\u2022 Slidingwindow/ClassificationModels\n\u2022 Regionproposal/ObjectDetectionModels\n1) SLIDINGWINDOW/CLASSIFICATIONMODELS\nIn the method to the sliding window, a box or window is\nmoved over a picture to select an area and use the object\nrecognition model to identify each frame patch covered by\nthewindow.Itisanexhaustivesearchoverthewholepicture\nforobjects.Notonlydoweneedtosearchinthepictureforall\nfeasibleplaces,wealsoneedtosearchondistinctscales.This\nis because models are usually trained on a particular range.\nTheoutcomesareintensofthousands(104)ofpicturespots\nbeing classified [56]. The sliding window method is com-\nputationally very costly because of the search with various\naspectratiosandespeciallyforeachpixelofanimageifthe\nstrideorstepvalueisless.\n2) REGIONPROPOSAL/OBJECTDETECTIONMODELS\nThistechniquetakesanimageastheboundingboxesofinput\nand output proposals related to all areas in a picture most\nprobable to be the object. These regional proposals may be\nFIGURE2. TrainingandOptimizationFlowDiagram.\nnoisy; coinciding not containing the object flawlessly, but\nthere is a proposal among these region proposals related to\npartialderivativeofcostfunctionJ(O)withamultiplierofthe\ntheoriginaltargetobject.Asthismethodtakesapictureasthe learningratealpha\u03b1 fromtheoldorpreviousweightvalue.\nboundingboxesofinputandoutputrelatedtoallpatchesina\nGradientdescentisthemainweightoptimizationalgorithm.\npicturemostprobabletobeacategory,soitproposesaregion\nIt is used as a base in all optimizers used for the modeling\nwiththemaximumscoreasthelocationofanobject.Instead\nandithelpsinconvergingthemodelandreachingtheminima\nof considering all possible regions of the input frame as\nwherewegetthebestanddesiredweightsvalues.\npossibilities,thismethodusesdetectionproposaltechniques\ntoselectregions[57].Region-basedCNNs(R-CNN)wasthe\nD. CONFUSIONOBJECTINCLUSION\nfirstdetectionmodeltointroduceCNNsunderthisapproach\nWe have formulated the problem to reduce the number\n[58].Theselectivesearchmethodofthisapproachproduces\nof false positives and negatives by adding relevant confu-\n2000boxeshavingmaximumlikelihood.\nsion object. The weapon category includes all the handheld\nSelective search is a widely used proposal generation\nweapons such as, pistol, revolver, shotgun and other than\nmethod because it is very fast having a good recall value.\nweaponincludestheobjectsthatcanmostbeconfusedwith\nItisdependentonthehierarchicalcalculationofdesiredareas\npistolclassese.g.mobile,metaldetector,selfiestick,purse,\nestablished on the compatibility of color, texture, size, and\netc.\nshape[59].\nBy understanding the differences between classification\nYolo series is among the state of the art object detection\nand detection algorithms, sliding window, and region pro-\nmodels. Unlike the other region proposal-based methods it\nposalmethods,let\u2019snowlookatthealgorithmsusedforboth\ndivides the input image into an SxS grid and then simulta-\napproaches.\nneously predicts the probability and bounding boxes for an\nobjectwithacenterfallingintoagridcell[49],[53].\nE. CLASSIFIERSANDOBJECTDETECTORS\nTheclassifiersusedundertheslidingwindowapproach:\nC. TRAININGMECHANISM\n\u2022 VGG16\nFig.2describesthegeneralmethodologyusedintrainingand\n\u2022 InceptionV3\noptimization. It starts with defining a problem, finding the\n\u2022 InceptionResnetV2\nrequireddataset,applyingpre-processingmethods,andthen\nfinally training and evaluating the dataset. If the evaluation Theobjectdetectorsusedforreal-timedetectionare:\nis correct then we save those weights as a classifier but \u2022 SSDMobilNetV1\nif it\u2019s incorrect then comes the process of backpropagation \u2022 YoloV3\nalgorithm along with the gradient descent algorithm [60]. \u2022 FasterRCNN-InceptionResNetV2\nInbackpropagation,weightsareoptimizedbysubtractingthe \u2022 YoloV4\nVOLUME9,2021 34371\nM.T.Bhattietal.:WeaponDetectioninReal-TimeCCTVVideosUsingDeepLearning\nThree databases named database1, database2, and\ndatabase3werecreatedonebyoneafterexperimentationon\ndifferent algorithms with diverse images, first for classifi-\ncation and then for object detection. Although the results\nobtained from the classification algorithms were not bad\nbut the frames per second were very slow for real-time\nimplementation. Detail for each database will be discussed\ninthenextsection.\nIV. DATASETCONSTRUCTION,ANNOTATIONAND\nPRE-PROCESSING(D-CAP)\nDataplaysakeyroleinthedevelopmentofanydeeplearning\nmodel as the model learns and extract feature from it. For\na real-time model to detect weapons with minimized pro-\ncessing time and high precision, the importance of accurate\nandrelevantdataincreasesfurtherasallotherprocessesare\ndependentonit.\nWhen we study the stats and goes through almost FIGURE3. DatasetsamplesforpistolClass-Toplefttobottomright[a-d]:\n50-60 videos of robbery on available online resources, (a)CCTVimage(b)MediumResolutionImage(c)ImagewithDark\nbackgroundandLowResolution,(d)FilteredImage.\nwe come to know that 95 percent of the videos have\nrevolver or pistol as a weapon, so we focused on binary\nclassificationwithpistolandrevolvertobeinasingleclass 3) REASONOFCHOOSINGDATACATEGORIESOFPISTOL\ncalledpistol.Besides,tomakethesystemmorepreciseandto CLASS\nreducethefalsepositiveandfalsenegativevaluesweadded Thereasonwechoosepistolandrevolverinthepistolclass\nobjectsthatcanbeconfusedwithaweaponsuchasawallet, is because of our study and analysis after watching many\ncellphone,metaldetectoretcandputtheminaseparateclass robberiesandshootingincidentCCTVvideos.Weconcluded\nnameditasnotpistol. thatalmost95%oftheweaponusedinthosecaseswereeither\nLet\u2019s now discuss the datasets used in our case because, pistolorrevolver.Fig.3showssomesampleimagesforreal-\nin a supervised learning case, the network learns the repre- timefromthecollecteddatasetofthepistolclass.\nsentation of the input data with given true answers, so the\ndatamustbeclean,preprocessed,andproperlyannotatedto\nmakethenetworklearnandpredictbetter. 4) WEAPONDATASETCATEGORIESFORNOT-PISTOLCLASS\nDatasets for this class include objects that can most likely\nbe confused with pistol class objects. Following are some\nA. DATASETCONSTRUCTIONANDSELECTION\nsamplescategoriesforthenotpistolclass:\nThe task of dataset construction and collection was very\nimportantandtoughaswellbecausetherewasnobenchmark \u2022 Wallet\ndatasetavailableforthis.Datasetforreal-timedetectionwas \u2022 MetalDetector\ncollected and constructed in different phases and data was \u2022 Cellphone\ncollected from the internet, extracted from YouTube CCTV \u2022 Selfiestick\nvideos,throughGitHubrepositories,databytheUniversityof\nGranadaresearchgroup,andinternetmoviefirearmdatabase\nimfdb.org. 5) REASONOFCHOOSINGDATACATEGORIESOF\nNOT-PISTOLCLASS\nWeintroducedthisrelevantconfusionobjectconceptbecause\n1) WEAPONDATASETCLASSES\nthese are the objects that can mostly be confused with our\nTheweapondatasetforreal-timeweapondetectionisdivided\ndesired weapon object, so predicting them correctly results\nintothefollowingtwoclasses:\ninreducingthenumberoffalsepositivesandfalsenegatives,\n\u2022 Pistol\nhenceincreasingoverallaccuracyandprecision.\n\u2022 Not-Pistol\nSome previously done work though had objects other\nthan weapons used for the background or class other than a\n2) WEAPONDATASETCATEGORIESFORPISTOLCLASS weapon but they had samples like cars, airplanes, cats, etc\nDatasetforthisclassincludesweaponsamplesofthefollow- andthereareveryfewerchancesforthemtobeconfusedwith\ningcategories: ourdesiredweapon,whichisverysmallascomparedtothem.\n\u2022 Pistol Asourdesiredobjectsofpistolclassaresmallsotherearelot\n\u2022 Revolver ofchancesforthemtobeconfusedwithsomeotherobjects\n\u2022 Othershothandheldweapons having some features like that. Fig. 4 shows some sample\n34372 VOLUME9,2021\nM.T.Bhattietal.:WeaponDetectioninReal-TimeCCTVVideosUsingDeepLearning\nTABLE1. DataDistribution.\ning the shortcomings and problems of the previous dataset.\nTheneedforthisdatasetarisesbecausethoughwegotarea-\nsonable accuracy from classification models but the frames\nper second were very few. To detect images from CCTV\nvideos,similarkindsoftrainingdatamustbeincludedsowe\nmadeourowndatasettotacklethisissue.\nThis dataset contains 8327 images divided into the pis-\ntol and not pistol class. In this case, a related confusion\nFIGURE4. Datasetsamplesfornot-pistolClass-Toplefttobottomright\ndata concept was introduced to reduce false positives and\n[a-d]:(a)CellPhone(b)MetalDetector(c)SelfieStick(d)Wallet.\nfalse negatives in real-time detection. Dataset images were\nextracted from several online sources, from CCTV videos\nimagesfromthecollecteddatasetofthenotpistolclasswhich\nfor the particular robbery scenario, made our own dataset\nhelpsinreducingfalsepositivesandnegatives.\nwith a weapon in hand for the diverse scenario, did data\naugmentation,andfinally,itwasseparatedfortestandtrain\nB. DATASETSFORREAL-TIMEDETECTION case.\nThisworkdealswiththebinaryclassificationforareal-time\nscenario so two classes were made and pistol and revolver C. DATADISTRIBUTION\nimages were included in pistol class and not pistol class Each of the aforementioned datasets are divided into the\nincludeconfusionclasseslikemobilephone,metaldetector, following categories mentioned in Table 1 with split size\nselfie stick, wallet, purse, etc. For the pistol and not pistol defining the separation percentage of the total data into test\nclasses, we have made three datasets, which are explained andtrain.\nbelow.\nD. DATAPRE-PROCESSINGANDANNOTATION\n1) DATASET1 Many things affect the performance of a Machine Learning\nThis was the initial dataset used while starting this work. (ML)modelforaspecifiedjob.First,therepresentationand\nInthisdataset,wehad1732imagesintotal,with750images qualityofthedataareessential.Iftherearemanyirrelevant\ninpistolclassand950innotpistolclass.Datasetwasdivided and redundant data existing or noisy and unreliable data,\nby the separation criteria described in Table 1 of train and then it is harder to discover representation during the train-\ntest. Images were collected from online sources and imfdb ing stage. Data preparation and filtering steps take signifi-\ndatabase and sliding window classification algorithms were cant processing time in ML issues [61]. The pre-processing\ntrainedandtestedonit. process involves data cleaning, standardization, processing,\nextraction and choice of features, etc. The final training\n2) DATASET2 datasetistheresultofpre-processingprocessesappliedtothe\nThiswastheseconddatasetmadeforthereal-timescenario. collecteddataset.\nThisdatasetcontains5254imagesandclassification,aswell Pre-processingisnecessaryforbettertrainingofamodel,\nasobjectdetectionalgorithms,weretrainedonthisdatasetto sothefirststepistomakethesamesizeorresolutionofthe\nmeet the task. Images were extracted for real-time scenario dataset.Thenextstepistoapplythemeannormalization.The\nwith the desired object in hand from online, sources, imfdb thirdstepismakingboundingboxesontheseimages,which\ndatabase,andImageNetwebsite.Datasetwasdividedbythe is also called annotation, localization, or labeling. In data,\nseparationcriteriaoftestandtrainexplainedinTable1. labeling a bounding box is made on each image. The value\nx,ycoordinates,andwidth,heightofthelabeledobjectwas\n3) DATASET3 storedinxml,csvortxtformat.Followingarethefourmain\nThis was the third dataset constructed for the real-time sce- stepsofdatapreprocessing:\nnarioand objectdetectionalgorithmswere performedonit. \u2022 Imagescaling\nThisdatabasewasmadebyenhancingdataset2byovercom- \u2022 Data-augmentation\nVOLUME9,2021 34373\nM.T.Bhattietal.:WeaponDetectioninReal-TimeCCTVVideosUsingDeepLearning\nFIGURE5. ImageAugmentationandScaling.\nFIGURE6. ImageAnnotationandLabelling.\n\u2022 Imagelabeling\n\u2022 ImageFilteringusingOpenCV\n\u2022 RGBtoGrayscale\n\u2022 Equalized\n\u2022 Clahe\nFig. 5, 6 and 7 shows the results after applying the afore-\nmentionedpre-processingtechniques.\nV. EXPERIMENTS,RESULTSANDANALYSIS\nWehavedetectedweaponsinreal-timeCCTVstreamsinlow\nresolution,darklightwithreal-timeframepersecond.Most\noftheworkdonebeforewasondetectingimagesandvideos\nof high quality and because those models were trained on\nhigh-qualitydatasets,itisnotpossibletothendetectanobject\noflowresolutioninreal-time.Theresultsareanalyzedafter\ntrainingandtestingmodelsondatasetsmentionedinTable1.\nAs described in the methodology section the results for FIGURE7. ImageFiltrationusingOpenCVFilters-(a)OriginalImage\n(b)EqualizedFilterResult(c)GrayScaleFilterResult(d)ClaheFilter\ndifferentapproachesareevaluated.Ourmainproblemstate-\nResult.\nmentisofreal-timedetectionbecause97%ofweaponused\ninrobberycaseswerepistolorrevolver,sodifferentdataset\nresults have been evaluated here for sliding window and\nTruePositives\nregionproposalapproach. Recall= (2)\nThe performance of these models was analyzed by com- TruePositives+FalseNegatives\nparingthemintermsofthestandardmetricsofF1-scoreand\n2\u2217Precision\u2217Recall\nF1-score= (3)\nframepersecondsalongwithmeanaverageprecision(mAP) Precision+Recall\nforthebestperformedmodelandthesetermsarecalculated\nbyusingthebelowequation1,2and3.F1scoreisratioofthe\nA. DATASET-1EXPERIMENTATIONANDRESULTS\nprecisionandrecallfunctions.\nDataset 1 contains 1732 images distributed between two\nTruePositives classes of pistol and not-pistol with 750 and 982 images\nPrecision = (1)\nTruePositives+FalsePositives ineachclassrespectively.Experimentationondataset-1has\n34374 VOLUME9,2021\nM.T.Bhattietal.:WeaponDetectioninReal-TimeCCTVVideosUsingDeepLearning\nTABLE2. SlidingWindowResultsComparisonDataset-1.\nFIGURE8. Bestslidingwindowmodelaccuracygraph:InceptionResNetv2.\nbeenperformedusingtheslidingwindow/classificationmod-\nelsofVGG16,Inceptionv3andInceptionResNetv2.\nAfter experimentation, we have analyzed that the results\nobtained are not good because most of the images of this\ndatasethavewhiteorthesamekindofbackgroundwhichlead\ntoapointwherethemodelalsostartslearningthebackground\nas its region of interest (ROI) and in real-time background\nvariessoanewdatasetwasrequiredtotrainandtestthemodel\nonimageswithdiversecasesandbackground.Table2shows\nthe results for the aforementioned models using this dataset\ngivingprecision,recall,andF1-score.\nFIGURE9. BestslidingwindowmodelLossgraph:InceptionResNetv2.\nB. DATASET-2EXPERIMENTATIONANDRESULTS\nThis dataset contains two classes of pistol and not-\npistol with 3000 and 2254 images in each class respec-\ntively. Table 3 shows results based on it. Experimenta-\ntion on dataset-2 has been performed using the sliding\nwindow/classification models of VGG16, Inceptionv3, and\nInceptionResNetv2.\nExperimentationresultsshowthatthoughwegetareason-\nable accuracy from classification models using this dataset\nbut the frames per second were very few and which was a\nbigprobleminmakingareal-timeweapondetector.Among\nthese classification models, InceptionResnetV2 performed\nbest and achieves the best results. Table 3 shows the results\nunder the sliding window methods using dataset 2 and Fig.\n8, 9, and 10 shows the accuracy, loss, and confusion matrix\nFIGURE10. BestslidingwindowmodelConfusionMatrix:\nrespectivelyforthebestclassificationmodelunderthesliding InceptionResNetv2.\nwindowapproach.\nC. DATASET-3EXPERIMENTATIONANDRESULTS \u2022 FasterRCNN-InceptionResNetV2\nAfter experimentation on the previous two datasets and not \u2022 YoloV4\nfindingsatisfactoryresultsforthereal-timecaseanewdataset Each model had its pros and cons. SSD-MobileNet is\nwas made. Images were collected from robbery videos, our goodintermsofprocessingframespersecond.FasterRCNN-\nown dataset images holding a weapon in different scenar- InceptionResNetv2 has good precision and recall but not\nios, images with a dark background and low resolution, processingspeed.Yolofamilyhasaseriesofmodels.Ithasa\nand images extracted from applying different OpenCV fil- differentapproachforthedetectionpurpose.Unliketheother\nters are added to make real-time detection possible. A total regionproposalbasedmethods,itdividestheinputimageinto\nof8327imagesareusedinthiscase.Followingobjectdetec- anSxSgridandthensimultaneouslypredictstheprobability\ntionmodelsweretrainedandevaluatedusingthisdataset: andboundingboxesforanobjectwiththecenterfallingintoa\n\u2022 SSDMobilNetV1 gridcell.WehavetrainedthelateststateoftheartYolov3and\n\u2022 YoloV3 Yolov4onourownweapondataset3forreal-timedetection\nVOLUME9,2021 34375\nM.T.Bhattietal.:WeaponDetectioninReal-TimeCCTVVideosUsingDeepLearning\nTABLE3. SlidingWindowResultsComparisonDataset-2.\nTABLE4. RegionProposal/ObjectDetectionModels-Dataset-3.\nFIGURE11. BestObjectDetectionModel-Yolov4:lossvsmAP.\nTABLE5. BestPerformedModelYolov4:mAPCalculation.\nandbestresultswereobtainedthroughYOLOv4intermsof\nboth processing speed and precision. Table 4 below shows\nthe results for the aforementioned detection models for this\ndatasetatastandardthresholdscoreof50%.\nYolov4 performs best among all the models of both the\nsliding window and region proposal approach. Performance\ngraphforyolov4intermsoflossandmeanaverageprecision\n(mAP)onavalidationdatasetisshowninFig.11.Wecansee\nthat how smooth is the model loss curve and how precisely\nit converges to the best level giving a very good loss score\nof1.062andameanaverageprecisionof91.73%.Themean\naverageprecisionisthemeanoftheaverageprecisionvalues\nforalltherelevantclasses.Theassociatedvaluesofaverage\nstandard metrics of precision, recall, and F1-score for\nprecision(AP)forpistolandnot-pistolclassforthecalcula-\nevaluation.\ntionofmeanaverageprecisionvalueisgiveninTable5.\n\u2022 Some classification models showed good results but\nThe mean average precision value is calculated for the\nthey were not suitable for a real-time scenario, were\nyolov4modelasitperformsbestinallscenarioandaccurately\nslow, not much accurate, and fast as compared to the\ndetected the desired object even when the object has a very\nobjectdetectionmodelsastheyperformsverywelland\nsmall presense in the frame and there were lots of other\nachievedhighprecisionandrecall.\nobjectsinthebackgroundaswell.\n\u2022 Thereasonwhysomeclassificationmodelshaveagood\nF1-scoreisthetrainingandevaluationoninitialdatasets\nD. ANALYSISANDDISCUSSION wemadewhenstartingthiswork,butafterexperimenta-\n\u2022 Table2,3,and4aboveshowsthecomparisonbetween tion,wecometoknowthatthesemodelsarenotsuitable\nthe classification and object detection models using forreal-timescenarioshavingthebackgroundobjects.\n34376 VOLUME9,2021\nM.T.Bhattietal.:WeaponDetectioninReal-TimeCCTVVideosUsingDeepLearning\nFIGURE13. Bestperformedmodelscomparison:AccuracyvsF1-score.\nFIGURE12. ObjectDetectionmodelsPerformance/ComparisonGraph.\nTABLE6. Yolov4HyperParameters.\n\u2022 Object detection models performed well for the real-\ntime scenario and performance comparison in terms of\nspeedandF1-scorebetweenthedetectionmodelscanbe\nseen from Fig. 12. Inference results are obtained using\ntheNVIDIARTX2080tiforeachmodel.\n\u2022 Thestandardmetricsofmeanaverageprecision(mAP),\nrecallandF1-scorearecalculateandallthemodelshave\nbeencomparedatabenchmarkIoUthresholdof0.50or\n50%.\n\u2022 Yolov4 performs best amongst all models with a mean\naverage precision and F1-score of 91.73% and 91%\nrespectively with detection confidence of 99% in the\nmajorityofcases.\n\u2022 Comparison in terms of test accuracy vs F1-score\nTABLE7. Comparisonwithsomeexistingstudies.\nfor the best-performed models of both classifica-\ntion and detection approaches is shown in the\nFig.13. Accuracy and F1-score for VGG, Inceptionv3,\nInceptionResNetv2, SSDMobileNet, FasterRCNN-\nInceptionResNetv2, Yolov3 and yolov4 are 78.20%,\n85.20%, 92.20%, 79%, 96%, 94%, 99% and 81.69%,\n84.36%,85.74%,59%,87%,86%and91%respectively.\n\u2022 Fig.14-19showstheinferenceordetectionresultsofour\nmodelforpistolandnotpistolclassonimages,videos,\nandreal-timeCCTVstreams.\n\u2022 Hyperparameters used in training the best-performed\ndetectorYolov4canbeobservedfromTable6.\nItisveryhardtodoacomparisonwithstudiesconducted\npreviously on this subject because each study has its own\ndataset, models and metrics used to evaluate performance. in terms of mAP and precisoin at a standard iou threshold\nItshouldalsobenoticedachieverealtimedetection,wealso of50%,whicheverwasavailable.\nneedtohavearealtimedatasetfortraningbecausewithhigh\nqualitytrainingimageswecannotachieveresultsinrealtime. E. DETECTIONRESULTS-PISTOLCLASSWITHOUT\nEach study also has different testing conditions, either just BACKGROUND\non images, videos or on images with high quality but our SeeFigure14.\napproachfromstartwastoachieverealtimeresults.Insome\nstudies,theperformanecemetricusedisaccuray,othershave F. DETECTIONRESULTS-PISTOLCLASSWITH\nprecisoinormeanaverageprecision(mAP)butmostlymAP BACKGROUND\nis used as standard so we have given comparison results SeeFigure15.\nVOLUME9,2021 34377\nM.T.Bhattietal.:WeaponDetectioninReal-TimeCCTVVideosUsingDeepLearning\nFIGURE14. DetectionResults-Onlyweaponinthewholeframewithoutanybackgroundatdifferentangles,brightness,sharpness,andquality.\nFIGURE15. DetectionResults-Toplefttobottomright(a-i):(a)Imagewithfrontandsideview,(b)Imageverticalview(c)ImagewithDarkbackground\nandLowResolutionfullytiltedsideview,(d)Lowbrightnessimagesideviewslightlytilted(e)Imagewiththebackview(f)Fullfrontview(g)SmallCCTV\nobject(h)Verysmallobjectwithsideview(i)Imagewithfullsideview.\nG. DETECTIONRESULTS-PISTOLCLASSINVIDEOS I. DETECTIONRESULTS\u2013NOTPISTOLCLASS\nSeeFigure16. SeeFigure18.\nH. DETECTIONRESULTS-PISTOLCLASSINREALTIME\nCCTVSTREAMS J. MISDETECTIONS\nSeeFigure17. SeeFigure19.\n34378 VOLUME9,2021\nM.T.Bhattietal.:WeaponDetectioninReal-TimeCCTVVideosUsingDeepLearning\nFIGURE16. DetectionResults-Toplefttobottomright(a-f)-video1inference(a-c),video2inference(d-f):(a)Smallobject-sideviewtilted,(b)Small\nobjectwithsideview(c)Smallobjectfrontview(d)sideview(e)Topviewdoubleobject(f)Smallobjectwithfrontandsideview.\nFIGURE17. DetectionResults-Toplefttobottomright(a-i)-cctvstream1(a-c),cctvstream2(d-f),cctvstream3(g-i):(a)SmallobjectinLowresolution(b)\nTiltedObject(c)LowResolutionverticalobject,(d)Daylightsideviewwithslightlytilted(e)Daylightsideview(f)Daylightsideviewflipped(g)Small\nobjectmediumresolution(h)verticalview(i)sideview.\nVI. CONCLUSIONANDFUTUREWORK and tourists, as security and safety are their primary needs.\nFor both monitoring and control purposes, this work has We have focused on detecting the weapon in live CCTV\npresentedanovelautomaticweapondetectionsysteminreal- streams and at the same time reduced the false negatives\ntime. This work will indeed help in improving the secu- and positives. To achieve high precision and recall we con-\nrity, law and order situation for the betterment and safety structed a new training database for the real-time scenario,\nof humanity, especially for the countries who had suffered then trained, and evaluated it on the latest state-of-the-art\na lot with these kind of violent activities. This will bring deep learning models using two approaches, i.e. sliding\na positive impact on the economy by attracting investors window/classification and region proposal/object detection.\nVOLUME9,2021 34379\nM.T.Bhattietal.:WeaponDetectioninReal-TimeCCTVVideosUsingDeepLearning\nFIGURE18. DetectionResults-Toplefttobottomright(a-d):(a)Cellphone(b)Metaldetector(c)Wallet(d)\nSelfiestick.\nFIGURE19. Misdetections:FalsepositivesandNegatives.\nDifferentalgorithmswereinvestigatedtogetgoodprecision ACKNOWLEDGMENT\nandrecall. TheauthorswishtoextendgratitudetoMr.RehanMushtaq\nThroughaseriesofexperiments,weconcludedthatobject of Ingenious Zone who provided assistance as an industrial\ndetection algorithms with ROI (Region of Interest) perform partnerinmakingthisworkpossible.\nbetter than algorithms without ROI. We have tested many\nREFERENCES\nmodels but among all of them, the state-of-the-art Yolov4,\n[1] (2019). Christchurch Mosque Shootzings. Accessed: Jul. 10, 2019.\ntrainedonournewdatabase,gaveveryfewfalsepositiveand\n[Online].Available:https://en.wikipedia.org/wiki/Christchurch_mosque_\nnegative values, hence achieved the most successful results. shootings\nItgave91.73%meanaverageprecision(mAP)andaF1-score [2] (2019). Global Study on Homicide. Accessed: Jul. 10, 2019. [Online].\nAvailable: https://www.unodc.org/unodc/en/data-and-analysis/global-\nof 91% with almost 99% confidence score on all types of\nstudy-on-homicide.html\nimagesandvideos.Wecansaythatitsatisfactorilyqualifies [3] W.Deisman,\u2018\u2018CCTV:Literaturereviewandbibliography,\u2019\u2019inResearch\nas an automatic real-time weapon detector. Looking at the and Evaluation Branch, Community, Contract and Aboriginal Policing\nServices Directorate. Ottawa, ON, Canada: Royal Canadian Mounted,\nresults,wegotthehighestmeanaverageprecision(mAP)F1-\n2003.\nscore as compared to the research done before for real-time [4] J. Ratcliffe, \u2018\u2018Video surveillance of public places,\u2019\u2019 US Dept. Justice,\nscenarios. Office Community Oriented Policing Services, Washington, DC, USA,\nTech.Rep.4,2006.\nThefutureworkincludesreducingthefalsepositivesand\n[5] M.Grega,A.Matiola\u0142ski,P.Guzik,andM.Leszczuk,\u2018\u2018Automateddetec-\nnegativesevenmoreasthereisstillaneedforimprovement. tionoffirearmsandknivesinaCCTVimage,\u2019\u2019Sensors,vol.16,no.1,\nWemightalsotrytoincreasethenumberofclassesorobjects p.47,Jan.2016.\n[6] TechCrunch. (2019). China\u2019s CCTV Surveillance Network Took Just 7\nin the future but the priority is to further improve precision\nMinutes to Capture BBC Reporter. Accessed: Jul. 15, 2019. [Online].\nandrecall. Available:https://techcrunch.com/2017/12/13/china-cctv-bbc-reporter/\n34380 VOLUME9,2021\nM.T.Bhattietal.:WeaponDetectioninReal-TimeCCTVVideosUsingDeepLearning\n[7] N. Cohen, J. Gattuso, and K. MacLennan-Brown. CCTV Operational [29] I.T.Darker,A.G.Gale,andA.Blechko,\u2018\u2018CCTVasanautomatedsensor\nRequirements Manual 2009. St Albans, U.K.: Home Office Scientific for firearms detection: Human-derived performance as a precursor to\nDevelopmentBranch,2009. automaticrecognition,\u2019\u2019Proc.SPIE,vol.7112,Oct.2008,Art.no.71120V.\n[8] G.Flitton,T.P.Breckon,andN.Megherbi,\u2018\u2018Acomparisonof3Dinterest [30] I. T. Darker, P. Kuo, M. Y. Yang, A. Blechko, C. Grecos, D. Makris,\npointdescriptorswithapplicationtoairportbaggageobjectdetectionin J.-C.Nebel,andA.Gale,\u2018\u2018AutomationoftheCCTV-mediateddetectionof\ncomplexCTimagery,\u2019\u2019PatternRecognit.,vol.46,no.9,pp.2420\u20132436, individualsillegallycarryingfirearms:Combiningpsychologicalandtech-\nSep.2013. nologicalapproaches,\u2019\u2019Proc.SPIE,vol.7341,Apr.2009,Art.no.73410P.\n[9] R.Gesick,C.Saritac,andC.-C.Hung,\u2018\u2018Automaticimageanalysisprocess [31] R.Al-Rfouetal.,\u2018\u2018Theano:APythonframeworkforfastcomputationof\nforthedetectionofconcealedweapons,\u2019\u2019inProc.5thAnnu.Workshop mathematicalexpressions,\u2019\u20192016,arXiv:1605.02688.[Online].Available:\nCyberSecur.Inf.Intell.Res.CyberSecur.Inf.Intell.ChallengesStrategies http://arxiv.org/abs/1605.02688\n(CSIIRW),2009,p.20. [32] F.Chollet.(2019).Fchollet.Accessed:Apr.10,2019.[Online].Available:\n[10] R. K. Tiwari and G. K. Verma, \u2018\u2018A computer vision based framework https://github.com/fchollet\nforvisualgundetectionusingHarrisinterestpointdetector,\u2019\u2019Procedia [33] K.He,X.Zhang,S.Ren,andJ.Sun,\u2018\u2018Deepresiduallearningforimage\nComput.Sci.,vol.54,pp.703\u2013712,Aug.2015. recognition,\u2019\u2019inProc.IEEEConf.Comput.Vis.PatternRecognit.(CVPR),\n[11] R.K.TiwariandG.K.Verma,\u2018\u2018Acomputervisionbasedframeworkfor Jun.2016,pp.770\u2013778.\nvisualgundetectionusingSURF,\u2019\u2019inProc.Int.Conf.Electr.,Electron., [34] (2016). Weapon Detection in Surveillance Cameras Images.\nSignals,Commun.Optim.(EESCO),Jan.2015,pp.1\u20135. Accessed: Feb. 13, 2021. [Online]. Available: http://www.diva-\n[12] Z. Xiao, X. Lu, J. Yan, L. Wu, and L. Ren, \u2018\u2018Automatic detection of portal.org/smash/record.jsf?pid=diva2%3A1054902&dswid=-1974\nconcealedpistolsusingpassivemillimeterwaveimaging,\u2019\u2019inProc.IEEE [35] M.Nakib,R.T.Khan,M.S.Hasan,andJ.Uddin,\u2018\u2018Crimesceneprediction\nInt.Conf.Imag.Syst.Techn.(IST),Sep.2015,pp.1\u20134. by detecting threatening objects using convolutional neural network,\u2019\u2019\n[13] D. M. Sheen, D. L. Mcmakin, and T. E. Hall, \u2018\u2018Three-dimensional in Proc. Int. Conf. Comput., Commun., Chem., Mater. Electron. Eng.\nmillimeter-waveimagingforconcealedweapondetection,\u2019\u2019IEEETrans. (IC4ME2),Feb.2018,pp.1\u20134.\nMicrow.TheoryTechn.,vol.49,no.9,pp.1581\u20131592,Sep.2001.\n[36] G.K.VermaandA.Dhillon,\u2018\u2018AhandheldgundetectionusingfasterR-\n[14] Z. Xue, R. S. Blum, and Y. Li, \u2018\u2018Fusion of visual and IR images for CNNdeeplearning,\u2019\u2019inProc.7thInt.Conf.Comput.Commun.Technol.\nconcealedweapondetection,\u2019\u2019inProc.5thInt.Conf.Inf.Fusion,vol.2, (ICCCT),2017,pp.84\u201388.\nJul.2002,pp.1198\u20131205.\n[37] R.Olmos,S.Tabik,andF.Herrera,\u2018\u2018Automatichandgundetectionalarm\n[15] R. Blum, Z. Xue, Z. Liu, and D. S. Forsyth, \u2018\u2018Multisensor concealed in videos using deep learning,\u2019\u2019 Neurocomputing, vol. 275, pp.66\u201372,\nweapon detection by using a multiresolution mosaic approach,\u2019\u2019 in\nJan.2018.\nProc. IEEE 60th Veh. Technol. Conf. (VTC-Fall), vol. 7, Sep. 2004,\n[38] J. Iqbal, M. A. Munir, A. Mahmood, A. Rafaqat Ali, and M. Ali,\npp.4597\u20134601.\n\u2018\u2018Orientationawareobjectdetectionwithapplicationtofirearms,\u2019\u20192019,\n[16] E.M.UpadhyayandN.K.Rana,\u2018\u2018Exposurefusionforconcealedweapon\narXiv:1904.10032.[Online].Available:https://arxiv.org/abs/1904.10032\ndetection,\u2019\u2019 in Proc. 2nd Int. Conf. Devices, Circuits Syst. (ICDCS),\n[39] J.L.S.Gonz\u00e1lez,C.Zaccaro,J.A.\u00c1lvarez-Garc\u00eda,L.M.S.Morillo,and\nMar.2014,pp.1\u20136.\nF.S.Caparrini,\u2018\u2018Real-timegundetectioninCCTV:Anopenproblem,\u2019\u2019\n[17] R.Maher,\u2018\u2018Modelingandsignalprocessingofacousticgunshotrecord-\nNeuralNetw.,vol.132,pp.297\u2013308,Dec.2020.\nings,\u2019\u2019inProc.IEEE12thDigit.SignalProcess.Workshop4thIEEESignal\n[40] (2017). Convolutional Neural Networks. Accessed: Aug. 15, 2018.\nProcess.Educ.Workshop,Sep.2006,pp.257\u2013261.\n[Online].Available:http://cs231n.github.io/convolutional-networks/\n[18] A.Chacon-Rodriguez,P.Julian,L.Castro,P.Alvarado,andN.Hernandez,\n[41] K.SimonyanandA.Zisserman,\u2018\u2018Verydeepconvolutionalnetworksfor\n\u2018\u2018Evaluationofgunshotdetectionalgorithms,\u2019\u2019IEEETrans.CircuitsSyst.I,\nlarge-scaleimagerecognition,\u2019\u20192014,arXiv:1409.1556.[Online].Avail-\nReg.Papers,vol.58,no.2,pp.363\u2013373,Feb.2011.\nable:http://arxiv.org/abs/1409.1556\n[19] (2019). From Edison to Internet: A History of Video Surveillance.\n[42] (2019). VGG16\u2014Convolutional Network for Classification and Detec-\nAccessed: Jun. 13, 2019. [Online]. Available: https://www.\ntion. Accessed: Dec. 19, 2018. [Online]. Available: https://neurohive.\nbusiness2community.com/tech-gadgets/from-edison-to-internet-a-\nio/en/popular-networks/vgg16/\nhistory-of-video-surveillance-0578308\n[20] (2019). Infographic: History of Video Surveillance\u2014IFSEC Global | [43] C.Szegedy,V.Vanhoucke,S.Ioffe,J.Shlens,andZ.Wojna,\u2018\u2018Rethinking\nSecurityandFireNewsandResources.Accessed:Sep.15,2019.[Online]. the inception architecture for computer vision,\u2019\u2019 in Proc. IEEE Conf.\nComput.Vis.PatternRecognit.(CVPR),Jun.2016,pp.2818\u20132826.\nAvailable: https://www.ifsecglobal.com/video-surveillance/infographic-\nhistory-of-video-surveillance/ [44] C. Szegedy, S. Ioffe, V. Vanhoucke, and A. A. Alemi, \u2018\u2018Inception-v4,\n[21] W.Hu,T.Tan,L.Wang,andS.Maybank,\u2018\u2018Asurveyonvisualsurveillance inception-resnetandtheimpactofresidualconnectionsonlearning,\u2019\u2019in\nofobjectmotionandbehaviors,\u2019\u2019IEEETrans.Syst.,Man,Cybern.C,Appl. Proc.31stAAAIConf.Artif.Intell.,2017,pp.1\u20137.\nRev.,vol.34,no.3,pp.334\u2013352,Aug.2004. [45] Medium. (2019). A Simple Guide to the Versions of the\n[22] A.C.Sankaranarayanan,A.Veeraraghavan,andR.Chellappa,\u2018\u2018Object Inception Network. Accessed: Jul. 27, 2019. [Online]. Available:\ndetection, tracking and recognition for multiple smart cameras,\u2019\u2019 Proc. https://towardsdatascience.com/a-simple-guide-to-the-versions-of-the-\nIEEE,vol.96,no.10,pp.1606\u20131624,Oct.2008. inception-network-7fc52b863202\n[23] S. Zhang, C. Wang, S.-C. Chan, X. Wei, and C.-H. Ho, \u2018\u2018New object [46] D.Anguelov,D.Erhan,C.Szegedy,S.Reed,C.-Y.Fu,andA.C.Berg,\ndetection, tracking, and recognition approaches for video surveillance \u2018\u2018SSD:Singleshotmultiboxdetector,\u2019\u2019inProc.Eur.Conf.Comput.Vis.\novercameranetwork,\u2019\u2019IEEESensorsJ.,vol.15,no.5,pp.2679\u20132691, Cham,Switzerland:Springer,2016,pp.21\u201337.\nMay2015. [47] Medium. (2019). Understanding SSD MultiBox\u2014Real-Time Object\n[24] J.C.NascimentoandJ.S.Marques,\u2018\u2018Performanceevaluationofobject Detection in Deep Learning. Accessed: Aug. 19, 2019. [Online].\ndetection algorithms for video surveillance,\u2019\u2019 IEEE Trans. Multimedia, Available: https://towardsdatascience.com/understanding-ssd-multibox-\nvol.8,no.4,pp.761\u2013774,Aug.2006. real-time-object-detection-in-deep-learning-495ef744fab\n[25] N. Dalal and B. Triggs, \u2018\u2018Histograms of oriented gradients for human [48] A.G.Howard,M.Zhu,B.Chen,D.Kalenichenko,W.Wang,T.Weyand,\ndetection,\u2019\u2019Tech.Rep.,2005. M.Andreetto,andH.Adam,\u2018\u2018MobileNets:Efficientconvolutionalneu-\n[26] C. Anagnostopoulos, I. Anagnostopoulos, G. Tsekouras, G. Kouzas, ral networks for mobile vision applications,\u2019\u2019 2017, arXiv:1704.04861.\nV.Loumos,andE.Kayafas,\u2018\u2018Usingslidingconcentricwindowsforlicense [Online].Available:http://arxiv.org/abs/1704.04861\nplatesegmentationandprocessing,\u2019\u2019inProc.IEEEWorkshopSignalPro- [49] J. Redmon and A. Farhadi, \u2018\u2018YOLOv3: An incremental improve-\ncess.Syst.DesignImplement.,Nov.2005,pp.337\u2013342. ment,\u2019\u20192018,arXiv:1804.02767.[Online].Available:http://arxiv.org/abs/\n[27] M.Grega,S.Lach,andR.Sieradzki,\u2018\u2018Automatedrecognitionoffirearms 1804.02767\ninsurveillancevideo,\u2019\u2019inProc.IEEEInt.Multi-DisciplinaryConf.Cog- [50] S.Ren,K.He,R.Girshick,andJ.Sun,\u2018\u2018FasterR-CNN:Towardsreal-time\nnit.MethodsSituationAwarenessDecis.Support(CogSIMA),Feb.2013, objectdetectionwithregionproposalnetworks,\u2019\u2019inProc.Adv.NeuralInf.\npp.45\u201350. Process.Syst.,2015.pp.91\u201399.\n[28] I.Darker,A.Gale,L.Ward,andA.Blechko,\u2018\u2018CanCCTVreliablydetect [51] (2019). Faster R-CNN Explained. Accessed:\nguncrime?\u2019\u2019inProc.41stAnnu.IEEEInt.CarnahanConf.Secur.Technol., Aug. 25, 2019. [Online]. Available: https://medium.\nOct.2007,pp.264\u2013271. com/@smallfishbigsea/faster-r-cnn-explained-864d4fb7e3f\nVOLUME9,2021 34381\nM.T.Bhattietal.:WeaponDetectioninReal-TimeCCTVVideosUsingDeepLearning\n[52] Medium.(2019).FasterRCNNObjectdetection.Accessed:Aug.27,2019. MUHAMMAD GUFRAN KHAN (SeniorMem-\n[Online]. Available: https://towardsdatascience.com/faster-rcnn-object- ber,IEEE)receivedtheB.Sc.degreeinelectrical\ndetection-f865e5ed7fc4 engineering from the University of Engineering\n[53] A. Bochkovskiy, C.-Y. Wang, and H.-Y. Mark Liao, \u2018\u2018YOLOv4: Opti- and Technology, Lahore, Pakistan, in 2003, and\nmal speed and accuracy of object detection,\u2019\u2019 2020, arXiv:2004.10934. theM.Sc.degreeinelectricalengineeringspecial-\n[Online].Available:http://arxiv.org/abs/2004.10934 izationinsignalprocessingandthePh.D.degree\n[54] GeeksforGeeks. (2020). Object Detection Vs Object Recognition Vs\nin electrical engineering specialization in wire-\nImage Segmentation. Accessed: Dec. 28, 2020. [Online]. Available:\nlesscommunicationfromtheBlekingeInstituteof\nhttps://www.geeksforgeeks.org/object-detection-vs-object-recognition-\nTechnology, Sweden, in 2005 and 2011, respec-\nvs-image-segmentation/\ntively.\n[55] (2019). ImageNet. Accessed: Jun. 5, 2019. [Online]. Available: http://\nHeiscurrentlyanAssociateProfessorandtheHeadoftheDepartmentof\nwww.image-net.org/\n[56] J. O. Laguna, A. G. Olaya, and D. Borrajo, \u2018\u2018A dynamic sliding win- ElectricalEngineering,FASTNUCESChiniot-FaisalabadCampus.Before\ndow approach for activity recognition,\u2019\u2019 in Proc. Int. Conf. User Mod- joiningFAST,hehasworkedasanAnalysisEngineerwithVolvoCarCorpo-\neling, Adaptation, Personalization. Berlin, Germany: Springer, 2011, ration,Sweden.Hehasconductedresearchintheareasofsignalprocessing,\npp.219\u2013230. computer vision, and machine learning. He has also worked on different\n[57] J. Hosang, R. Benenson, P. Dollar, and B. Schiele, \u2018\u2018What makes for fundedresearchprojectsintheareaofembeddedsystemsandrobotics.He\neffectivedetectionproposals?\u2019\u2019IEEETrans.PatternAnal.Mach.Intell., isactivelyinvolvedintheapplicationsoftheAIandIoTtechnologytosolve\nvol.38,no.4,pp.814\u2013830,Apr.2016. real-world problems. He is also the Chairperson of the IEEE Faisalabad\n[58] R.Girshick,J.Donahue,T.Darrell,andJ.Malik,\u2018\u2018Richfeaturehierarchies Subsection.\nforaccurateobjectdetectionandsemanticsegmentation,\u2019\u2019inProc.IEEE\nConf.Comput.Vis.PatternRecognit.,Jun.2014,pp.580\u2013587.\n[59] A. Consulting. (2019). Selective Search for Object Detection (C++ /\nPython) | Learn OpenCV. Accessed: May 25, 2019. [Online]. Avail-\nable: https://www.learnopencv.com/selective-search-for-object-detection-\nMASOOD ASLAM received the B.S. degree in\ncpp-python/\nelectrical engineering from The University of\n[60] Y.Lecun,L.Bottou,Y.Bengio,andP.Haffner,\u2018\u2018Gradient-basedlearn-\nFaisalabad,Faisalabad,Pakistan,in2013,andthe\ning applied to document recognition,\u2019\u2019 Proc. IEEE, vol. 86, no. 11,\npp.2278\u20132324,Nov.1998. M.S.degreeinelectricalengineeringfromFAST-\n[61] S.B.Kotsiantis,D.Kanellopoulos,andP.E.Pintelas,\u2018\u2018Datapreprocessing NUCES,Islamabad,Pakistan,in2018.\nforsupervisedleaning,\u2019\u2019Int.J.Comput.Sci.,vol.1,no.2,pp.111\u2013117, HeiscurrentlyworkingasaResearchAssociate\n2006. withtheVisualComputingTechnology(VC-Tech)\nLaboratory, Islamabad. Before joining VC-Tech,\nhe worked as a Research Assistant with FAST-\nNUCES.Hisresearchinterestsincludecomputer\nMUHAMMAD TAHIR BHATTI received the visionandimageprocessing.\nB.Sc. degree in electrical and electronics engi-\nneeringfromAirUniversity,Islamabad,Pakistan,\nin2013,andtheM.S.degreeinelectricalengineer-\ningwithspecializationandresearchinthefieldof\nartificialIntelligenceandmachinelearningfrom MUHAMMADJUNAID FIAZreceivedtheB.S.\ntheNationalUniversityofComputerandEmerg- degreeincomputersciencefromGovernmentCol-\ningSciences,Faisalabad,Pakistan,in2019. legeUniversityFaisalabad,Pakistan,in2019.\nHehasworkedasanElectricalandElectronics He has worked as an Android Developer. He\nEngineerwithNationalSilkandRayanPvtLtd. is currently working as a Research Assistant in\nHe is currently working as a Research Assistant in the field of artificial thefieldofartificialintelligencewiththeNational\nintelligencewiththeNationalUniversityofComputerandEmergingSci- UniversityofComputerandEmergingSciences,\nences.HewasaCertifiedArtificialIntelligenceEngineerfromSaylaniMass Faisalabad,Pakistan.\nTrainingProgram,Faisalabad,in2018.Hereceivedthebronzemedalfrom\ntheNationalUniversityofComputerandEmergingSciences.\n34382 VOLUME9,2021",
    "metadata": {
      "CreationDate": "D:20240225183655",
      "Creator": "PDFium",
      "Producer": "PDFium"
    },
    "error_message": null,
    "pages": [
      {
        "page_number": 0,
        "text_content": "ReceivedJanuary13,2021,acceptedFebruary1,2021,dateofpublicationFebruary12,2021,dateofcurrentversionMarch4,2021.\nDigitalObjectIdentifier10.1109/ACCESS.2021.3059170\nWeapon Detection in Real-Time CCTV Videos\nUsing Deep Learning\nMUHAMMADTAHIRBHATTI 1,MUHAMMADGUFRANKHAN 1,(SeniorMember,IEEE),\nMASOODASLAM 2,ANDMUHAMMADJUNAIDFIAZ1\n1DepartmentofElectricalEngineering,NationalUniversityofComputerandEmergingSciences,Faisalabad35400,Pakistan\n2DepartmentofElectricalEngineering,ComsatsUniversityIslamabad,Islamabad45550,Pakistan\nCorrespondingauthor:MuhammadTahirBhatti(tahir.bhatti320@gmail.com)\nThisworkwassupportedbytheHigherEducationCommission(HEC)ofPakistanthroughtheTechnologyDevelopmentFund(TDF)\nunderGrantTDF-02-161.\nABSTRACT Securityandsafetyisabigconcernfortoday\u2019smodernworld.Foracountrytobeeconomically\nstrong, it must ensure a safe and secure environment for investors and tourists. Having said that, Closed\nCircuitTelevision(CCTV)camerasarebeingusedforsurveillanceandtomonitoractivitiesi.e.robberies\nbutthesecamerasstillrequirehumansupervisionandintervention.Weneedasystemthatcanautomatically\ndetect these illegal activities. Despite state-of-the-art deep learning algorithms, fast processing hardware,\nand advanced CCTV cameras, weapon detection in real-time is still a serious challenge. Observing angle\ndifferences,occlusionsbythecarrierofthefirearmandpersonsarounditfurtherenhancesthedifficultyofthe\nchallenge.ThisworkfocusesonprovidingasecureplaceusingCCTVfootageasasourcetodetectharmful\nweapons by applying the state of the art open-source deep learning algorithms. We have implemented\nbinary classification assuming pistol class as the reference class and relevant confusion objects inclusion\nconcept is introduced to reduce false positives and false negatives. No standard dataset was available for\nreal-timescenariosowemadeourowndatasetbymakingweaponphotosfromourowncamera,manually\ncollected images from internet, extracted data from YouTube CCTV videos, through GitHub repositories,\ndatabyuniversityofGranadaandInternetMoviesFirearmsDatabase(IMFDB)imfdb.org.Twoapproaches\nare used i.e. sliding window/classification and region proposal/object detection. Some of the algorithms\nusedareVGG16,Inception-V3,Inception-ResnetV2,SSDMobileNetV1,Faster-RCNNInception-ResnetV2\n(FRIRv2), YOLOv3, and YOLOv4. Precision and recall count the most rather than accuracy when object\ndetection is performed so these entire algorithms were tested in terms of them. Yolov4 stands out best\namongstallotheralgorithmsandgaveaF1-scoreof91%alongwithameanaverageprecisionof91.73%\nhigherthanpreviouslyachieved.\nINDEXTERMS Gundetection,deeplearning,objectdetection,artificialintelligence,computervision.\nI. INTRODUCTION what we speak or write has an impact on the people. Even\nThecrimerateacrosstheglobehasincreasedmainlybecause if the news they heard is crafted having no truth but as it\nofthefrequentuseofhandheldweaponsduringviolentactiv- getsviralinafewhoursbecauseofthemediaandespecially\nity. For a country to progress, the law-and-order situation socialmedia,thedamagewillbedone.Peoplenowhavemore\nmustbeincontrol.Whetherwewanttoattractinvestorsfor depression and have less control over their anger, and hate\ninvestmentortogeneraterevenuewiththetourismindustry, speeches can get those people to lose their minds. People\nalltheseneedsisapeacefulandsafeenvironment.Thecrime canbebrainwashedandpsychologicalstudiesshowthatifa\nratio because of guns is very critical in numerous parts of personhasaweaponinthissituation,hemaylosehissenses\nthe world. It includes mainly those countries in which it is andcommitaviolentactivity.\nlegaltokeepafirearm.Theworldisaglobalvillagenowand High incidents were recorded in past few years with the\nuse of harmful weapons in public areas. Starting with the\nThe associate editor coordinating the review of this manuscript and pastyear\u2019sattacksonacoupleofMosquesinNewZealand,\napprovingitforpublicationwasShadiAlawneh . on March 15, 2019 at 1:40 pm, the attacker attacks the\nThisworkislicensedunderaCreativeCommonsAttribution-NonCommercial-NoDerivatives4.0License.\n34366 Formoreinformation,seehttps://creativecommons.org/licenses/by-nc-nd/4.0/ VOLUME9,2021",
        "tables": [],
        "forms": {},
        "confidence_score": 1.0
      },
      {
        "page_number": 1,
        "text_content": "M.T.Bhattietal.:WeaponDetectioninReal-TimeCCTVVideosUsingDeepLearning\nChristchurchAL-NoorMosqueduringaFridayprayerkilling of selecting features manually, CNN automatically learns\nalmost 44 innocent and unarmed worshippers. On the same featuresfromgivendata.\nday just after 15 minutes at 1:55 PM, another attack hap- This article presents an automatic detection and classifi-\npened killing seven more civilians [1]. Active shooter inci- cationmethodofweaponsforreal-timescenariousingstate\ndents had also occurred in USA and then in Europe. The of the art deep learning models. For real-time implemen-\nmostsignificantcaseswerethoseatColumbineHighSchool tation relating the problem question of this work \u2018\u2018detect-\n(USA, 37 victims), Andreas Broeivik\u2019s assault on Uotya ingweaponsinreal-timeforpotentialrobbers/terroristusing\nIsland(Norway,179victims)ortheCharlieHebdonewspaper deep learning\u2019\u2019, detection and classification was done for\nattackkilling23.AccordingtostatsprovidedbytheUNODC, pistol, revolver and other shot handheld weapons as in sin-\namong0.1Millionpeopleofacountry,thecrimesinvolving gle class called pistol and related confusion objects such as\ngunsareveryhighi-e.1.6inBelgium,UnitedStateshaving cell phone, metal detector, wallet, selfie stick in not pistol\n4.7andMexicowithanumberof21.5[2]. class. A major reason behind this was our research done on\nCCTV cameras play an important role to overcome this weapons used in robbery cases and it further motivated us\nproblemandareconsideredtobeoneofthemostimportant to choose pistol and revolver as our target object. We go\nrequirementsforthesecurityaspect.[3].CCTVsareinstalled throughseveralCCTVcapturedrobberyvideosonYouTube\nineverypublicplacetodayandaremainlyusedforproviding and found that almost 95% of cases have pistol or revolver\nsafety, crime investigation, and other security measures for astheweaponused.Withtheimplementationofthissystem,\ndetection. CCTV footage is the most important evidence in manyrobberycrimes,andotherincidentslikewhathappened\ncourts.Afteracrimeiscommitted,lawenforcementagencies last year in New Zealand\u2019s Christchurch mosque could be\narriveatthesceneandtaketherecordingoffootagewiththem controlledusingearlyalarmsystembyalertingtheoperator\n[4].Ifwelookatthesurveillancesystemofdifferentcountries andconcernedauthoritiessoactioncanbetakenimmediately.\naroundtheworld,UKhasabout4.5millioncameras,which Gun detection in real-time is a very challenging task.\nare used for surveillance. Sweden has about 50000 cameras As our desired object has a small size so, detecting it in an\ninstalled around 2010. The government of Poland was able image is also very challenging in presence of other objects,\nto reduce drug cases by 60% and street fights by 40% by especially those objects that can be confused with it. Deep\ninstalling just 450 cameras in the city of Poznan [5]. China learning models faced several below mentioned challenges\nhastheworld\u2019sbiggestsurveillancesystemand170million fordetectionandclassificationtask:\ncamerasaroundthenation,andtheseareexpectedtoexpand\n\u2022 The first and main problem is the data through which\nthreetimes,throughanadditional400milliontobeconnected\nCNNlearnitsfeaturestobeusedlaterforclassification\nby 2020. It took only seven minutes for Chinese officials\nanddetection.\nto find and apprehend BBC reporter John Sudworth using\n\u2022 Nostandarddatasetwasavailableforweapons.\ntheir strong CCTV cameras network and facial recognition\n\u2022 For real-time scenarios, making a novel dataset manu-\ntechnologyandputthecriminalbehindthebar[6].\nallywasaverylongandtime-consumingprocess.\nIn previous years, though having surveillance cameras\n\u2022 Labelingthedesireddatabaseisnotaneasytask,asall\ninstalled, to use them for security purposes was not an easy\ndataneedstobelabeledmanually.\nand dependable method. A human has to be there all the\n\u2022 Different detection algorithms were used, so a labeled\ntimetomonitorscreens.CCTVoperatorhastomonitor20-\ndatasetforonealgorithmcannotbeutilizedfortheother\n25screensfor10hours.Hehastolook,observe,identify,and\none.\ncontrolthesituationthatcanbeharmfultotheindividualsand\n\u2022 Every algorithm requires different labeling and pre-\ntheproperty.Asthenumberofscreensincreases,theconcen-\nprocessingoperationsforthesame-labeleddatabase.\ntrationofthepersondecreasesconsiderablytomonitoreach\n\u2022 As for real-time implementation, detection systems\nscreen with time. It is impossible for the person monitoring\nrequire the exact location of the weapon so gun block-\nthescreenstokeepthesamelevelofattentionallthetime[7].\ningorocclusionisalsoaproblemthatarisesfrequently\nThe solution to aforementioned problem is to install\nanditcouldoccurbecauseofself,inter-object,orback-\nsurveillancecameraswiththeabilitytoautomaticallydetect\ngroundblocking.\nweaponsandraisealarmtoalerttheoperatorsorsecurityper-\nsonals.However,thereisnotmuchworkdoneonalgorithms Different approaches are used in this work for weapon\nfor weapon detection in surveillance cameras, and related classificationanddetectionpurposebutallhavedeeplearning\nstudies are often considering concealed weapon detection and CNN architecture behind them because oftheir state of\n(CWD), mostly using X-rays or millimeter waves images the art performance. Training from scratch took very much\nemployingtraditionalmachinelearningtechniques[8]\u2013[12]. time so the Transfer learning approach was used and Ima-\nIn the past few years, deep learning in particular convo- geNet and COCO (common objects in context) pre-trained\nlutional neural network (CNN) has given groundbreaking models are used. Different datasets were made for classifi-\nresults in object categorizing and detection. It has achieved cation and detection. For real-time purposes, we made our\nfinest results thus far in classical problems of image pro- datasetbytakingweaponphotosfromthecamera,datawas\ncessingsuchasgrouping,detectionandlocalization.Instead extractedmanuallyfromrobberyCCTVvideos,downloaded\nVOLUME9,2021 34367",
        "tables": [],
        "forms": {},
        "confidence_score": 1.0
      },
      {
        "page_number": 2,
        "text_content": "M.T.Bhattietal.:WeaponDetectioninReal-TimeCCTVVideosUsingDeepLearning\nfromimfdb(internetmoviefirearmdatabase),databyuniver- detect hidden weapons in a situation where the image of\nsityofGranadaandotheronlinerepositories.Alltheworkhas the scene was present over and under exposed area. Their\nbeendonetoachieveresultsinreal-time. methodologywastoapplyahomomorphicfiltercapturedat\nThemaincontributionsofthisworkare:presentationofa distinct exposure conditions to visual and IR pictures [16].\nfirst detailed and comprehensive work on weapon detection Current techniques attain high precision by using various\nthat can achieve detection in videos from real-time CCTV combinations of extractors and detectors, either by using\nandworkswelleveninlowresolutionandbrightnessbecause easy intensity descriptors, boundary detection, and pattern\nmost of the work done earlier is on high definition training matching[9]orbyusingmorecomplicatedtechniquessuch\nimages but realtime scenario needs realtime training data ascascadeclassifierswithboosting.\nas well for better results, finding of the most suitable and CWD though had worked for some sort of cases but it\nappropriateCNNbasedobjectdetectorfortheapplicationof had many limitations. These systems were based on metal\nweapondetectioninreal-timeCCTVvideostreams,making detection; non-metallic guns cannot be detected. They were\nofanewdatasetbecausereal-timedetectionalsoneedsreal- costly to use in many locations because they need to be\ntimetrainingdatasowemadeanewdatabaseof8327images coupledwithX-rayscannersandconveyorbeltsandresponds\nandpreprocesseditusingdifferentOpenCVfiltersi.e.Equal- to all metallic objects, so were not accurate. Economic cost\nized, Grayscale and clahe that helped in detecting images andhealthriskslimitedthepracticalimplementationofsuch\ninlowbrightnessandresolution,introducingtheconceptof methods. Furthermore, video-based firearm detection was a\nrelatedconfusionclassestoreducefalsepositivesandnega- preventivemeasureforacousticdetectionofgunshotandcan\ntives,trainingandtestingofournoveldatabaseonthelatest becombinedwithitforimplementation[17],[18].\nstate of the deep learning based classification and detection Theideaofautomatedimageprocessingforpublicsecurity\nmodelsamongthemYolov4performedbestintermsofboth purposesinmanyfieldshasbeenwellrecognizedandstudied.\nspeed and accuracy and our selected trained model predict CCTVwastheultimateneedforthiskindofworktoprogress.\nimagesatalmosteveryorientation,angle,andview,achieving CCTV was first used back in 1946 in Germany and at that\nthe highest mean average precision of 91.73% along with a time, these cameras were installed to observe the launch of\nF1-scoreof91%onYolov4. a rocket named V2 [19]. Although it had been used earlier,\nTherestofthepaperisorganizedasfollows:relatedwork majorimprovementshappenedinthelasttwodecades.With\nisdiscussedinSectionII.Theimplementationmethodology the advancement in CCTV technology, visual object recog-\nbasedondeeplearningalgorithmsisexplainedinSectionIII. nition and detection for surveillance, control, and security\nThedatasetconstruction,annotation,andpreprocessingusing were performed. In 1973, Charge-Coupled Device (CCD)\ndifferent filters have been discussed in section IV, which was developed, which made the deployment of surveillance\nfollows the experiments and results in Section V. Finally, cameras possible by 1980 [20]. If we go a bit forward in\ntheconclusionandfutureworkisdiscussedinSectionVI. time,acompanynamedAxisCommunicationdevelopedthe\nfirst-evernetworkcamera,whichenabledthetransformation\nII. RELATEDWORK of surveillance cameras from analog to digital [20]. This\nTheproblemofdetectionandclassificationofobjectsinreal- transformation of analog to digital video made it possible\ntime started after major developments in the CCTV field, for everyone to apply image processing, machine learning,\nprocessing hardware, and deep learning models. Very little and computer vision techniques on videos recorded from\nwork has been done in this field before and most of the surveillancecameras.In2003,RoyalPalmMiddleSchoolin\nprevious effort was related to concealed weapon detection Phoenixusedfacialrecognitionforthefirsttimefortracking\n(CWD). missingchildren.\nStartingwithconcealedweapondetection(CWD),before Several object detection algorithms were proposed in the\nits use in weapon detection, it was used for luggage control fieldofcomputervisiontomakesurveillancesystembetter.\nand other security purposes at airports and was based on Objectdetectionalgorithmswereusedinseveralsectorslike\nimagingtechniqueslikemillimeter-waveandinfraredimag- anomaly detection, deterrence, human detection, and traffic\ning[8].Sheenetal.suggestedCWDmethodbasedonathree- monitoring [21]. R. Chellapa et.al. discussed briefly object\ndimensional millimeter (mm) wave imaging method, for tracking and detection in surveillance cameras [22]. The\ndetectinghiddenweaponsatairportsandothersafelocations authors had explained the tracking of an object using mul-\nin the body [13]. Z. Xue et al. suggested a CWD technique tiple surveillance cameras. Another author addressed tech-\nbasedonafusion-basedtechniqueofmulti-scaledecomposi- niques for detecting objects that come into contact with\ntion,whichcombinescolorvisualpicturewithinfrared(IR) another object and are occluded. They also wrote regarding\npicture integration [14]. R. Blum et al. suggested a CWD the segmentation of mean fluctuations. They outlined how\nmethodbasedontheinclusionofvisualpictureandIRormm mean segmentation of shifts can help detect objects. They\nwave picture using a multi-resolution mosaic technique to used a Bayesian Kalman filter with a simplified Gaussian\nhighlightthehiddenweaponofthetargetpicture[15]. blend (BKF-SGM) algorithm to track the detected object\nE.M.Upadhyayet.al.suggestedaCWDtechniqueusing [23].J.SMarquesproposeddistincttechniquesforevaluating\nimage fusion. They used IR image and visual fusion to the efficiency of distinct algorithms for object recognition\n34368 VOLUME9,2021",
        "tables": [],
        "forms": {},
        "confidence_score": 1.0
      },
      {
        "page_number": 3,
        "text_content": "M.T.Bhattietal.:WeaponDetectioninReal-TimeCCTVVideosUsingDeepLearning\n[24]. B. Triggs et.al. described histogram oriented gradient learnedthedeeprepresentationofthedataandreducedalot\n(HOG).HOGbecameanovelarchitectureforfeatureextrac- ofmanualwork,andsavedtimeandenergy.\ntion. It was used mostly in applications involved in human RohithVajhalaetal.proposedthetechniqueofknifeand\ndetection [25]. In 2005, the sliding window technique was gun detection in surveillance systems. They had used HOG\nproposedfortherecognitionofnumberplates[26].Theyhad asafeatureextractoralongwithbackpropagationofartificial\nusedaslidingwindowforthepurposeofsegmentationanda neural networks for classification purposes. The detection\nneuralnetworkforcharacterrecognitiononthenumberplate. was performed using different scenarios, first weapon only\nAsdescribedabove,objectiondetectionforthecomputer and then using HOG and background subtraction methods\nvisiontaskswasusedforsomeapplicationswithbigobjects for human before the desired object and claimed to have\nto identity like a person, transport or traffic monitoring, an accuracy of 83% [34]. The aforementioned work uses\netc. Literature review on weapon detection left me with the the CNN along with non-linearity of ReLu, convolutional\nopinionthatregardlessofmanyobjectdetectionalgorithms, neurallayer,fullyconnectedlayer,anddropoutlayerofCNN\nthe algorithms proposed for weapon detection are very few. to reach a result for detection with multiple classes and\nAt last, the idea of firearm detection using the images and implemented their work using the Tensor flow open-source\nvideoswasproposedandfalsealarmswerereducedbyclas- platform. Their system achieved a test accuracy of 90.2 %\nsifying neural networks with region-based descriptors and for their dataset [35]. Micha\u0142Grega et al. proposed knives\ndeterminingregionofinterest(ROI)usingtheslidingwindow and firearm detection in CCTV images. They had applied\ntechniqueandthentrainedtheneuralnetworkclassifierwith MPEG-7 and principle component analysis along with the\nimagepixels[27]. slidingwindowapproach,whichmadetheirworkslowerfor\nWith the development in CCTV\u2019s, object detection for real-time scenarios, although they claimed to achieve good\ndifferent computer vision problems for real-time were per- accuracyontheirtestdataset.[5].\nformedandtheideatodetectfirearmswereintroducedfirst Verma et al. had also used the deep learning technique\nby L. Ward et al. in 2007 [28] and a surveillance system to detect weapons and used the Faster RCNN model. The\nwas also implemented by them a year later in 2008 [29]. work was performed on imfdb, which in my opinion is not\nIntheaforementionedwork,writerscreatedanaccuratepistol suitable to train a model for real-time case. They claimed\ndetection model for RGB pictures. However, in the same to have an accuracy of 93.1% on that dataset but in the\nscene,theirmethoddidnotdetectvariouspistols[11],[10], case of weapon detection, only achieving higher accuracy\n[29]. The approach used comprises of first removing non- is not enough, and precision and recall must the considered\nrelated items from the segmented picture using the K-mean [36]. Siham Tabik et al. work was very much related to\nclustering algorithm and then applying the SURF (Speed the real-time scenario. They used Faster RCNN to detect\nup Robust Features) method to detect points of interest. weapons in real-time using sliding window and region pro-\nDarker gave the concept of SIFT based weapon detection posalmethods.Bestresultswereobtainedbyusingtheregion\nalgorithmandforROIestimation,usedthemotionsegmen- proposaltechnique.Theslidingwindowwasalsoverytime-\ntationmethod[30].SIFTalgorithmispronetofalsealarms, consumingandtook14s/image,ontheotherhand,theregion\nso for estimating ROI, authors used motion segmentation proposal method processed the image in 140ms with 7 fps\nrather than using SIFT on complete image. When ROI was [37]. They trained the network on Faster RCNN using only\ndetermined,thenSIFTwasappliedtodetectfirearmsintheir oneclassfocusingonreducingthefalsepositive.Recentpast\ncase. objectiondetectionworkwiththeapplicationtofirearmswas\nDifferentapproachesthenusedforweapondetectionusing proposedin2019,whereagroupofresearchers,JavedIqbal\nsliding window and region proposal algorithms. HOG (His- etal.proposedorientationawaredetectionoftheobject.This\ntogram of oriented Gradient) models were used to predict system is more suitable for long and thin objects like rifles\nthe objects in the frame. HOG significant work used low- etc. The predicted bounding box in their case was aligned\nlevelfeatures,discriminativelearning,andpictorialstructure with the object and had the less unnecessary area to deal\nalongwithSVM[25],[31],[32].Thesealgorithmswereslow with. Images of very high quality were used for training\nfor real-time scenarios with 14s per image. Although these and testing purposes, which may make it less suitable for\nclassifiersgavegoodaccuracies,theslownessofthesliding real-time scenarios [38]. Jose Luis Salazar Gonz\u2019alez et al.\nwindow method was a big problem, especially for the real- work was very much related to achieve real-time results.\ntimeimplementationpurpose. They did immense experimentation using different datasets\nThisworkfocusesonthestateoftheartdeeplearningnet- and trained Faster\u2013RCNN using Feature Pyramid Network\nwork rather SIFT and HOG features which use handcrafted withResnet50andimprovesthepreviousstateoftheartby\nrules for feature extraction, selection, and detection in real- 3.91%[39].\ntime visual scenario using CCTV cameras. X. Zhang et al.\nconcluded an important finding that helped my work. They III. METHODOLOGY\nconcluded that the automatic feature representation gave Deeplearningisabranchofmachinelearninginspiredbythe\nimproved results rather than manual features [33]. Not only functionalityandstructureofthehumanbrainalsocalledan\nthelearnedfeatureswerebetterinperformance,theyalsohad artificial neural network. The methodology adopted in this\nVOLUME9,2021 34369",
        "tables": [],
        "forms": {},
        "confidence_score": 1.0
      },
      {
        "page_number": 4,
        "text_content": "M.T.Bhattietal.:WeaponDetectioninReal-TimeCCTVVideosUsingDeepLearning\nwork features the state of art deep learning, especially the\nconvolutional neural networks due to their exceptional per-\nformanceinthisfield.[40].Theaforementionedtechniques\nare used for both the classification as well as localizing the\nspecific object in a frame so both the object classification\nand detection algorithms were used and because our object\nis small with other object in background so after experi-\nmentationwefoundthebestalgorithmforourcase.Sliding\nwindow/classification and region proposal/object detection\nalgorithmswereused,andthesetechniqueswillbediscussed FIGURE1. ObjectRecognitiontodetectionHierarchy.\nlaterinthissection.\nWe had started by doing the classification using different\nthe feature extracted, it then predicts the label based on the\ndeeplearningmodelsandachievedgoodprecisionbutforthe\nprobability.\nreal-timescenarios,thelowframepersecondsofclassifica-\ntion models were the real issue in implementation. Oxford\nVGG [41], [42], Google Inceptionv3 [43] and Inception- 2) OBJECTLOCALIZATION\nResnetv2 [44], [45] were trained using the aforementioned This method outputs the actual location of an object in an\napproach. imagebygivingtheassociatedheightandwidthalongwith\nTo achieve high precision, increase number of frame itscoordinates.\nper seconds and improve localization, we moved to the\nobject detection and region proposal methods. The differ-\n3) OBJECTDETECTION\nent state of the art deep learning models for object detec-\nThis task uses the properties of the aforementioned algo-\ntion were used and the results were compared in terms of\nrithms. The detection algorithm tells us the bounding box\nprecision, speed, and standard metric of F1 score. State\nhavingxandycoordinateswithassociatedwidthandheight\nof the art deep learning based SSDMobileNetv1 [46]\u2013[48],\nalong with the class label. Non-max suppression is used to\nYOLOv3 [49], FasterRCNN-InceptionResnetv2 [50]\u2013[52],\noutputtheboxwithourdesiredthreshold[54].Thisprocess\nandYOLOv4[53]weretrainedandtested.\ngivesthefollowingresultsaltogether:\nDifferentdatasetsweremadekeepinginmindtheclassifi-\ncationanddetectionproblemasbothhaveaseparaterequire- \u2022 BoundingBox\nmentforperformingthetaskstoachievehighaccuracy,mean \u2022 Probability\naverage precision as well as frame per second for the real-\nIn past object detection was very limited because of less\ntimeimplementation.Tounderstandobjectclassificationand\ndata and low processing power of computers but with the\ndetection let us first briefly understand object recognition\npassageoftimethecomputingpowerofcomputersincreased\nas both the aforementioned types come under the umbrella\nand world moved from CPU\u2019s to Graphic Processing Units\nof this and combined classification and localization make\n(GPU). GPU\u2019s were firstly made for increasing the graphic\ndetection possible for any kind of detection problem giving\nquality of the systems and for gaming but later GPUs were\nclassnameaswellastheregionwhereourdesiredobjectis\nused extensively for deep learning. In ImageNet, competi-\nintheframe.\ntionsstartedandcontainedabout1000classes[55].Thiswas\nthe evolution of machine learning and deep learning. In the\nA. OBJECTRECOGNITION beginning,themodelswerenotverydeep,meanstherewere\nAsthenamesuggests,itistheprocessofpredictingthereal not many layers as they are now in an algorithm. Because\nclassorcategoryofanimagetowhichitbelongsbymaking oftheaforementioneddevelopments,in2012A.Krizhevsky\nprobability high only for that particular class. CNN\u2019s are presentedamodelcalledAlexNettrainedonImageNetand\nused to efficiently perform this process. Many state of the gotthefirstpositioninthatcompetition.Thiswasthebegin-\nart Classification and Detection algorithms uses CNN as a ning of object detection in deep learning. It gave a way to\nbackendtoperformtheirtasks. researchers and then every year the algorithms and models\nFig. 1 depicts that classification and localization come keeponcoming.Allthesealgorithmscontainlayersthatwork\nunderthecategoryofrecognitionandcombinedclassification ontheprincipleoftheconvolutionalneuralnetwork(CNN).\nand localization is performed to do object detection. Let us\nhaveabriefoverviewoftheobjectclassification,localization,\nB. CLASSIFICATIONANDDETECTIONAPPROACH\nanddetection.\nThere are many ways to generate region proposals, but the\nsimplestwayofgeneratingthemisbyusingtheslidingwin-\n1) IMAGECLASSIFICATION dow approach. The sliding window method is slow because\nThe classification model takes an image and slide the ker- filterslidesovertheentireframeandhaslimitations,which\nnel/filteroverthewholeimagetogetthefeaturemaps.From were tackled by the region proposal approach, so we have\n34370 VOLUME9,2021",
        "tables": [],
        "forms": {},
        "confidence_score": 1.0
      },
      {
        "page_number": 5,
        "text_content": "M.T.Bhattietal.:WeaponDetectioninReal-TimeCCTVVideosUsingDeepLearning\nthe following two approaches used in our work for both\nclassificationanddetectionmodelsare:\n\u2022 Slidingwindow/ClassificationModels\n\u2022 Regionproposal/ObjectDetectionModels\n1) SLIDINGWINDOW/CLASSIFICATIONMODELS\nIn the method to the sliding window, a box or window is\nmoved over a picture to select an area and use the object\nrecognition model to identify each frame patch covered by\nthewindow.Itisanexhaustivesearchoverthewholepicture\nforobjects.Notonlydoweneedtosearchinthepictureforall\nfeasibleplaces,wealsoneedtosearchondistinctscales.This\nis because models are usually trained on a particular range.\nTheoutcomesareintensofthousands(104)ofpicturespots\nbeing classified [56]. The sliding window method is com-\nputationally very costly because of the search with various\naspectratiosandespeciallyforeachpixelofanimageifthe\nstrideorstepvalueisless.\n2) REGIONPROPOSAL/OBJECTDETECTIONMODELS\nThistechniquetakesanimageastheboundingboxesofinput\nand output proposals related to all areas in a picture most\nprobable to be the object. These regional proposals may be\nFIGURE2. TrainingandOptimizationFlowDiagram.\nnoisy; coinciding not containing the object flawlessly, but\nthere is a proposal among these region proposals related to\npartialderivativeofcostfunctionJ(O)withamultiplierofthe\ntheoriginaltargetobject.Asthismethodtakesapictureasthe learningratealpha\u03b1 fromtheoldorpreviousweightvalue.\nboundingboxesofinputandoutputrelatedtoallpatchesina\nGradientdescentisthemainweightoptimizationalgorithm.\npicturemostprobabletobeacategory,soitproposesaregion\nIt is used as a base in all optimizers used for the modeling\nwiththemaximumscoreasthelocationofanobject.Instead\nandithelpsinconvergingthemodelandreachingtheminima\nof considering all possible regions of the input frame as\nwherewegetthebestanddesiredweightsvalues.\npossibilities,thismethodusesdetectionproposaltechniques\ntoselectregions[57].Region-basedCNNs(R-CNN)wasthe\nD. CONFUSIONOBJECTINCLUSION\nfirstdetectionmodeltointroduceCNNsunderthisapproach\nWe have formulated the problem to reduce the number\n[58].Theselectivesearchmethodofthisapproachproduces\nof false positives and negatives by adding relevant confu-\n2000boxeshavingmaximumlikelihood.\nsion object. The weapon category includes all the handheld\nSelective search is a widely used proposal generation\nweapons such as, pistol, revolver, shotgun and other than\nmethod because it is very fast having a good recall value.\nweaponincludestheobjectsthatcanmostbeconfusedwith\nItisdependentonthehierarchicalcalculationofdesiredareas\npistolclassese.g.mobile,metaldetector,selfiestick,purse,\nestablished on the compatibility of color, texture, size, and\netc.\nshape[59].\nBy understanding the differences between classification\nYolo series is among the state of the art object detection\nand detection algorithms, sliding window, and region pro-\nmodels. Unlike the other region proposal-based methods it\nposalmethods,let\u2019snowlookatthealgorithmsusedforboth\ndivides the input image into an SxS grid and then simulta-\napproaches.\nneously predicts the probability and bounding boxes for an\nobjectwithacenterfallingintoagridcell[49],[53].\nE. CLASSIFIERSANDOBJECTDETECTORS\nTheclassifiersusedundertheslidingwindowapproach:\nC. TRAININGMECHANISM\n\u2022 VGG16\nFig.2describesthegeneralmethodologyusedintrainingand\n\u2022 InceptionV3\noptimization. It starts with defining a problem, finding the\n\u2022 InceptionResnetV2\nrequireddataset,applyingpre-processingmethods,andthen\nfinally training and evaluating the dataset. If the evaluation Theobjectdetectorsusedforreal-timedetectionare:\nis correct then we save those weights as a classifier but \u2022 SSDMobilNetV1\nif it\u2019s incorrect then comes the process of backpropagation \u2022 YoloV3\nalgorithm along with the gradient descent algorithm [60]. \u2022 FasterRCNN-InceptionResNetV2\nInbackpropagation,weightsareoptimizedbysubtractingthe \u2022 YoloV4\nVOLUME9,2021 34371",
        "tables": [],
        "forms": {},
        "confidence_score": 1.0
      },
      {
        "page_number": 6,
        "text_content": "M.T.Bhattietal.:WeaponDetectioninReal-TimeCCTVVideosUsingDeepLearning\nThree databases named database1, database2, and\ndatabase3werecreatedonebyoneafterexperimentationon\ndifferent algorithms with diverse images, first for classifi-\ncation and then for object detection. Although the results\nobtained from the classification algorithms were not bad\nbut the frames per second were very slow for real-time\nimplementation. Detail for each database will be discussed\ninthenextsection.\nIV. DATASETCONSTRUCTION,ANNOTATIONAND\nPRE-PROCESSING(D-CAP)\nDataplaysakeyroleinthedevelopmentofanydeeplearning\nmodel as the model learns and extract feature from it. For\na real-time model to detect weapons with minimized pro-\ncessing time and high precision, the importance of accurate\nandrelevantdataincreasesfurtherasallotherprocessesare\ndependentonit.\nWhen we study the stats and goes through almost FIGURE3. DatasetsamplesforpistolClass-Toplefttobottomright[a-d]:\n50-60 videos of robbery on available online resources, (a)CCTVimage(b)MediumResolutionImage(c)ImagewithDark\nbackgroundandLowResolution,(d)FilteredImage.\nwe come to know that 95 percent of the videos have\nrevolver or pistol as a weapon, so we focused on binary\nclassificationwithpistolandrevolvertobeinasingleclass 3) REASONOFCHOOSINGDATACATEGORIESOFPISTOL\ncalledpistol.Besides,tomakethesystemmorepreciseandto CLASS\nreducethefalsepositiveandfalsenegativevaluesweadded Thereasonwechoosepistolandrevolverinthepistolclass\nobjectsthatcanbeconfusedwithaweaponsuchasawallet, is because of our study and analysis after watching many\ncellphone,metaldetectoretcandputtheminaseparateclass robberiesandshootingincidentCCTVvideos.Weconcluded\nnameditasnotpistol. thatalmost95%oftheweaponusedinthosecaseswereeither\nLet\u2019s now discuss the datasets used in our case because, pistolorrevolver.Fig.3showssomesampleimagesforreal-\nin a supervised learning case, the network learns the repre- timefromthecollecteddatasetofthepistolclass.\nsentation of the input data with given true answers, so the\ndatamustbeclean,preprocessed,andproperlyannotatedto\nmakethenetworklearnandpredictbetter. 4) WEAPONDATASETCATEGORIESFORNOT-PISTOLCLASS\nDatasets for this class include objects that can most likely\nbe confused with pistol class objects. Following are some\nA. DATASETCONSTRUCTIONANDSELECTION\nsamplescategoriesforthenotpistolclass:\nThe task of dataset construction and collection was very\nimportantandtoughaswellbecausetherewasnobenchmark \u2022 Wallet\ndatasetavailableforthis.Datasetforreal-timedetectionwas \u2022 MetalDetector\ncollected and constructed in different phases and data was \u2022 Cellphone\ncollected from the internet, extracted from YouTube CCTV \u2022 Selfiestick\nvideos,throughGitHubrepositories,databytheUniversityof\nGranadaresearchgroup,andinternetmoviefirearmdatabase\nimfdb.org. 5) REASONOFCHOOSINGDATACATEGORIESOF\nNOT-PISTOLCLASS\nWeintroducedthisrelevantconfusionobjectconceptbecause\n1) WEAPONDATASETCLASSES\nthese are the objects that can mostly be confused with our\nTheweapondatasetforreal-timeweapondetectionisdivided\ndesired weapon object, so predicting them correctly results\nintothefollowingtwoclasses:\ninreducingthenumberoffalsepositivesandfalsenegatives,\n\u2022 Pistol\nhenceincreasingoverallaccuracyandprecision.\n\u2022 Not-Pistol\nSome previously done work though had objects other\nthan weapons used for the background or class other than a\n2) WEAPONDATASETCATEGORIESFORPISTOLCLASS weapon but they had samples like cars, airplanes, cats, etc\nDatasetforthisclassincludesweaponsamplesofthefollow- andthereareveryfewerchancesforthemtobeconfusedwith\ningcategories: ourdesiredweapon,whichisverysmallascomparedtothem.\n\u2022 Pistol Asourdesiredobjectsofpistolclassaresmallsotherearelot\n\u2022 Revolver ofchancesforthemtobeconfusedwithsomeotherobjects\n\u2022 Othershothandheldweapons having some features like that. Fig. 4 shows some sample\n34372 VOLUME9,2021",
        "tables": [],
        "forms": {},
        "confidence_score": 1.0
      },
      {
        "page_number": 7,
        "text_content": "M.T.Bhattietal.:WeaponDetectioninReal-TimeCCTVVideosUsingDeepLearning\nTABLE1. DataDistribution.\ning the shortcomings and problems of the previous dataset.\nTheneedforthisdatasetarisesbecausethoughwegotarea-\nsonable accuracy from classification models but the frames\nper second were very few. To detect images from CCTV\nvideos,similarkindsoftrainingdatamustbeincludedsowe\nmadeourowndatasettotacklethisissue.\nThis dataset contains 8327 images divided into the pis-\ntol and not pistol class. In this case, a related confusion\nFIGURE4. Datasetsamplesfornot-pistolClass-Toplefttobottomright\ndata concept was introduced to reduce false positives and\n[a-d]:(a)CellPhone(b)MetalDetector(c)SelfieStick(d)Wallet.\nfalse negatives in real-time detection. Dataset images were\nextracted from several online sources, from CCTV videos\nimagesfromthecollecteddatasetofthenotpistolclasswhich\nfor the particular robbery scenario, made our own dataset\nhelpsinreducingfalsepositivesandnegatives.\nwith a weapon in hand for the diverse scenario, did data\naugmentation,andfinally,itwasseparatedfortestandtrain\nB. DATASETSFORREAL-TIMEDETECTION case.\nThisworkdealswiththebinaryclassificationforareal-time\nscenario so two classes were made and pistol and revolver C. DATADISTRIBUTION\nimages were included in pistol class and not pistol class Each of the aforementioned datasets are divided into the\nincludeconfusionclasseslikemobilephone,metaldetector, following categories mentioned in Table 1 with split size\nselfie stick, wallet, purse, etc. For the pistol and not pistol defining the separation percentage of the total data into test\nclasses, we have made three datasets, which are explained andtrain.\nbelow.\nD. DATAPRE-PROCESSINGANDANNOTATION\n1) DATASET1 Many things affect the performance of a Machine Learning\nThis was the initial dataset used while starting this work. (ML)modelforaspecifiedjob.First,therepresentationand\nInthisdataset,wehad1732imagesintotal,with750images qualityofthedataareessential.Iftherearemanyirrelevant\ninpistolclassand950innotpistolclass.Datasetwasdivided and redundant data existing or noisy and unreliable data,\nby the separation criteria described in Table 1 of train and then it is harder to discover representation during the train-\ntest. Images were collected from online sources and imfdb ing stage. Data preparation and filtering steps take signifi-\ndatabase and sliding window classification algorithms were cant processing time in ML issues [61]. The pre-processing\ntrainedandtestedonit. process involves data cleaning, standardization, processing,\nextraction and choice of features, etc. The final training\n2) DATASET2 datasetistheresultofpre-processingprocessesappliedtothe\nThiswastheseconddatasetmadeforthereal-timescenario. collecteddataset.\nThisdatasetcontains5254imagesandclassification,aswell Pre-processingisnecessaryforbettertrainingofamodel,\nasobjectdetectionalgorithms,weretrainedonthisdatasetto sothefirststepistomakethesamesizeorresolutionofthe\nmeet the task. Images were extracted for real-time scenario dataset.Thenextstepistoapplythemeannormalization.The\nwith the desired object in hand from online, sources, imfdb thirdstepismakingboundingboxesontheseimages,which\ndatabase,andImageNetwebsite.Datasetwasdividedbythe is also called annotation, localization, or labeling. In data,\nseparationcriteriaoftestandtrainexplainedinTable1. labeling a bounding box is made on each image. The value\nx,ycoordinates,andwidth,heightofthelabeledobjectwas\n3) DATASET3 storedinxml,csvortxtformat.Followingarethefourmain\nThis was the third dataset constructed for the real-time sce- stepsofdatapreprocessing:\nnarioand objectdetectionalgorithmswere performedonit. \u2022 Imagescaling\nThisdatabasewasmadebyenhancingdataset2byovercom- \u2022 Data-augmentation\nVOLUME9,2021 34373",
        "tables": [
          [
            [
              "",
              "",
              "",
              "",
              "",
              ""
            ],
            [
              "",
              "",
              "",
              "",
              "",
              ""
            ],
            [
              "",
              "",
              "",
              "",
              "",
              ""
            ],
            [
              "",
              "",
              "",
              "",
              "",
              ""
            ],
            [
              "",
              "",
              "",
              "",
              "",
              ""
            ],
            [
              "",
              "",
              "",
              "",
              "",
              ""
            ],
            [
              "",
              "",
              "",
              "",
              "",
              ""
            ],
            [
              "",
              "",
              "",
              "",
              "",
              ""
            ],
            [
              "",
              "",
              "",
              "",
              "",
              ""
            ]
          ]
        ],
        "forms": {},
        "confidence_score": 1.0
      },
      {
        "page_number": 8,
        "text_content": "M.T.Bhattietal.:WeaponDetectioninReal-TimeCCTVVideosUsingDeepLearning\nFIGURE5. ImageAugmentationandScaling.\nFIGURE6. ImageAnnotationandLabelling.\n\u2022 Imagelabeling\n\u2022 ImageFilteringusingOpenCV\n\u2022 RGBtoGrayscale\n\u2022 Equalized\n\u2022 Clahe\nFig. 5, 6 and 7 shows the results after applying the afore-\nmentionedpre-processingtechniques.\nV. EXPERIMENTS,RESULTSANDANALYSIS\nWehavedetectedweaponsinreal-timeCCTVstreamsinlow\nresolution,darklightwithreal-timeframepersecond.Most\noftheworkdonebeforewasondetectingimagesandvideos\nof high quality and because those models were trained on\nhigh-qualitydatasets,itisnotpossibletothendetectanobject\noflowresolutioninreal-time.Theresultsareanalyzedafter\ntrainingandtestingmodelsondatasetsmentionedinTable1.\nAs described in the methodology section the results for FIGURE7. ImageFiltrationusingOpenCVFilters-(a)OriginalImage\n(b)EqualizedFilterResult(c)GrayScaleFilterResult(d)ClaheFilter\ndifferentapproachesareevaluated.Ourmainproblemstate-\nResult.\nmentisofreal-timedetectionbecause97%ofweaponused\ninrobberycaseswerepistolorrevolver,sodifferentdataset\nresults have been evaluated here for sliding window and\nTruePositives\nregionproposalapproach. Recall= (2)\nThe performance of these models was analyzed by com- TruePositives+FalseNegatives\nparingthemintermsofthestandardmetricsofF1-scoreand\n2\u2217Precision\u2217Recall\nF1-score= (3)\nframepersecondsalongwithmeanaverageprecision(mAP) Precision+Recall\nforthebestperformedmodelandthesetermsarecalculated\nbyusingthebelowequation1,2and3.F1scoreisratioofthe\nA. DATASET-1EXPERIMENTATIONANDRESULTS\nprecisionandrecallfunctions.\nDataset 1 contains 1732 images distributed between two\nTruePositives classes of pistol and not-pistol with 750 and 982 images\nPrecision = (1)\nTruePositives+FalsePositives ineachclassrespectively.Experimentationondataset-1has\n34374 VOLUME9,2021",
        "tables": [],
        "forms": {},
        "confidence_score": 1.0
      },
      {
        "page_number": 9,
        "text_content": "M.T.Bhattietal.:WeaponDetectioninReal-TimeCCTVVideosUsingDeepLearning\nTABLE2. SlidingWindowResultsComparisonDataset-1.\nFIGURE8. Bestslidingwindowmodelaccuracygraph:InceptionResNetv2.\nbeenperformedusingtheslidingwindow/classificationmod-\nelsofVGG16,Inceptionv3andInceptionResNetv2.\nAfter experimentation, we have analyzed that the results\nobtained are not good because most of the images of this\ndatasethavewhiteorthesamekindofbackgroundwhichlead\ntoapointwherethemodelalsostartslearningthebackground\nas its region of interest (ROI) and in real-time background\nvariessoanewdatasetwasrequiredtotrainandtestthemodel\nonimageswithdiversecasesandbackground.Table2shows\nthe results for the aforementioned models using this dataset\ngivingprecision,recall,andF1-score.\nFIGURE9. BestslidingwindowmodelLossgraph:InceptionResNetv2.\nB. DATASET-2EXPERIMENTATIONANDRESULTS\nThis dataset contains two classes of pistol and not-\npistol with 3000 and 2254 images in each class respec-\ntively. Table 3 shows results based on it. Experimenta-\ntion on dataset-2 has been performed using the sliding\nwindow/classification models of VGG16, Inceptionv3, and\nInceptionResNetv2.\nExperimentationresultsshowthatthoughwegetareason-\nable accuracy from classification models using this dataset\nbut the frames per second were very few and which was a\nbigprobleminmakingareal-timeweapondetector.Among\nthese classification models, InceptionResnetV2 performed\nbest and achieves the best results. Table 3 shows the results\nunder the sliding window methods using dataset 2 and Fig.\n8, 9, and 10 shows the accuracy, loss, and confusion matrix\nFIGURE10. BestslidingwindowmodelConfusionMatrix:\nrespectivelyforthebestclassificationmodelunderthesliding InceptionResNetv2.\nwindowapproach.\nC. DATASET-3EXPERIMENTATIONANDRESULTS \u2022 FasterRCNN-InceptionResNetV2\nAfter experimentation on the previous two datasets and not \u2022 YoloV4\nfindingsatisfactoryresultsforthereal-timecaseanewdataset Each model had its pros and cons. SSD-MobileNet is\nwas made. Images were collected from robbery videos, our goodintermsofprocessingframespersecond.FasterRCNN-\nown dataset images holding a weapon in different scenar- InceptionResNetv2 has good precision and recall but not\nios, images with a dark background and low resolution, processingspeed.Yolofamilyhasaseriesofmodels.Ithasa\nand images extracted from applying different OpenCV fil- differentapproachforthedetectionpurpose.Unliketheother\nters are added to make real-time detection possible. A total regionproposalbasedmethods,itdividestheinputimageinto\nof8327imagesareusedinthiscase.Followingobjectdetec- anSxSgridandthensimultaneouslypredictstheprobability\ntionmodelsweretrainedandevaluatedusingthisdataset: andboundingboxesforanobjectwiththecenterfallingintoa\n\u2022 SSDMobilNetV1 gridcell.WehavetrainedthelateststateoftheartYolov3and\n\u2022 YoloV3 Yolov4onourownweapondataset3forreal-timedetection\nVOLUME9,2021 34375",
        "tables": [
          [
            [
              "",
              "",
              "",
              "",
              "",
              "",
              ""
            ],
            [
              "",
              "",
              "",
              "",
              "",
              "",
              ""
            ],
            [
              "",
              "",
              "",
              "",
              "",
              "",
              ""
            ],
            [
              "",
              "",
              "",
              "",
              "",
              "",
              ""
            ],
            [
              "",
              "",
              "",
              "",
              "",
              "",
              ""
            ],
            [
              "",
              "",
              "",
              "",
              "",
              "",
              ""
            ],
            [
              "",
              "",
              "",
              "",
              "",
              "",
              ""
            ],
            [
              "",
              "",
              "",
              "",
              "",
              "",
              ""
            ],
            [
              "",
              "",
              "",
              "",
              "",
              "",
              ""
            ],
            [
              "",
              "",
              "",
              "",
              "",
              "",
              ""
            ],
            [
              "",
              "",
              "",
              "",
              "",
              "",
              ""
            ],
            [
              "",
              "",
              "",
              "",
              "",
              "",
              ""
            ]
          ],
          [
            [
              "",
              ""
            ]
          ]
        ],
        "forms": {},
        "confidence_score": 1.0
      },
      {
        "page_number": 10,
        "text_content": "M.T.Bhattietal.:WeaponDetectioninReal-TimeCCTVVideosUsingDeepLearning\nTABLE3. SlidingWindowResultsComparisonDataset-2.\nTABLE4. RegionProposal/ObjectDetectionModels-Dataset-3.\nFIGURE11. BestObjectDetectionModel-Yolov4:lossvsmAP.\nTABLE5. BestPerformedModelYolov4:mAPCalculation.\nandbestresultswereobtainedthroughYOLOv4intermsof\nboth processing speed and precision. Table 4 below shows\nthe results for the aforementioned detection models for this\ndatasetatastandardthresholdscoreof50%.\nYolov4 performs best among all the models of both the\nsliding window and region proposal approach. Performance\ngraphforyolov4intermsoflossandmeanaverageprecision\n(mAP)onavalidationdatasetisshowninFig.11.Wecansee\nthat how smooth is the model loss curve and how precisely\nit converges to the best level giving a very good loss score\nof1.062andameanaverageprecisionof91.73%.Themean\naverageprecisionisthemeanoftheaverageprecisionvalues\nforalltherelevantclasses.Theassociatedvaluesofaverage\nstandard metrics of precision, recall, and F1-score for\nprecision(AP)forpistolandnot-pistolclassforthecalcula-\nevaluation.\ntionofmeanaverageprecisionvalueisgiveninTable5.\n\u2022 Some classification models showed good results but\nThe mean average precision value is calculated for the\nthey were not suitable for a real-time scenario, were\nyolov4modelasitperformsbestinallscenarioandaccurately\nslow, not much accurate, and fast as compared to the\ndetected the desired object even when the object has a very\nobjectdetectionmodelsastheyperformsverywelland\nsmall presense in the frame and there were lots of other\nachievedhighprecisionandrecall.\nobjectsinthebackgroundaswell.\n\u2022 Thereasonwhysomeclassificationmodelshaveagood\nF1-scoreisthetrainingandevaluationoninitialdatasets\nD. ANALYSISANDDISCUSSION wemadewhenstartingthiswork,butafterexperimenta-\n\u2022 Table2,3,and4aboveshowsthecomparisonbetween tion,wecometoknowthatthesemodelsarenotsuitable\nthe classification and object detection models using forreal-timescenarioshavingthebackgroundobjects.\n34376 VOLUME9,2021",
        "tables": [
          [
            [
              "",
              "",
              "",
              "",
              "",
              ""
            ],
            [
              "",
              "",
              "",
              "",
              "",
              ""
            ],
            [
              "",
              "",
              "",
              "",
              "",
              ""
            ],
            [
              "",
              "",
              "",
              "",
              "",
              ""
            ],
            [
              "",
              "",
              "",
              "",
              "",
              ""
            ],
            [
              "",
              "",
              "",
              "",
              "",
              ""
            ],
            [
              "",
              "",
              "",
              "",
              "",
              ""
            ],
            [
              "",
              "",
              "",
              "",
              "",
              ""
            ],
            [
              "",
              "",
              "",
              "",
              "",
              ""
            ],
            [
              "",
              "",
              "",
              "",
              "",
              ""
            ],
            [
              "",
              "",
              "",
              "",
              "",
              ""
            ],
            [
              "",
              "",
              "",
              "",
              "",
              ""
            ]
          ],
          [
            [
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              ""
            ],
            [
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              ""
            ],
            [
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              ""
            ],
            [
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              ""
            ],
            [
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              ""
            ],
            [
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              ""
            ],
            [
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              ""
            ],
            [
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              ""
            ],
            [
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              ""
            ],
            [
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              ""
            ],
            [
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              ""
            ],
            [
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              ""
            ],
            [
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              ""
            ],
            [
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              ""
            ],
            [
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              ""
            ]
          ],
          [
            [
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              ""
            ],
            [
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              ""
            ],
            [
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              ""
            ],
            [
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              ""
            ],
            [
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              ""
            ],
            [
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              ""
            ],
            [
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              ""
            ],
            [
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              ""
            ],
            [
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              ""
            ],
            [
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              ""
            ],
            [
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              ""
            ],
            [
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              ""
            ],
            [
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              ""
            ],
            [
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              ""
            ],
            [
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              "",
              ""
            ]
          ],
          [
            [
              ""
            ],
            [
              ""
            ],
            [
              ""
            ]
          ]
        ],
        "forms": {},
        "confidence_score": 1.0
      },
      {
        "page_number": 11,
        "text_content": "M.T.Bhattietal.:WeaponDetectioninReal-TimeCCTVVideosUsingDeepLearning\nFIGURE13. Bestperformedmodelscomparison:AccuracyvsF1-score.\nFIGURE12. ObjectDetectionmodelsPerformance/ComparisonGraph.\nTABLE6. Yolov4HyperParameters.\n\u2022 Object detection models performed well for the real-\ntime scenario and performance comparison in terms of\nspeedandF1-scorebetweenthedetectionmodelscanbe\nseen from Fig. 12. Inference results are obtained using\ntheNVIDIARTX2080tiforeachmodel.\n\u2022 Thestandardmetricsofmeanaverageprecision(mAP),\nrecallandF1-scorearecalculateandallthemodelshave\nbeencomparedatabenchmarkIoUthresholdof0.50or\n50%.\n\u2022 Yolov4 performs best amongst all models with a mean\naverage precision and F1-score of 91.73% and 91%\nrespectively with detection confidence of 99% in the\nmajorityofcases.\n\u2022 Comparison in terms of test accuracy vs F1-score\nTABLE7. Comparisonwithsomeexistingstudies.\nfor the best-performed models of both classifica-\ntion and detection approaches is shown in the\nFig.13. Accuracy and F1-score for VGG, Inceptionv3,\nInceptionResNetv2, SSDMobileNet, FasterRCNN-\nInceptionResNetv2, Yolov3 and yolov4 are 78.20%,\n85.20%, 92.20%, 79%, 96%, 94%, 99% and 81.69%,\n84.36%,85.74%,59%,87%,86%and91%respectively.\n\u2022 Fig.14-19showstheinferenceordetectionresultsofour\nmodelforpistolandnotpistolclassonimages,videos,\nandreal-timeCCTVstreams.\n\u2022 Hyperparameters used in training the best-performed\ndetectorYolov4canbeobservedfromTable6.\nItisveryhardtodoacomparisonwithstudiesconducted\npreviously on this subject because each study has its own\ndataset, models and metrics used to evaluate performance. in terms of mAP and precisoin at a standard iou threshold\nItshouldalsobenoticedachieverealtimedetection,wealso of50%,whicheverwasavailable.\nneedtohavearealtimedatasetfortraningbecausewithhigh\nqualitytrainingimageswecannotachieveresultsinrealtime. E. DETECTIONRESULTS-PISTOLCLASSWITHOUT\nEach study also has different testing conditions, either just BACKGROUND\non images, videos or on images with high quality but our SeeFigure14.\napproachfromstartwastoachieverealtimeresults.Insome\nstudies,theperformanecemetricusedisaccuray,othershave F. DETECTIONRESULTS-PISTOLCLASSWITH\nprecisoinormeanaverageprecision(mAP)butmostlymAP BACKGROUND\nis used as standard so we have given comparison results SeeFigure15.\nVOLUME9,2021 34377",
        "tables": [
          [
            [
              "",
              "",
              "",
              ""
            ],
            [
              "",
              "",
              "",
              ""
            ],
            [
              "",
              "",
              "",
              ""
            ],
            [
              "",
              "",
              "",
              ""
            ],
            [
              "",
              "",
              "",
              ""
            ],
            [
              "",
              "",
              "",
              ""
            ],
            [
              "",
              "",
              "",
              ""
            ],
            [
              "",
              "",
              "",
              ""
            ],
            [
              "",
              "",
              "",
              ""
            ],
            [
              "",
              "",
              "",
              ""
            ],
            [
              "",
              "",
              "",
              ""
            ],
            [
              "",
              "",
              "",
              ""
            ],
            [
              "",
              "",
              "",
              ""
            ],
            [
              "",
              "",
              "",
              ""
            ],
            [
              "",
              "",
              "",
              ""
            ],
            [
              "",
              "",
              "",
              ""
            ],
            [
              "",
              "",
              "",
              ""
            ]
          ],
          [
            [
              "",
              "",
              "",
              ""
            ],
            [
              "",
              "",
              "",
              ""
            ],
            [
              "",
              "",
              "",
              ""
            ],
            [
              "",
              "",
              "",
              ""
            ],
            [
              "",
              "",
              "",
              ""
            ],
            [
              "",
              "",
              "",
              ""
            ],
            [
              "",
              "",
              "",
              ""
            ],
            [
              "",
              "",
              "",
              ""
            ],
            [
              "",
              "",
              "",
              ""
            ],
            [
              "",
              "",
              "",
              ""
            ],
            [
              "",
              "",
              "",
              ""
            ]
          ]
        ],
        "forms": {},
        "confidence_score": 1.0
      },
      {
        "page_number": 12,
        "text_content": "M.T.Bhattietal.:WeaponDetectioninReal-TimeCCTVVideosUsingDeepLearning\nFIGURE14. DetectionResults-Onlyweaponinthewholeframewithoutanybackgroundatdifferentangles,brightness,sharpness,andquality.\nFIGURE15. DetectionResults-Toplefttobottomright(a-i):(a)Imagewithfrontandsideview,(b)Imageverticalview(c)ImagewithDarkbackground\nandLowResolutionfullytiltedsideview,(d)Lowbrightnessimagesideviewslightlytilted(e)Imagewiththebackview(f)Fullfrontview(g)SmallCCTV\nobject(h)Verysmallobjectwithsideview(i)Imagewithfullsideview.\nG. DETECTIONRESULTS-PISTOLCLASSINVIDEOS I. DETECTIONRESULTS\u2013NOTPISTOLCLASS\nSeeFigure16. SeeFigure18.\nH. DETECTIONRESULTS-PISTOLCLASSINREALTIME\nCCTVSTREAMS J. MISDETECTIONS\nSeeFigure17. SeeFigure19.\n34378 VOLUME9,2021",
        "tables": [],
        "forms": {},
        "confidence_score": 1.0
      },
      {
        "page_number": 13,
        "text_content": "M.T.Bhattietal.:WeaponDetectioninReal-TimeCCTVVideosUsingDeepLearning\nFIGURE16. DetectionResults-Toplefttobottomright(a-f)-video1inference(a-c),video2inference(d-f):(a)Smallobject-sideviewtilted,(b)Small\nobjectwithsideview(c)Smallobjectfrontview(d)sideview(e)Topviewdoubleobject(f)Smallobjectwithfrontandsideview.\nFIGURE17. DetectionResults-Toplefttobottomright(a-i)-cctvstream1(a-c),cctvstream2(d-f),cctvstream3(g-i):(a)SmallobjectinLowresolution(b)\nTiltedObject(c)LowResolutionverticalobject,(d)Daylightsideviewwithslightlytilted(e)Daylightsideview(f)Daylightsideviewflipped(g)Small\nobjectmediumresolution(h)verticalview(i)sideview.\nVI. CONCLUSIONANDFUTUREWORK and tourists, as security and safety are their primary needs.\nFor both monitoring and control purposes, this work has We have focused on detecting the weapon in live CCTV\npresentedanovelautomaticweapondetectionsysteminreal- streams and at the same time reduced the false negatives\ntime. This work will indeed help in improving the secu- and positives. To achieve high precision and recall we con-\nrity, law and order situation for the betterment and safety structed a new training database for the real-time scenario,\nof humanity, especially for the countries who had suffered then trained, and evaluated it on the latest state-of-the-art\na lot with these kind of violent activities. This will bring deep learning models using two approaches, i.e. sliding\na positive impact on the economy by attracting investors window/classification and region proposal/object detection.\nVOLUME9,2021 34379",
        "tables": [],
        "forms": {},
        "confidence_score": 1.0
      },
      {
        "page_number": 14,
        "text_content": "M.T.Bhattietal.:WeaponDetectioninReal-TimeCCTVVideosUsingDeepLearning\nFIGURE18. DetectionResults-Toplefttobottomright(a-d):(a)Cellphone(b)Metaldetector(c)Wallet(d)\nSelfiestick.\nFIGURE19. Misdetections:FalsepositivesandNegatives.\nDifferentalgorithmswereinvestigatedtogetgoodprecision ACKNOWLEDGMENT\nandrecall. TheauthorswishtoextendgratitudetoMr.RehanMushtaq\nThroughaseriesofexperiments,weconcludedthatobject of Ingenious Zone who provided assistance as an industrial\ndetection algorithms with ROI (Region of Interest) perform partnerinmakingthisworkpossible.\nbetter than algorithms without ROI. We have tested many\nREFERENCES\nmodels but among all of them, the state-of-the-art Yolov4,\n[1] (2019). Christchurch Mosque Shootzings. Accessed: Jul. 10, 2019.\ntrainedonournewdatabase,gaveveryfewfalsepositiveand\n[Online].Available:https://en.wikipedia.org/wiki/Christchurch_mosque_\nnegative values, hence achieved the most successful results. shootings\nItgave91.73%meanaverageprecision(mAP)andaF1-score [2] (2019). Global Study on Homicide. Accessed: Jul. 10, 2019. [Online].\nAvailable: https://www.unodc.org/unodc/en/data-and-analysis/global-\nof 91% with almost 99% confidence score on all types of\nstudy-on-homicide.html\nimagesandvideos.Wecansaythatitsatisfactorilyqualifies [3] W.Deisman,\u2018\u2018CCTV:Literaturereviewandbibliography,\u2019\u2019inResearch\nas an automatic real-time weapon detector. Looking at the and Evaluation Branch, Community, Contract and Aboriginal Policing\nServices Directorate. Ottawa, ON, Canada: Royal Canadian Mounted,\nresults,wegotthehighestmeanaverageprecision(mAP)F1-\n2003.\nscore as compared to the research done before for real-time [4] J. Ratcliffe, \u2018\u2018Video surveillance of public places,\u2019\u2019 US Dept. Justice,\nscenarios. Office Community Oriented Policing Services, Washington, DC, USA,\nTech.Rep.4,2006.\nThefutureworkincludesreducingthefalsepositivesand\n[5] M.Grega,A.Matiola\u0142ski,P.Guzik,andM.Leszczuk,\u2018\u2018Automateddetec-\nnegativesevenmoreasthereisstillaneedforimprovement. tionoffirearmsandknivesinaCCTVimage,\u2019\u2019Sensors,vol.16,no.1,\nWemightalsotrytoincreasethenumberofclassesorobjects p.47,Jan.2016.\n[6] TechCrunch. (2019). China\u2019s CCTV Surveillance Network Took Just 7\nin the future but the priority is to further improve precision\nMinutes to Capture BBC Reporter. Accessed: Jul. 15, 2019. [Online].\nandrecall. Available:https://techcrunch.com/2017/12/13/china-cctv-bbc-reporter/\n34380 VOLUME9,2021",
        "tables": [],
        "forms": {},
        "confidence_score": 1.0
      },
      {
        "page_number": 15,
        "text_content": "M.T.Bhattietal.:WeaponDetectioninReal-TimeCCTVVideosUsingDeepLearning\n[7] N. Cohen, J. Gattuso, and K. MacLennan-Brown. CCTV Operational [29] I.T.Darker,A.G.Gale,andA.Blechko,\u2018\u2018CCTVasanautomatedsensor\nRequirements Manual 2009. St Albans, U.K.: Home Office Scientific for firearms detection: Human-derived performance as a precursor to\nDevelopmentBranch,2009. automaticrecognition,\u2019\u2019Proc.SPIE,vol.7112,Oct.2008,Art.no.71120V.\n[8] G.Flitton,T.P.Breckon,andN.Megherbi,\u2018\u2018Acomparisonof3Dinterest [30] I. T. Darker, P. Kuo, M. Y. Yang, A. Blechko, C. Grecos, D. Makris,\npointdescriptorswithapplicationtoairportbaggageobjectdetectionin J.-C.Nebel,andA.Gale,\u2018\u2018AutomationoftheCCTV-mediateddetectionof\ncomplexCTimagery,\u2019\u2019PatternRecognit.,vol.46,no.9,pp.2420\u20132436, individualsillegallycarryingfirearms:Combiningpsychologicalandtech-\nSep.2013. nologicalapproaches,\u2019\u2019Proc.SPIE,vol.7341,Apr.2009,Art.no.73410P.\n[9] R.Gesick,C.Saritac,andC.-C.Hung,\u2018\u2018Automaticimageanalysisprocess [31] R.Al-Rfouetal.,\u2018\u2018Theano:APythonframeworkforfastcomputationof\nforthedetectionofconcealedweapons,\u2019\u2019inProc.5thAnnu.Workshop mathematicalexpressions,\u2019\u20192016,arXiv:1605.02688.[Online].Available:\nCyberSecur.Inf.Intell.Res.CyberSecur.Inf.Intell.ChallengesStrategies http://arxiv.org/abs/1605.02688\n(CSIIRW),2009,p.20. [32] F.Chollet.(2019).Fchollet.Accessed:Apr.10,2019.[Online].Available:\n[10] R. K. Tiwari and G. K. Verma, \u2018\u2018A computer vision based framework https://github.com/fchollet\nforvisualgundetectionusingHarrisinterestpointdetector,\u2019\u2019Procedia [33] K.He,X.Zhang,S.Ren,andJ.Sun,\u2018\u2018Deepresiduallearningforimage\nComput.Sci.,vol.54,pp.703\u2013712,Aug.2015. recognition,\u2019\u2019inProc.IEEEConf.Comput.Vis.PatternRecognit.(CVPR),\n[11] R.K.TiwariandG.K.Verma,\u2018\u2018Acomputervisionbasedframeworkfor Jun.2016,pp.770\u2013778.\nvisualgundetectionusingSURF,\u2019\u2019inProc.Int.Conf.Electr.,Electron., [34] (2016). Weapon Detection in Surveillance Cameras Images.\nSignals,Commun.Optim.(EESCO),Jan.2015,pp.1\u20135. Accessed: Feb. 13, 2021. [Online]. Available: http://www.diva-\n[12] Z. Xiao, X. Lu, J. Yan, L. Wu, and L. Ren, \u2018\u2018Automatic detection of portal.org/smash/record.jsf?pid=diva2%3A1054902&dswid=-1974\nconcealedpistolsusingpassivemillimeterwaveimaging,\u2019\u2019inProc.IEEE [35] M.Nakib,R.T.Khan,M.S.Hasan,andJ.Uddin,\u2018\u2018Crimesceneprediction\nInt.Conf.Imag.Syst.Techn.(IST),Sep.2015,pp.1\u20134. by detecting threatening objects using convolutional neural network,\u2019\u2019\n[13] D. M. Sheen, D. L. Mcmakin, and T. E. Hall, \u2018\u2018Three-dimensional in Proc. Int. Conf. Comput., Commun., Chem., Mater. Electron. Eng.\nmillimeter-waveimagingforconcealedweapondetection,\u2019\u2019IEEETrans. (IC4ME2),Feb.2018,pp.1\u20134.\nMicrow.TheoryTechn.,vol.49,no.9,pp.1581\u20131592,Sep.2001.\n[36] G.K.VermaandA.Dhillon,\u2018\u2018AhandheldgundetectionusingfasterR-\n[14] Z. Xue, R. S. Blum, and Y. Li, \u2018\u2018Fusion of visual and IR images for CNNdeeplearning,\u2019\u2019inProc.7thInt.Conf.Comput.Commun.Technol.\nconcealedweapondetection,\u2019\u2019inProc.5thInt.Conf.Inf.Fusion,vol.2, (ICCCT),2017,pp.84\u201388.\nJul.2002,pp.1198\u20131205.\n[37] R.Olmos,S.Tabik,andF.Herrera,\u2018\u2018Automatichandgundetectionalarm\n[15] R. Blum, Z. Xue, Z. Liu, and D. S. Forsyth, \u2018\u2018Multisensor concealed in videos using deep learning,\u2019\u2019 Neurocomputing, vol. 275, pp.66\u201372,\nweapon detection by using a multiresolution mosaic approach,\u2019\u2019 in\nJan.2018.\nProc. IEEE 60th Veh. Technol. Conf. (VTC-Fall), vol. 7, Sep. 2004,\n[38] J. Iqbal, M. A. Munir, A. Mahmood, A. Rafaqat Ali, and M. Ali,\npp.4597\u20134601.\n\u2018\u2018Orientationawareobjectdetectionwithapplicationtofirearms,\u2019\u20192019,\n[16] E.M.UpadhyayandN.K.Rana,\u2018\u2018Exposurefusionforconcealedweapon\narXiv:1904.10032.[Online].Available:https://arxiv.org/abs/1904.10032\ndetection,\u2019\u2019 in Proc. 2nd Int. Conf. Devices, Circuits Syst. (ICDCS),\n[39] J.L.S.Gonz\u00e1lez,C.Zaccaro,J.A.\u00c1lvarez-Garc\u00eda,L.M.S.Morillo,and\nMar.2014,pp.1\u20136.\nF.S.Caparrini,\u2018\u2018Real-timegundetectioninCCTV:Anopenproblem,\u2019\u2019\n[17] R.Maher,\u2018\u2018Modelingandsignalprocessingofacousticgunshotrecord-\nNeuralNetw.,vol.132,pp.297\u2013308,Dec.2020.\nings,\u2019\u2019inProc.IEEE12thDigit.SignalProcess.Workshop4thIEEESignal\n[40] (2017). Convolutional Neural Networks. Accessed: Aug. 15, 2018.\nProcess.Educ.Workshop,Sep.2006,pp.257\u2013261.\n[Online].Available:http://cs231n.github.io/convolutional-networks/\n[18] A.Chacon-Rodriguez,P.Julian,L.Castro,P.Alvarado,andN.Hernandez,\n[41] K.SimonyanandA.Zisserman,\u2018\u2018Verydeepconvolutionalnetworksfor\n\u2018\u2018Evaluationofgunshotdetectionalgorithms,\u2019\u2019IEEETrans.CircuitsSyst.I,\nlarge-scaleimagerecognition,\u2019\u20192014,arXiv:1409.1556.[Online].Avail-\nReg.Papers,vol.58,no.2,pp.363\u2013373,Feb.2011.\nable:http://arxiv.org/abs/1409.1556\n[19] (2019). From Edison to Internet: A History of Video Surveillance.\n[42] (2019). VGG16\u2014Convolutional Network for Classification and Detec-\nAccessed: Jun. 13, 2019. [Online]. Available: https://www.\ntion. Accessed: Dec. 19, 2018. [Online]. Available: https://neurohive.\nbusiness2community.com/tech-gadgets/from-edison-to-internet-a-\nio/en/popular-networks/vgg16/\nhistory-of-video-surveillance-0578308\n[20] (2019). Infographic: History of Video Surveillance\u2014IFSEC Global | [43] C.Szegedy,V.Vanhoucke,S.Ioffe,J.Shlens,andZ.Wojna,\u2018\u2018Rethinking\nSecurityandFireNewsandResources.Accessed:Sep.15,2019.[Online]. the inception architecture for computer vision,\u2019\u2019 in Proc. IEEE Conf.\nComput.Vis.PatternRecognit.(CVPR),Jun.2016,pp.2818\u20132826.\nAvailable: https://www.ifsecglobal.com/video-surveillance/infographic-\nhistory-of-video-surveillance/ [44] C. Szegedy, S. Ioffe, V. Vanhoucke, and A. A. Alemi, \u2018\u2018Inception-v4,\n[21] W.Hu,T.Tan,L.Wang,andS.Maybank,\u2018\u2018Asurveyonvisualsurveillance inception-resnetandtheimpactofresidualconnectionsonlearning,\u2019\u2019in\nofobjectmotionandbehaviors,\u2019\u2019IEEETrans.Syst.,Man,Cybern.C,Appl. Proc.31stAAAIConf.Artif.Intell.,2017,pp.1\u20137.\nRev.,vol.34,no.3,pp.334\u2013352,Aug.2004. [45] Medium. (2019). A Simple Guide to the Versions of the\n[22] A.C.Sankaranarayanan,A.Veeraraghavan,andR.Chellappa,\u2018\u2018Object Inception Network. Accessed: Jul. 27, 2019. [Online]. Available:\ndetection, tracking and recognition for multiple smart cameras,\u2019\u2019 Proc. https://towardsdatascience.com/a-simple-guide-to-the-versions-of-the-\nIEEE,vol.96,no.10,pp.1606\u20131624,Oct.2008. inception-network-7fc52b863202\n[23] S. Zhang, C. Wang, S.-C. Chan, X. Wei, and C.-H. Ho, \u2018\u2018New object [46] D.Anguelov,D.Erhan,C.Szegedy,S.Reed,C.-Y.Fu,andA.C.Berg,\ndetection, tracking, and recognition approaches for video surveillance \u2018\u2018SSD:Singleshotmultiboxdetector,\u2019\u2019inProc.Eur.Conf.Comput.Vis.\novercameranetwork,\u2019\u2019IEEESensorsJ.,vol.15,no.5,pp.2679\u20132691, Cham,Switzerland:Springer,2016,pp.21\u201337.\nMay2015. [47] Medium. (2019). Understanding SSD MultiBox\u2014Real-Time Object\n[24] J.C.NascimentoandJ.S.Marques,\u2018\u2018Performanceevaluationofobject Detection in Deep Learning. Accessed: Aug. 19, 2019. [Online].\ndetection algorithms for video surveillance,\u2019\u2019 IEEE Trans. Multimedia, Available: https://towardsdatascience.com/understanding-ssd-multibox-\nvol.8,no.4,pp.761\u2013774,Aug.2006. real-time-object-detection-in-deep-learning-495ef744fab\n[25] N. Dalal and B. Triggs, \u2018\u2018Histograms of oriented gradients for human [48] A.G.Howard,M.Zhu,B.Chen,D.Kalenichenko,W.Wang,T.Weyand,\ndetection,\u2019\u2019Tech.Rep.,2005. M.Andreetto,andH.Adam,\u2018\u2018MobileNets:Efficientconvolutionalneu-\n[26] C. Anagnostopoulos, I. Anagnostopoulos, G. Tsekouras, G. Kouzas, ral networks for mobile vision applications,\u2019\u2019 2017, arXiv:1704.04861.\nV.Loumos,andE.Kayafas,\u2018\u2018Usingslidingconcentricwindowsforlicense [Online].Available:http://arxiv.org/abs/1704.04861\nplatesegmentationandprocessing,\u2019\u2019inProc.IEEEWorkshopSignalPro- [49] J. Redmon and A. Farhadi, \u2018\u2018YOLOv3: An incremental improve-\ncess.Syst.DesignImplement.,Nov.2005,pp.337\u2013342. ment,\u2019\u20192018,arXiv:1804.02767.[Online].Available:http://arxiv.org/abs/\n[27] M.Grega,S.Lach,andR.Sieradzki,\u2018\u2018Automatedrecognitionoffirearms 1804.02767\ninsurveillancevideo,\u2019\u2019inProc.IEEEInt.Multi-DisciplinaryConf.Cog- [50] S.Ren,K.He,R.Girshick,andJ.Sun,\u2018\u2018FasterR-CNN:Towardsreal-time\nnit.MethodsSituationAwarenessDecis.Support(CogSIMA),Feb.2013, objectdetectionwithregionproposalnetworks,\u2019\u2019inProc.Adv.NeuralInf.\npp.45\u201350. Process.Syst.,2015.pp.91\u201399.\n[28] I.Darker,A.Gale,L.Ward,andA.Blechko,\u2018\u2018CanCCTVreliablydetect [51] (2019). Faster R-CNN Explained. Accessed:\nguncrime?\u2019\u2019inProc.41stAnnu.IEEEInt.CarnahanConf.Secur.Technol., Aug. 25, 2019. [Online]. Available: https://medium.\nOct.2007,pp.264\u2013271. com/@smallfishbigsea/faster-r-cnn-explained-864d4fb7e3f\nVOLUME9,2021 34381",
        "tables": [],
        "forms": {},
        "confidence_score": 1.0
      },
      {
        "page_number": 16,
        "text_content": "M.T.Bhattietal.:WeaponDetectioninReal-TimeCCTVVideosUsingDeepLearning\n[52] Medium.(2019).FasterRCNNObjectdetection.Accessed:Aug.27,2019. MUHAMMAD GUFRAN KHAN (SeniorMem-\n[Online]. Available: https://towardsdatascience.com/faster-rcnn-object- ber,IEEE)receivedtheB.Sc.degreeinelectrical\ndetection-f865e5ed7fc4 engineering from the University of Engineering\n[53] A. Bochkovskiy, C.-Y. Wang, and H.-Y. Mark Liao, \u2018\u2018YOLOv4: Opti- and Technology, Lahore, Pakistan, in 2003, and\nmal speed and accuracy of object detection,\u2019\u2019 2020, arXiv:2004.10934. theM.Sc.degreeinelectricalengineeringspecial-\n[Online].Available:http://arxiv.org/abs/2004.10934 izationinsignalprocessingandthePh.D.degree\n[54] GeeksforGeeks. (2020). Object Detection Vs Object Recognition Vs\nin electrical engineering specialization in wire-\nImage Segmentation. Accessed: Dec. 28, 2020. [Online]. Available:\nlesscommunicationfromtheBlekingeInstituteof\nhttps://www.geeksforgeeks.org/object-detection-vs-object-recognition-\nTechnology, Sweden, in 2005 and 2011, respec-\nvs-image-segmentation/\ntively.\n[55] (2019). ImageNet. Accessed: Jun. 5, 2019. [Online]. Available: http://\nHeiscurrentlyanAssociateProfessorandtheHeadoftheDepartmentof\nwww.image-net.org/\n[56] J. O. Laguna, A. G. Olaya, and D. Borrajo, \u2018\u2018A dynamic sliding win- ElectricalEngineering,FASTNUCESChiniot-FaisalabadCampus.Before\ndow approach for activity recognition,\u2019\u2019 in Proc. Int. Conf. User Mod- joiningFAST,hehasworkedasanAnalysisEngineerwithVolvoCarCorpo-\neling, Adaptation, Personalization. Berlin, Germany: Springer, 2011, ration,Sweden.Hehasconductedresearchintheareasofsignalprocessing,\npp.219\u2013230. computer vision, and machine learning. He has also worked on different\n[57] J. Hosang, R. Benenson, P. Dollar, and B. Schiele, \u2018\u2018What makes for fundedresearchprojectsintheareaofembeddedsystemsandrobotics.He\neffectivedetectionproposals?\u2019\u2019IEEETrans.PatternAnal.Mach.Intell., isactivelyinvolvedintheapplicationsoftheAIandIoTtechnologytosolve\nvol.38,no.4,pp.814\u2013830,Apr.2016. real-world problems. He is also the Chairperson of the IEEE Faisalabad\n[58] R.Girshick,J.Donahue,T.Darrell,andJ.Malik,\u2018\u2018Richfeaturehierarchies Subsection.\nforaccurateobjectdetectionandsemanticsegmentation,\u2019\u2019inProc.IEEE\nConf.Comput.Vis.PatternRecognit.,Jun.2014,pp.580\u2013587.\n[59] A. Consulting. (2019). Selective Search for Object Detection (C++ /\nPython) | Learn OpenCV. Accessed: May 25, 2019. [Online]. Avail-\nable: https://www.learnopencv.com/selective-search-for-object-detection-\nMASOOD ASLAM received the B.S. degree in\ncpp-python/\nelectrical engineering from The University of\n[60] Y.Lecun,L.Bottou,Y.Bengio,andP.Haffner,\u2018\u2018Gradient-basedlearn-\nFaisalabad,Faisalabad,Pakistan,in2013,andthe\ning applied to document recognition,\u2019\u2019 Proc. IEEE, vol. 86, no. 11,\npp.2278\u20132324,Nov.1998. M.S.degreeinelectricalengineeringfromFAST-\n[61] S.B.Kotsiantis,D.Kanellopoulos,andP.E.Pintelas,\u2018\u2018Datapreprocessing NUCES,Islamabad,Pakistan,in2018.\nforsupervisedleaning,\u2019\u2019Int.J.Comput.Sci.,vol.1,no.2,pp.111\u2013117, HeiscurrentlyworkingasaResearchAssociate\n2006. withtheVisualComputingTechnology(VC-Tech)\nLaboratory, Islamabad. Before joining VC-Tech,\nhe worked as a Research Assistant with FAST-\nNUCES.Hisresearchinterestsincludecomputer\nMUHAMMAD TAHIR BHATTI received the visionandimageprocessing.\nB.Sc. degree in electrical and electronics engi-\nneeringfromAirUniversity,Islamabad,Pakistan,\nin2013,andtheM.S.degreeinelectricalengineer-\ningwithspecializationandresearchinthefieldof\nartificialIntelligenceandmachinelearningfrom MUHAMMADJUNAID FIAZreceivedtheB.S.\ntheNationalUniversityofComputerandEmerg- degreeincomputersciencefromGovernmentCol-\ningSciences,Faisalabad,Pakistan,in2019. legeUniversityFaisalabad,Pakistan,in2019.\nHehasworkedasanElectricalandElectronics He has worked as an Android Developer. He\nEngineerwithNationalSilkandRayanPvtLtd. is currently working as a Research Assistant in\nHe is currently working as a Research Assistant in the field of artificial thefieldofartificialintelligencewiththeNational\nintelligencewiththeNationalUniversityofComputerandEmergingSci- UniversityofComputerandEmergingSciences,\nences.HewasaCertifiedArtificialIntelligenceEngineerfromSaylaniMass Faisalabad,Pakistan.\nTrainingProgram,Faisalabad,in2018.Hereceivedthebronzemedalfrom\ntheNationalUniversityofComputerandEmergingSciences.\n34382 VOLUME9,2021",
        "tables": [],
        "forms": {},
        "confidence_score": 1.0
      }
    ]
  },
  {
    "file_path": "pdfs\\9 th paper.pdf",
    "total_pages": 6,
    "combined_text": "e-ISSN: 2582-5208\nInternational Research Journal of Modernization in Engineering Technology and Science\n( Peer-Reviewed, Open Access, Fully Refereed International Journal )\nVolume:05/Issue:05/May-2023 Impact Factor- 7.868 www.irjmets.com\nREAL - TIME CROWD MONITORING SYSTEM\nHrishikesh Gaikwad*1, Sumit Jadhav*2, Nirbhaya Gunjal*3, Sushant Survase*4,\nProf. Kaustubh Shinde*5\n*1,2,3,4Students, Dept Of Computer Engineering, SITS, Maharashtra, India.\n*5Professor, Dept Of Computer Engineering, SITS, Maharashtra, India.\nDOI : https://www.doi.org/10.56726/IRJMETS38950\nABSTRACT\nThe use of video-based monitoring systems for crowd analysis is becoming increasingly important due to\npopulation growth and the high cost of human monitoring. This paper proposes a framework for detecting,\ntracking, and counting crowds using video-based monitoring systems. Compared to sensor-based and human-\nbased solutions, video-based systems offer more flexible functionalities, better performance, and lower costs.\nCrowd management is a crucial research area that requires attention to prevent potential losses, disasters, and\naccidents. The integration of different crowd detection and monitoring techniques can enhance performance\nand control compared to limited stand-alone techniques. Crowd management involves accessing and\ninterpreting information sources, predicting crowd behavior, and deciding on the use of interventions based on\ncontext. The paper concludes that more investigative work is needed to further advance the field of crowd\nmanagement.\nKeywords: Crowd Detection, Tracking, Image Processing.\nI. INTRODUCTION\nCrowd management has become an increasingly important research area due to the potential losses, disasters,\nand accidents that could occur if it were neglected. The rise in video-based monitoring systems has led to a\ngrowing interest in the field of computer vision. The collection of information of people passing by surveillance\ncameras has opened up new avenues for studying crowd control, detection, and tracking, based on the acquired\ndataset. This dataset can be used for various purposes, such as counting, surveying, and monitoring the\npopulation in a specific area or controlling traffic in the same area. With the aid of different methods and\nalgorithms developed for image and video processing, video sequences obtained through observation cameras\ncan be analyzed to extract the required information. Automated tools based on computers and recent\ntechnologies like laser, RFID, Wi-Fi, Bluetooth, and AI have been developed to detect and recognize crowds. The\nsecond phase of managing crowds is to monitor, track, and analyze the detected crowd to obtain reliable\ninsights. Many researchers have investigated this topic using theoretical, statistical, data mining, machine\nlearning, and prediction techniques. The detection, tracking, counting, etc. tools are tallied by advanced\nprogrammed software, making crowd management an active and flourishing research area that needs\nattention. In recent years, the need for effective crowd monitoring has increased due to the growing concerns of\nsecurity, safety, and efficiency in various public places, such as airports, railway stations, and sports arenas. The\nuse of video-based monitoring systems for crowd analysis has gained popularity as they offer more flexible\nfunctionalities, enhanced performance, and lower costs compared to sensor-based and human-based\nmonitoring systems.\nII. PROBLEM\nThe problem statement highlights the need for an automated system that can count and monitor people in\nvarious public places such as universities, shopping malls, railway stations, and airports. Traditional methods of\nmanual counting and monitoring are time-consuming and impractical, especially in areas with a high volume of\nfoot traffic. This is where an automated system that uses video surveillance can provide valuable insights. Such\na system can be developed using deep learning techniques that enable the detection and tracking of individuals\nin real-time. By analyzing the video footage, the system can count the number of people present in a specific\narea and monitor their movements.\nwww.irjmets.com @International Research Journal of Modernization in Engineering, Technology and Science\n[2807]\ne-ISSN: 2582-5208\nInternational Research Journal of Modernization in Engineering Technology and Science\n( Peer-Reviewed, Open Access, Fully Refereed International Journal )\nVolume:05/Issue:05/May-2023 Impact Factor- 7.868 www.irjmets.com\nThis information can be used for various purposes such as improving crowd control, optimizing resource\nallocation, and enhancing security measures. For instance, in a university setting, the system can monitor the\nnumber of students present in a lecture hall or a library. This information can be used to optimize the use of\nspace and resources and prevent overcrowding. Similarly, in a shopping mall or an airport, the system can help\nto monitor the movement of people and prevent the formation of queues or overcrowding in certain areas.\nOverall, an automated crowd detection and monitoring system using video surveillance and deep learning\ntechniques can provide valuable insights into the behavior and movement of people in public places. This can\nlead to more efficient resource allocation, better crowd control, and enhanced security measures.\nIII. SOLUTION\nThe proposed methodology for crowd detection and tracking involves a framework that can be used to analyze\nand count human beings in a crowd. The first step in this process is to subtract the background and remove\nunwanted pixels from the image. This method helps to identify and count people in the crowd. The database\nused to test the performance of the proposed system is also described in detail, which helps to identify the\nlimitations of the model. By using this methodology, it will be possible to develop an automated system that can\nprovide meaningful information from recorded videos, thereby reducing the need for manual monitoring and\ncounting of people in various places such as universities, shopping malls, railway stations, airports, and other\ncrowded areas. The proposed system will be able to detect and track crowds in real-time using Frame by Frame\nanalysis, advanced algorithms and image processing techniques.\nIV. PROPOSED SYSTEM\nGenerate Train set and Test set: In this this phase we first create training and testing dataset for proposed\nsystem. The basic objective of this module to generate the ground truth values for both training and testing\ndataset.\nThree different features have been extracted from each image like height, width and channel. It extracts the\nactual pixel values of each image during data creation. The outcome this process the .csv files both training and\ntesting respectively. The crowd monitoring system will detect the number of people count them and it will\nshow to count as frame count.\nPython: Python is a popular high-level programming language that is widely used in machine learning and\ncomputer vision applications, including crowd monitoring detection systems. Python's popularity in these\ndomains is mainly due to its simplicity, readability, and ease of use, as well as the availability of many open-\nsource libraries and tools that can be used for developing such systems. In the context of crowd monitoring\ndetection systems, Python can be used to perform a range of tasks, including image processing, data analysis,\nand machine learning. For example, Python libraries such as OpenCV, NumPy, and SciPy can be used for image\nprocessing tasks such as image segmentation, feature extraction, and classification. Similarly, machine learning\nwww.irjmets.com @International Research Journal of Modernization in Engineering, Technology and Science\n[2808]\ne-ISSN: 2582-5208\nInternational Research Journal of Modernization in Engineering Technology and Science\n( Peer-Reviewed, Open Access, Fully Refereed International Journal )\nVolume:05/Issue:05/May-2023 Impact Factor- 7.868 www.irjmets.com\nlibraries such as TensorFlow, Keras, and PyTorch can be used for training and deploying machine learning\nmodels, for crowd detection and analysis.\nCNN (Training and Testing): Convolutional Neural Networks (CNNs) are a type of deep learning algorithm\nthat is particularly well-suited for image processing tasks, such as crowd detection. CNNs are designed to learn\nfeatures from images in a hierarchical manner, where lower layers learn simple features, such as edges and\ncorners, and higher layers learn more complex features, such as shapes and objects. In the context of crowd\ndetection, CNNs can be trained on a large dataset of images and videos, such as aerial photographs or security\ncamera footage, to learn how to identify patterns and features that are indicative of a crowd. The CNN\nalgorithm works by processing the input image through a series of convolutional layers, pooling layers, and\nfully connected layers. The convolutional layers use a set of filters to scan the input image and extract features\nthat are relevant to the task of crowd detection. These filters learn to identify patterns and edges in the image,\nsuch as the shapes of individuals or groups of individuals in the crowd. The pooling layers down sample the\noutput of the convolutional layers by selecting the most relevant features and reducing the resolution of the\nimage. This helps to reduce the computational complexity of the algorithm and prevent overfitting to the\ntraining data. The fully connected layers take the output of the convolutional and pooling layers and use it to\nclassify the input image as containing a crowd or not. This is done by mapping the extracted features to a set of\noutput classes, such as \"crowd\" or \"no crowd,\" using a set of learned weights. To train the CNN algorithm for\ncrowd detection, a large dataset of labeled images is required. The images in the dataset are first preprocessed\nto normalize the pixel values and apply data augmentation techniques to increase the size of the dataset. The\nCNN is then trained using a loss function, such as binary cross-entropy, which measures the difference between\nthe predicted output and the true label. Once the CNN is trained, it can be used to detect crowds in new images\nby applying the same convolutional filters and fully connected layers to the input image. The output of the CNN\nis a probability score indicating the likelihood that the input image contains a crowd. Overall, CNNs are a\npowerful tool for crowd detection systems, as they can learn to extract useful features from images and classify\nthem with high accuracy. By training CNNs on large datasets of labeled images, developers can create robust\nand effective crowd detection systems that can be used for various applications, such as crowd control, security,\nand event planning.\nTensorFlow Library Module: In the first module we implement the access interfaces and should be\ncustomized for every deep learning tool called TensorFlow. With the help of this APIs often need to be\ncompatible with application\u2019s source code.\nDeep Learning: Deep learning is a subset of machine learning that uses neural networks with multiple layers\nto learn representations of data. It has been shown to be highly effective in computer vision tasks, such as\nimage classification, object detection, and segmentation, making it a useful tool for crowd detection systems.\nDeep learning algorithm, such as Convolutional Neural Networks (CNNs), can be trained on large datasets of\ncrowd images to learn how to identify patterns and features that are indicative of a crowd. These algorithms\ncan extract useful information from images, such as crowd size, density, and movement patterns, which can be\nused for various applications, such as crowd control, security, and event planning. In the context of crowd\ndetection, deep learning algorithms can be used to perform a range of tasks, including crowd counting, crowd\nsegmentation, and crowd behavior analysis. For example, a CNN can be trained to detect the presence of crowds\nin images by learning to identify patterns of individuals and their distribution across the image. Similarly, Deep\nlearning algorithms can be trained using a supervised or unsupervised learning approach. In supervised\nlearning, the algorithm is trained on a labeled dataset of crowd images, where each image is labeled with the\nnumber of individuals in the image or whether it contains a crowd or not. In unsupervised learning, the\nalgorithm is trained on an unlabeled dataset of crowd images and is tasked with learning to identify patterns\nand features in the data without any explicit guidance.\nwww.irjmets.com @International Research Journal of Modernization in Engineering, Technology and Science\n[2809]\ne-ISSN: 2582-5208\nInternational Research Journal of Modernization in Engineering Technology and Science\n( Peer-Reviewed, Open Access, Fully Refereed International Journal )\nVolume:05/Issue:05/May-2023 Impact Factor- 7.868 www.irjmets.com\nV. CROWD DETECTION\nA crowd detection system is a system designed to detect and monitor the presence and movement of crowds in\nreal-time using computer vision and machine learning techniques. These systems are used in a variety of\napplications, including public safety, event management, and transportation. The main purpose of a crowd\ndetection system is to provide real-time insights into crowd behavior and movement that can help prevent and\nrespond to potential crowd-related incidents such as overcrowding, stampedes, and unauthorized gatherings.\nThese systems use various techniques such as object detection, segmentation, and tracking to identify\nindividuals and groups within the crowd and estimate crowd density. A typical crowd detection system consists\nof multiple cameras and sensors placed strategically in public spaces to capture visual and environmental data.\nThe data is then processed using algorithms and techniques such as convolutional neural networks (CNNs) to\nextract features from images and classify them with high accuracy.\nImage Processing: Public detection using image processing frame by frame and using gray scale conversion is\na commonly used technique in crowd detection. The basic idea is to convert the video frames to grayscale,\nwhich helps in reducing the amount of information that needs to be processed. Grayscale images only contain\nluminance information, which makes them easier to work with than full-color images. Once the video frames\nare converted to grayscale, image processing techniques can be used to detect and track people in the scene.\nOne approach is to use a technique called background subtraction, which involves subtracting the current\nframe from a background model to detect moving objects in the scene. The basic idea behind image processing\nframe by frame is to analyze each frame of the video separately, and extract features that can be used to identify\nand track people in the scene. These features can include properties such as color, texture, and shape, which\ncan be used to distinguish people from the background and other objects in the scene. One common technique\nused in frame-by-frame image processing for crowd detection is background subtraction. This involves creating\na model of the scene's background, and then subtracting the current frame from this model to detect moving\nobjects in the scene. By comparing each frame to the background model, it is possible to detect and track people\nas they move through the scene. Another technique used in frame-by-frame image processing for crowd\ndetection is blob detection. This involves identifying regions of pixels in the image that have similar properties,\nsuch as color or intensity, and grouping them together into blobs. By analyzing the properties of these blobs, it\nis possible to identify and track individual people in the scene.\nwww.irjmets.com @International Research Journal of Modernization in Engineering, Technology and Science\n[2810]\ne-ISSN: 2582-5208\nInternational Research Journal of Modernization in Engineering Technology and Science\n( Peer-Reviewed, Open Access, Fully Refereed International Journal )\nVolume:05/Issue:05/May-2023 Impact Factor- 7.868 www.irjmets.com\nFeature Extraction: Feature extraction is a fundamental step in computer vision and machine learning, and it\ninvolves identifying and extracting important information or features from raw images that can be used for\nfurther analysis, such as classification, detection, or segmentation. These features can be statistical or\nstructural, and can capture information such as texture, shape, or color.\nThe process of feature extraction can be broken down into several elements, including:\nPreprocessing: The first step in feature extraction is often preprocessing, which involves preparing the image\nfor analysis. This can include steps such as resizing, cropping, or color normalization, which help to improve the\nquality and consistency of the image.\nFeature Selection: Once the image is preprocessed, the next step is to select the most relevant features that\ncan help to distinguish between different objects or classes. This can be done manually, by identifying\nimportant visual cues or properties that are specific to the problem at hand, or automatically, using machine\nlearning techniques such as principal component analysis (PCA) or linear discriminant analysis (LDA).\nFeature Representation: After the relevant features have been selected, the next step is to represent them in a\nsuitable format that can be used for analysis. This can involve transforming the raw image data into a more\ncompact or meaningful representation, such as a histogram of color values, a texture descriptor, or a shape\nmodel.\nFeature Extraction: Once the features have been selected and represented, the final step is to extract them\nfrom the image. This can involve applying a set of operations or filters to the image data, such as convolution or\nwavelet transforms, to identify and extract the relevant features.\nFeature Normalization: In some cases, it may also be necessary to normalize the extracted features to\nimprove their consistency and robustness. This can involve techniques such as z-score normalization or min-\nmax normalization, which help to ensure that the features are scaled and centered appropriately.\nImage Segmentation: Image segmentation is a process of dividing an image into multiple segments or regions,\neach of which corresponds to a different object or part of the image. Image segmentation plays a critical role in\ncomputer vision applications, such as object detection, tracking, and recognition. The goal of image\nsegmentation is to partition an image into meaningful regions that can be analyzed and processed\nindependently. This is typically achieved by applying a set of image processing techniques to identify regions\nthat share similar visual characteristics, such as color, texture, or intensity.\nThere are several techniques for image segmentation, including:\nThresholding: This technique involves selecting a threshold value and partitioning the image into regions\nbased on the intensity values of each pixel. Pixels with intensity values above the threshold are assigned to one\nregion, while pixels with intensity values below the threshold are assigned to another region.\nEdge Detection: This technique involves detecting the edges or boundaries between different regions in an\nimage. This can be done using techniques such as the Canny edge detector or the Sobel edge detector.\nRegion Growing: This technique involves starting with a seed pixel and iteratively adding neighboring pixels\nto the region based on some similarity criterion. This process continues until all pixels in the region have been\nadded.\nClustering: This technique involves grouping pixels into clusters based on their visual similarity. This can be\ndone using algorithms such as k-means clustering or hierarchical clustering.\nClassification: In a crowd detection system, classification of images typically involves identifying and\ncategorizing different objects or groups within the crowd. This process can be performed using various\ntechniques, including traditional computer vision methods and deep learning-based approaches. In traditional\ncomputer vision methods, feature extraction algorithms are used to identify and extract relevant features from\nthe images, such as edges, corners, and texture patterns. These features are then used to train a classifier, such\nas a support vector machine (SVM) or k-nearest neighbor (KNN), to recognize different objects or groups\nwithin the crowd, such as individuals or clusters. In deep learning-based approaches, Convolutional Neural\nNetworks (CNNs) are commonly used for image classification tasks. CNNs are a type of neural network that are\nwww.irjmets.com @International Research Journal of Modernization in Engineering, Technology and Science\n[2811]\ne-ISSN: 2582-5208\nInternational Research Journal of Modernization in Engineering Technology and Science\n( Peer-Reviewed, Open Access, Fully Refereed International Journal )\nVolume:05/Issue:05/May-2023 Impact Factor- 7.868 www.irjmets.com\nspecifically designed for image processing tasks, as they can automatically learn and extract relevant features\nfrom images by using multiple layers of convolutional filters.\nThe process of image classification in a crowd detection system using CNNs typically involves the following\nsteps:\nData Preparation: Collecting and preparing a large dataset of labeled images for training the model.\nTraining the Model: Using the prepared dataset to train the CNN model to recognize different objects or\ngroups within the crowd.\nValidation: Testing the trained model on a separate dataset of images to evaluate its accuracy and\nperformance.\nDeployment: Using the trained model to classify new images in real-time.\nDuring the training process, the CNN model learns to recognize patterns and features in the crowd images by\nadjusting the weights of the neural network connections based on the errors between the predicted and actual\nclass labels. Once the model is trained and validated, it can be used to classify new crowd images by feeding\nthem into the model and predicting the corresponding class labels based on the learned features and patterns.\nVI. CONCLUSION\nIn conclusion, the proposed CNN-based method provides a promising approach for crowd detection and\ncounting in still images from various scenes. The use of features derived from a CNN model trained for other\ncomputer vision tasks enables accurate representation of crowd density. The neighboring local counts are also\nstrongly correlated, and feature extraction techniques contribute to good detection accuracy. The system uses\nthe RESNET deep convolutional network, which provides up to 152 hidden layers, and can be extended with\nmultiple convolutional layers with an ensemble deep learning model for even higher accuracy. Experimental\nfindings demonstrate that the proposed method outperforms other recent related methods and can be\nextended to work on image and video datasets. Overall, this system offers better accuracy for crowd detection\nfrom heterogeneous images, making it a valuable tool for various applications.\nVII. REFERENCES\n[1] Crowd Detection And Tracking In Surveillance Video Sequences Sohail Salim; Othman O Khalifa; Farah\nAbdul Rahman; Adidah Lajis 2019 IEEE International Conference on Smart Instrumentation,\nMeasurement and Application (ICSIMA)\n[2] Multi-UAV Based Crowd Monitoring System Rodrigo Saar de Moraes; Edison Pignaton de Freitas IEEE\nTransactions on Aerospace and Electronic Systems\n[3] In-Depth Survey to Detect, Monitor and Manage Crowd Ali M. Al-Shaery; Shroug S. Alshehri; Norah S.\nFarooqi; Mohamed O. Khozium IEEE Access\n[4] Analysis and Design of Public Places Crowd Stampede Early-Warning Simulating System Shangnan Liu;\nZhenjiang Zhu; Qiang Cheng; Hao Zhang 2016 International Conference on Industrial Informatics -\nComputing Technology, Intelligent Technology, Industrial Information Integration (ICIICII)\n[5] Crowd Monitoring Using Mobile Phones Yaoxuan Yuan 2014 Sixth International Conference on\nIntelligent Human-Machine Systems and Cybernetics\n[6] A Real-Time Crowd Detection and Monitoring System using Machine Learning Pooja Shrivastav; Vakula\nRani J 2023 International Conference on Intelligent Data Communication Technologies and Internet of\nThings (IDCIoT)\n[7] Crowd detection and management using cascade classifier on ARMv8 and OpenCV Python S. Syed\nAmeer Abbas; P. Oliver Jayaprakash; M. Anitha; X. Vinitha Jaini 2017 International Conference on\nInnovations iasdfghjkl;n Information, Embedded and Communication Systems (ICIIECS)\n[8] Multi-Person Tracking in Smart Surveillance System for Crowd Counting and Normal/Abnormal Events\nDetection Ahsan Shehzed; Ahmad Jalal; Kibum Kim 2019 International Conference on Applied and\nEngineering Mathematics (ICAEM).\nwww.irjmets.com @International Research Journal of Modernization in Engineering, Technology and Science\n[2812]",
    "metadata": {
      "Author": "xyz",
      "Creator": "Microsoft\u00ae Word 2010",
      "CreationDate": "D:20230514103146+05'30'",
      "ModDate": "D:20230514103146+05'30'",
      "Producer": "Microsoft\u00ae Word 2010"
    },
    "error_message": null,
    "pages": [
      {
        "page_number": 0,
        "text_content": "e-ISSN: 2582-5208\nInternational Research Journal of Modernization in Engineering Technology and Science\n( Peer-Reviewed, Open Access, Fully Refereed International Journal )\nVolume:05/Issue:05/May-2023 Impact Factor- 7.868 www.irjmets.com\nREAL - TIME CROWD MONITORING SYSTEM\nHrishikesh Gaikwad*1, Sumit Jadhav*2, Nirbhaya Gunjal*3, Sushant Survase*4,\nProf. Kaustubh Shinde*5\n*1,2,3,4Students, Dept Of Computer Engineering, SITS, Maharashtra, India.\n*5Professor, Dept Of Computer Engineering, SITS, Maharashtra, India.\nDOI : https://www.doi.org/10.56726/IRJMETS38950\nABSTRACT\nThe use of video-based monitoring systems for crowd analysis is becoming increasingly important due to\npopulation growth and the high cost of human monitoring. This paper proposes a framework for detecting,\ntracking, and counting crowds using video-based monitoring systems. Compared to sensor-based and human-\nbased solutions, video-based systems offer more flexible functionalities, better performance, and lower costs.\nCrowd management is a crucial research area that requires attention to prevent potential losses, disasters, and\naccidents. The integration of different crowd detection and monitoring techniques can enhance performance\nand control compared to limited stand-alone techniques. Crowd management involves accessing and\ninterpreting information sources, predicting crowd behavior, and deciding on the use of interventions based on\ncontext. The paper concludes that more investigative work is needed to further advance the field of crowd\nmanagement.\nKeywords: Crowd Detection, Tracking, Image Processing.\nI. INTRODUCTION\nCrowd management has become an increasingly important research area due to the potential losses, disasters,\nand accidents that could occur if it were neglected. The rise in video-based monitoring systems has led to a\ngrowing interest in the field of computer vision. The collection of information of people passing by surveillance\ncameras has opened up new avenues for studying crowd control, detection, and tracking, based on the acquired\ndataset. This dataset can be used for various purposes, such as counting, surveying, and monitoring the\npopulation in a specific area or controlling traffic in the same area. With the aid of different methods and\nalgorithms developed for image and video processing, video sequences obtained through observation cameras\ncan be analyzed to extract the required information. Automated tools based on computers and recent\ntechnologies like laser, RFID, Wi-Fi, Bluetooth, and AI have been developed to detect and recognize crowds. The\nsecond phase of managing crowds is to monitor, track, and analyze the detected crowd to obtain reliable\ninsights. Many researchers have investigated this topic using theoretical, statistical, data mining, machine\nlearning, and prediction techniques. The detection, tracking, counting, etc. tools are tallied by advanced\nprogrammed software, making crowd management an active and flourishing research area that needs\nattention. In recent years, the need for effective crowd monitoring has increased due to the growing concerns of\nsecurity, safety, and efficiency in various public places, such as airports, railway stations, and sports arenas. The\nuse of video-based monitoring systems for crowd analysis has gained popularity as they offer more flexible\nfunctionalities, enhanced performance, and lower costs compared to sensor-based and human-based\nmonitoring systems.\nII. PROBLEM\nThe problem statement highlights the need for an automated system that can count and monitor people in\nvarious public places such as universities, shopping malls, railway stations, and airports. Traditional methods of\nmanual counting and monitoring are time-consuming and impractical, especially in areas with a high volume of\nfoot traffic. This is where an automated system that uses video surveillance can provide valuable insights. Such\na system can be developed using deep learning techniques that enable the detection and tracking of individuals\nin real-time. By analyzing the video footage, the system can count the number of people present in a specific\narea and monitor their movements.\nwww.irjmets.com @International Research Journal of Modernization in Engineering, Technology and Science\n[2807]",
        "tables": [],
        "forms": {},
        "confidence_score": 1.0
      },
      {
        "page_number": 1,
        "text_content": "e-ISSN: 2582-5208\nInternational Research Journal of Modernization in Engineering Technology and Science\n( Peer-Reviewed, Open Access, Fully Refereed International Journal )\nVolume:05/Issue:05/May-2023 Impact Factor- 7.868 www.irjmets.com\nThis information can be used for various purposes such as improving crowd control, optimizing resource\nallocation, and enhancing security measures. For instance, in a university setting, the system can monitor the\nnumber of students present in a lecture hall or a library. This information can be used to optimize the use of\nspace and resources and prevent overcrowding. Similarly, in a shopping mall or an airport, the system can help\nto monitor the movement of people and prevent the formation of queues or overcrowding in certain areas.\nOverall, an automated crowd detection and monitoring system using video surveillance and deep learning\ntechniques can provide valuable insights into the behavior and movement of people in public places. This can\nlead to more efficient resource allocation, better crowd control, and enhanced security measures.\nIII. SOLUTION\nThe proposed methodology for crowd detection and tracking involves a framework that can be used to analyze\nand count human beings in a crowd. The first step in this process is to subtract the background and remove\nunwanted pixels from the image. This method helps to identify and count people in the crowd. The database\nused to test the performance of the proposed system is also described in detail, which helps to identify the\nlimitations of the model. By using this methodology, it will be possible to develop an automated system that can\nprovide meaningful information from recorded videos, thereby reducing the need for manual monitoring and\ncounting of people in various places such as universities, shopping malls, railway stations, airports, and other\ncrowded areas. The proposed system will be able to detect and track crowds in real-time using Frame by Frame\nanalysis, advanced algorithms and image processing techniques.\nIV. PROPOSED SYSTEM\nGenerate Train set and Test set: In this this phase we first create training and testing dataset for proposed\nsystem. The basic objective of this module to generate the ground truth values for both training and testing\ndataset.\nThree different features have been extracted from each image like height, width and channel. It extracts the\nactual pixel values of each image during data creation. The outcome this process the .csv files both training and\ntesting respectively. The crowd monitoring system will detect the number of people count them and it will\nshow to count as frame count.\nPython: Python is a popular high-level programming language that is widely used in machine learning and\ncomputer vision applications, including crowd monitoring detection systems. Python's popularity in these\ndomains is mainly due to its simplicity, readability, and ease of use, as well as the availability of many open-\nsource libraries and tools that can be used for developing such systems. In the context of crowd monitoring\ndetection systems, Python can be used to perform a range of tasks, including image processing, data analysis,\nand machine learning. For example, Python libraries such as OpenCV, NumPy, and SciPy can be used for image\nprocessing tasks such as image segmentation, feature extraction, and classification. Similarly, machine learning\nwww.irjmets.com @International Research Journal of Modernization in Engineering, Technology and Science\n[2808]",
        "tables": [],
        "forms": {},
        "confidence_score": 1.0
      },
      {
        "page_number": 2,
        "text_content": "e-ISSN: 2582-5208\nInternational Research Journal of Modernization in Engineering Technology and Science\n( Peer-Reviewed, Open Access, Fully Refereed International Journal )\nVolume:05/Issue:05/May-2023 Impact Factor- 7.868 www.irjmets.com\nlibraries such as TensorFlow, Keras, and PyTorch can be used for training and deploying machine learning\nmodels, for crowd detection and analysis.\nCNN (Training and Testing): Convolutional Neural Networks (CNNs) are a type of deep learning algorithm\nthat is particularly well-suited for image processing tasks, such as crowd detection. CNNs are designed to learn\nfeatures from images in a hierarchical manner, where lower layers learn simple features, such as edges and\ncorners, and higher layers learn more complex features, such as shapes and objects. In the context of crowd\ndetection, CNNs can be trained on a large dataset of images and videos, such as aerial photographs or security\ncamera footage, to learn how to identify patterns and features that are indicative of a crowd. The CNN\nalgorithm works by processing the input image through a series of convolutional layers, pooling layers, and\nfully connected layers. The convolutional layers use a set of filters to scan the input image and extract features\nthat are relevant to the task of crowd detection. These filters learn to identify patterns and edges in the image,\nsuch as the shapes of individuals or groups of individuals in the crowd. The pooling layers down sample the\noutput of the convolutional layers by selecting the most relevant features and reducing the resolution of the\nimage. This helps to reduce the computational complexity of the algorithm and prevent overfitting to the\ntraining data. The fully connected layers take the output of the convolutional and pooling layers and use it to\nclassify the input image as containing a crowd or not. This is done by mapping the extracted features to a set of\noutput classes, such as \"crowd\" or \"no crowd,\" using a set of learned weights. To train the CNN algorithm for\ncrowd detection, a large dataset of labeled images is required. The images in the dataset are first preprocessed\nto normalize the pixel values and apply data augmentation techniques to increase the size of the dataset. The\nCNN is then trained using a loss function, such as binary cross-entropy, which measures the difference between\nthe predicted output and the true label. Once the CNN is trained, it can be used to detect crowds in new images\nby applying the same convolutional filters and fully connected layers to the input image. The output of the CNN\nis a probability score indicating the likelihood that the input image contains a crowd. Overall, CNNs are a\npowerful tool for crowd detection systems, as they can learn to extract useful features from images and classify\nthem with high accuracy. By training CNNs on large datasets of labeled images, developers can create robust\nand effective crowd detection systems that can be used for various applications, such as crowd control, security,\nand event planning.\nTensorFlow Library Module: In the first module we implement the access interfaces and should be\ncustomized for every deep learning tool called TensorFlow. With the help of this APIs often need to be\ncompatible with application\u2019s source code.\nDeep Learning: Deep learning is a subset of machine learning that uses neural networks with multiple layers\nto learn representations of data. It has been shown to be highly effective in computer vision tasks, such as\nimage classification, object detection, and segmentation, making it a useful tool for crowd detection systems.\nDeep learning algorithm, such as Convolutional Neural Networks (CNNs), can be trained on large datasets of\ncrowd images to learn how to identify patterns and features that are indicative of a crowd. These algorithms\ncan extract useful information from images, such as crowd size, density, and movement patterns, which can be\nused for various applications, such as crowd control, security, and event planning. In the context of crowd\ndetection, deep learning algorithms can be used to perform a range of tasks, including crowd counting, crowd\nsegmentation, and crowd behavior analysis. For example, a CNN can be trained to detect the presence of crowds\nin images by learning to identify patterns of individuals and their distribution across the image. Similarly, Deep\nlearning algorithms can be trained using a supervised or unsupervised learning approach. In supervised\nlearning, the algorithm is trained on a labeled dataset of crowd images, where each image is labeled with the\nnumber of individuals in the image or whether it contains a crowd or not. In unsupervised learning, the\nalgorithm is trained on an unlabeled dataset of crowd images and is tasked with learning to identify patterns\nand features in the data without any explicit guidance.\nwww.irjmets.com @International Research Journal of Modernization in Engineering, Technology and Science\n[2809]",
        "tables": [],
        "forms": {},
        "confidence_score": 1.0
      },
      {
        "page_number": 3,
        "text_content": "e-ISSN: 2582-5208\nInternational Research Journal of Modernization in Engineering Technology and Science\n( Peer-Reviewed, Open Access, Fully Refereed International Journal )\nVolume:05/Issue:05/May-2023 Impact Factor- 7.868 www.irjmets.com\nV. CROWD DETECTION\nA crowd detection system is a system designed to detect and monitor the presence and movement of crowds in\nreal-time using computer vision and machine learning techniques. These systems are used in a variety of\napplications, including public safety, event management, and transportation. The main purpose of a crowd\ndetection system is to provide real-time insights into crowd behavior and movement that can help prevent and\nrespond to potential crowd-related incidents such as overcrowding, stampedes, and unauthorized gatherings.\nThese systems use various techniques such as object detection, segmentation, and tracking to identify\nindividuals and groups within the crowd and estimate crowd density. A typical crowd detection system consists\nof multiple cameras and sensors placed strategically in public spaces to capture visual and environmental data.\nThe data is then processed using algorithms and techniques such as convolutional neural networks (CNNs) to\nextract features from images and classify them with high accuracy.\nImage Processing: Public detection using image processing frame by frame and using gray scale conversion is\na commonly used technique in crowd detection. The basic idea is to convert the video frames to grayscale,\nwhich helps in reducing the amount of information that needs to be processed. Grayscale images only contain\nluminance information, which makes them easier to work with than full-color images. Once the video frames\nare converted to grayscale, image processing techniques can be used to detect and track people in the scene.\nOne approach is to use a technique called background subtraction, which involves subtracting the current\nframe from a background model to detect moving objects in the scene. The basic idea behind image processing\nframe by frame is to analyze each frame of the video separately, and extract features that can be used to identify\nand track people in the scene. These features can include properties such as color, texture, and shape, which\ncan be used to distinguish people from the background and other objects in the scene. One common technique\nused in frame-by-frame image processing for crowd detection is background subtraction. This involves creating\na model of the scene's background, and then subtracting the current frame from this model to detect moving\nobjects in the scene. By comparing each frame to the background model, it is possible to detect and track people\nas they move through the scene. Another technique used in frame-by-frame image processing for crowd\ndetection is blob detection. This involves identifying regions of pixels in the image that have similar properties,\nsuch as color or intensity, and grouping them together into blobs. By analyzing the properties of these blobs, it\nis possible to identify and track individual people in the scene.\nwww.irjmets.com @International Research Journal of Modernization in Engineering, Technology and Science\n[2810]",
        "tables": [],
        "forms": {},
        "confidence_score": 1.0
      },
      {
        "page_number": 4,
        "text_content": "e-ISSN: 2582-5208\nInternational Research Journal of Modernization in Engineering Technology and Science\n( Peer-Reviewed, Open Access, Fully Refereed International Journal )\nVolume:05/Issue:05/May-2023 Impact Factor- 7.868 www.irjmets.com\nFeature Extraction: Feature extraction is a fundamental step in computer vision and machine learning, and it\ninvolves identifying and extracting important information or features from raw images that can be used for\nfurther analysis, such as classification, detection, or segmentation. These features can be statistical or\nstructural, and can capture information such as texture, shape, or color.\nThe process of feature extraction can be broken down into several elements, including:\nPreprocessing: The first step in feature extraction is often preprocessing, which involves preparing the image\nfor analysis. This can include steps such as resizing, cropping, or color normalization, which help to improve the\nquality and consistency of the image.\nFeature Selection: Once the image is preprocessed, the next step is to select the most relevant features that\ncan help to distinguish between different objects or classes. This can be done manually, by identifying\nimportant visual cues or properties that are specific to the problem at hand, or automatically, using machine\nlearning techniques such as principal component analysis (PCA) or linear discriminant analysis (LDA).\nFeature Representation: After the relevant features have been selected, the next step is to represent them in a\nsuitable format that can be used for analysis. This can involve transforming the raw image data into a more\ncompact or meaningful representation, such as a histogram of color values, a texture descriptor, or a shape\nmodel.\nFeature Extraction: Once the features have been selected and represented, the final step is to extract them\nfrom the image. This can involve applying a set of operations or filters to the image data, such as convolution or\nwavelet transforms, to identify and extract the relevant features.\nFeature Normalization: In some cases, it may also be necessary to normalize the extracted features to\nimprove their consistency and robustness. This can involve techniques such as z-score normalization or min-\nmax normalization, which help to ensure that the features are scaled and centered appropriately.\nImage Segmentation: Image segmentation is a process of dividing an image into multiple segments or regions,\neach of which corresponds to a different object or part of the image. Image segmentation plays a critical role in\ncomputer vision applications, such as object detection, tracking, and recognition. The goal of image\nsegmentation is to partition an image into meaningful regions that can be analyzed and processed\nindependently. This is typically achieved by applying a set of image processing techniques to identify regions\nthat share similar visual characteristics, such as color, texture, or intensity.\nThere are several techniques for image segmentation, including:\nThresholding: This technique involves selecting a threshold value and partitioning the image into regions\nbased on the intensity values of each pixel. Pixels with intensity values above the threshold are assigned to one\nregion, while pixels with intensity values below the threshold are assigned to another region.\nEdge Detection: This technique involves detecting the edges or boundaries between different regions in an\nimage. This can be done using techniques such as the Canny edge detector or the Sobel edge detector.\nRegion Growing: This technique involves starting with a seed pixel and iteratively adding neighboring pixels\nto the region based on some similarity criterion. This process continues until all pixels in the region have been\nadded.\nClustering: This technique involves grouping pixels into clusters based on their visual similarity. This can be\ndone using algorithms such as k-means clustering or hierarchical clustering.\nClassification: In a crowd detection system, classification of images typically involves identifying and\ncategorizing different objects or groups within the crowd. This process can be performed using various\ntechniques, including traditional computer vision methods and deep learning-based approaches. In traditional\ncomputer vision methods, feature extraction algorithms are used to identify and extract relevant features from\nthe images, such as edges, corners, and texture patterns. These features are then used to train a classifier, such\nas a support vector machine (SVM) or k-nearest neighbor (KNN), to recognize different objects or groups\nwithin the crowd, such as individuals or clusters. In deep learning-based approaches, Convolutional Neural\nNetworks (CNNs) are commonly used for image classification tasks. CNNs are a type of neural network that are\nwww.irjmets.com @International Research Journal of Modernization in Engineering, Technology and Science\n[2811]",
        "tables": [],
        "forms": {},
        "confidence_score": 1.0
      },
      {
        "page_number": 5,
        "text_content": "e-ISSN: 2582-5208\nInternational Research Journal of Modernization in Engineering Technology and Science\n( Peer-Reviewed, Open Access, Fully Refereed International Journal )\nVolume:05/Issue:05/May-2023 Impact Factor- 7.868 www.irjmets.com\nspecifically designed for image processing tasks, as they can automatically learn and extract relevant features\nfrom images by using multiple layers of convolutional filters.\nThe process of image classification in a crowd detection system using CNNs typically involves the following\nsteps:\nData Preparation: Collecting and preparing a large dataset of labeled images for training the model.\nTraining the Model: Using the prepared dataset to train the CNN model to recognize different objects or\ngroups within the crowd.\nValidation: Testing the trained model on a separate dataset of images to evaluate its accuracy and\nperformance.\nDeployment: Using the trained model to classify new images in real-time.\nDuring the training process, the CNN model learns to recognize patterns and features in the crowd images by\nadjusting the weights of the neural network connections based on the errors between the predicted and actual\nclass labels. Once the model is trained and validated, it can be used to classify new crowd images by feeding\nthem into the model and predicting the corresponding class labels based on the learned features and patterns.\nVI. CONCLUSION\nIn conclusion, the proposed CNN-based method provides a promising approach for crowd detection and\ncounting in still images from various scenes. The use of features derived from a CNN model trained for other\ncomputer vision tasks enables accurate representation of crowd density. The neighboring local counts are also\nstrongly correlated, and feature extraction techniques contribute to good detection accuracy. The system uses\nthe RESNET deep convolutional network, which provides up to 152 hidden layers, and can be extended with\nmultiple convolutional layers with an ensemble deep learning model for even higher accuracy. Experimental\nfindings demonstrate that the proposed method outperforms other recent related methods and can be\nextended to work on image and video datasets. Overall, this system offers better accuracy for crowd detection\nfrom heterogeneous images, making it a valuable tool for various applications.\nVII. REFERENCES\n[1] Crowd Detection And Tracking In Surveillance Video Sequences Sohail Salim; Othman O Khalifa; Farah\nAbdul Rahman; Adidah Lajis 2019 IEEE International Conference on Smart Instrumentation,\nMeasurement and Application (ICSIMA)\n[2] Multi-UAV Based Crowd Monitoring System Rodrigo Saar de Moraes; Edison Pignaton de Freitas IEEE\nTransactions on Aerospace and Electronic Systems\n[3] In-Depth Survey to Detect, Monitor and Manage Crowd Ali M. Al-Shaery; Shroug S. Alshehri; Norah S.\nFarooqi; Mohamed O. Khozium IEEE Access\n[4] Analysis and Design of Public Places Crowd Stampede Early-Warning Simulating System Shangnan Liu;\nZhenjiang Zhu; Qiang Cheng; Hao Zhang 2016 International Conference on Industrial Informatics -\nComputing Technology, Intelligent Technology, Industrial Information Integration (ICIICII)\n[5] Crowd Monitoring Using Mobile Phones Yaoxuan Yuan 2014 Sixth International Conference on\nIntelligent Human-Machine Systems and Cybernetics\n[6] A Real-Time Crowd Detection and Monitoring System using Machine Learning Pooja Shrivastav; Vakula\nRani J 2023 International Conference on Intelligent Data Communication Technologies and Internet of\nThings (IDCIoT)\n[7] Crowd detection and management using cascade classifier on ARMv8 and OpenCV Python S. Syed\nAmeer Abbas; P. Oliver Jayaprakash; M. Anitha; X. Vinitha Jaini 2017 International Conference on\nInnovations iasdfghjkl;n Information, Embedded and Communication Systems (ICIIECS)\n[8] Multi-Person Tracking in Smart Surveillance System for Crowd Counting and Normal/Abnormal Events\nDetection Ahsan Shehzed; Ahmad Jalal; Kibum Kim 2019 International Conference on Applied and\nEngineering Mathematics (ICAEM).\nwww.irjmets.com @International Research Journal of Modernization in Engineering, Technology and Science\n[2812]",
        "tables": [],
        "forms": {},
        "confidence_score": 1.0
      }
    ]
  },
  {
    "file_path": "pdfs\\Critical Defects Analysis Reports 001.pdf",
    "total_pages": 2,
    "combined_text": "Critical Defects Analysis Reports\nBasic Information\nName of the Unique Identification\nDepartment System Sub-System OEM\nShip/Establishment Code\nINS Bagh Electrical Radar System Magnetron 11111 ABC\nDate Defect Date Defect\nIf defect still outstanding, reasons thereof\nOccurred Resolved\n01/01/2024 05/01/24 Not Applicable\nDefect Analysis\nBrief Description of the Defect Defect Analysis\nThe defect was localised to Defective Magnetron and it was rectified by\nSystem not transmitting\nreplacement of the defective magnetron with new magnetron\nDefect Resolution\nImportant correspondence related to\nSpare parts used to resolve the\nSteps taken to resolve the defect defect analysis and resolution\ndefect\n(OPDEF, STOREDEM, etc,)\nThe defect was reported to Local\nheadquarters through proper\nchannel. The repair agency\nMagnetron OPDEF 01/24, STOREDEM 01/24\nundertook the Defect identification\nand after careful scrutiny, isolated\nthe problem to defective\nMagnetron.\nThe demand of Magnetron was\nplaced on Material organisation at\nMumbai. This item thereafter was\ndelivered by Material Organisation\nto INS Bagh and repair of System 1\nwas undertaken successfully\nKey Personnel involved in Defect Analysis and Resolution\nSl. No. Name Rank/Designation Unit\n01. Tika Ram Senior Foreman Repair Unit 1\n02. Ram Vilas HSK 1 Repair Unit 1\n03. S Desai HSK 2 Repair Unit 1",
    "metadata": {
      "Author": "Skand Pujari",
      "Creator": "Microsoft\u00ae Word 2021",
      "CreationDate": "D:20241105210133+05'30'",
      "ModDate": "D:20241105210133+05'30'",
      "Producer": "Microsoft\u00ae Word 2021"
    },
    "error_message": null,
    "pages": [
      {
        "page_number": 0,
        "text_content": "Critical Defects Analysis Reports\nBasic Information\nName of the Unique Identification\nDepartment System Sub-System OEM\nShip/Establishment Code\nINS Bagh Electrical Radar System Magnetron 11111 ABC\nDate Defect Date Defect\nIf defect still outstanding, reasons thereof\nOccurred Resolved\n01/01/2024 05/01/24 Not Applicable\nDefect Analysis\nBrief Description of the Defect Defect Analysis\nThe defect was localised to Defective Magnetron and it was rectified by\nSystem not transmitting\nreplacement of the defective magnetron with new magnetron\nDefect Resolution\nImportant correspondence related to\nSpare parts used to resolve the\nSteps taken to resolve the defect defect analysis and resolution\ndefect\n(OPDEF, STOREDEM, etc,)\nThe defect was reported to Local\nheadquarters through proper\nchannel. The repair agency\nMagnetron OPDEF 01/24, STOREDEM 01/24\nundertook the Defect identification\nand after careful scrutiny, isolated\nthe problem to defective",
        "tables": [
          [
            [
              "Name of the\nShip/Establishment",
              "Department",
              "System",
              "Sub-System",
              "Unique Identification\nCode",
              "OEM"
            ],
            [
              "INS Bagh",
              "Electrical",
              "Radar System",
              "Magnetron",
              "11111",
              "ABC"
            ]
          ],
          [
            [
              "Date Defect\nOccurred",
              "Date Defect\nResolved",
              "If defect still outstanding, reasons thereof"
            ],
            [
              "01/01/2024",
              "05/01/24",
              "Not Applicable"
            ]
          ],
          [
            [
              "Brief Description of the Defect",
              "Defect Analysis"
            ],
            [
              "System not transmitting",
              "The defect was localised to Defective Magnetron and it was rectified by\nreplacement of the defective magnetron with new magnetron"
            ]
          ],
          [
            [
              "Steps taken to resolve the defect",
              "Spare parts used to resolve the\ndefect",
              "Important correspondence related to\ndefect analysis and resolution\n(OPDEF, STOREDEM, etc,)"
            ],
            [
              "The defect was reported to Local\nheadquarters through proper\nchannel. The repair agency\nundertook the Defect identification\nand after careful scrutiny, isolated\nthe problem to defective",
              "Magnetron",
              "OPDEF 01/24, STOREDEM 01/24"
            ]
          ]
        ],
        "forms": {},
        "confidence_score": 1.0
      },
      {
        "page_number": 1,
        "text_content": "Magnetron.\nThe demand of Magnetron was\nplaced on Material organisation at\nMumbai. This item thereafter was\ndelivered by Material Organisation\nto INS Bagh and repair of System 1\nwas undertaken successfully\nKey Personnel involved in Defect Analysis and Resolution\nSl. No. Name Rank/Designation Unit\n01. Tika Ram Senior Foreman Repair Unit 1\n02. Ram Vilas HSK 1 Repair Unit 1\n03. S Desai HSK 2 Repair Unit 1",
        "tables": [
          [
            [
              "Magnetron.\nThe demand of Magnetron was\nplaced on Material organisation at\nMumbai. This item thereafter was\ndelivered by Material Organisation\nto INS Bagh and repair of System 1\nwas undertaken successfully",
              "",
              ""
            ]
          ],
          [
            [
              "Sl. No.",
              "Name",
              "Rank/Designation",
              "Unit"
            ],
            [
              "01.",
              "Tika Ram",
              "Senior Foreman",
              "Repair Unit 1"
            ],
            [
              "02.",
              "Ram Vilas",
              "HSK 1",
              "Repair Unit 1"
            ],
            [
              "03.",
              "S Desai",
              "HSK 2",
              "Repair Unit 1"
            ]
          ]
        ],
        "forms": {},
        "confidence_score": 1.0
      }
    ]
  },
  {
    "file_path": "pdfs\\Critical Defects Analysis Reports 002.pdf",
    "total_pages": 2,
    "combined_text": "Critical Defects Analysis Reports\nBasic Information\nName of the Unique Identification\nDepartment System Sub-System OEM\nShip/Establishment Code\nINS Bagh Electrical Radar System Power Supply Unit 22222 ABC\nDate Defect Date Defect\nIf defect still outstanding, reasons thereof\nOccurred Resolved\n01/02/2024 05/02/24 Applicable\nDefect Analysis\nBrief Description of the Defect Defect Analysis\nThe magnetron showed signs of overheating, which was resolved by\nWeak or intermittent signal output\nreplacing it with a new unit.\nDefect Resolution\nImportant correspondence related to\nSpare parts used to resolve the\nSteps taken to resolve the defect defect analysis and resolution\ndefect\n(OPDEF, STOREDEM, etc,)\nThe radar system was powered down\nand disconnected from its power\nsource. All high-voltage capacitors\nPower Supply Unit REPAIRREQ 02/24, MATREQ 03/24\nwere discharged to ensure safety.\nThe faulty Power Supply Unit was\nidentified according to the system's\nmaintenance manual and\nsubsequently replaced. Once the new\nunit was installed, the input/output\nvoltage, current, and overall\nperformance were tested to confirm\ncompliance with the operational\nstandards outlined in the manual.\"\nKey Personnel involved in Defect Analysis and Resolution\nSl. No. Name Rank/Designation Unit\n01. Sanatan Gridharan Foreman Repair Unit 4\n02. Vishwanath Shrirang HSK 3 Repair Unit 4\n03. S Desai HSK 2 Repair Unit 4",
    "metadata": {
      "Author": "Skand Pujari",
      "Creator": "Microsoft\u00ae Word 2021",
      "CreationDate": "D:20241105210141+05'30'",
      "ModDate": "D:20241105210141+05'30'",
      "Producer": "Microsoft\u00ae Word 2021"
    },
    "error_message": null,
    "pages": [
      {
        "page_number": 0,
        "text_content": "Critical Defects Analysis Reports\nBasic Information\nName of the Unique Identification\nDepartment System Sub-System OEM\nShip/Establishment Code\nINS Bagh Electrical Radar System Power Supply Unit 22222 ABC\nDate Defect Date Defect\nIf defect still outstanding, reasons thereof\nOccurred Resolved\n01/02/2024 05/02/24 Applicable\nDefect Analysis\nBrief Description of the Defect Defect Analysis\nThe magnetron showed signs of overheating, which was resolved by\nWeak or intermittent signal output\nreplacing it with a new unit.\nDefect Resolution\nImportant correspondence related to\nSpare parts used to resolve the\nSteps taken to resolve the defect defect analysis and resolution\ndefect\n(OPDEF, STOREDEM, etc,)\nThe radar system was powered down\nand disconnected from its power\nsource. All high-voltage capacitors\nPower Supply Unit REPAIRREQ 02/24, MATREQ 03/24\nwere discharged to ensure safety.\nThe faulty Power Supply Unit was\nidentified according to the system's",
        "tables": [
          [
            [
              "Name of the\nShip/Establishment",
              "Department",
              "System",
              "Sub-System",
              "Unique Identification\nCode",
              "OEM"
            ],
            [
              "INS Bagh",
              "Electrical",
              "Radar System",
              "Power Supply Unit",
              "22222",
              "ABC"
            ]
          ],
          [
            [
              "Date Defect\nOccurred",
              "Date Defect\nResolved",
              "If defect still outstanding, reasons thereof"
            ],
            [
              "01/02/2024",
              "05/02/24",
              "Applicable"
            ]
          ],
          [
            [
              "Brief Description of the Defect",
              "Defect Analysis"
            ],
            [
              "Weak or intermittent signal output",
              "The magnetron showed signs of overheating, which was resolved by\nreplacing it with a new unit."
            ]
          ],
          [
            [
              "Steps taken to resolve the defect",
              "Spare parts used to resolve the\ndefect",
              "Important correspondence related to\ndefect analysis and resolution\n(OPDEF, STOREDEM, etc,)"
            ],
            [
              "The radar system was powered down\nand disconnected from its power\nsource. All high-voltage capacitors\nwere discharged to ensure safety.\nThe faulty Power Supply Unit was\nidentified according to the system's",
              "Power Supply Unit",
              "REPAIRREQ 02/24, MATREQ 03/24"
            ]
          ]
        ],
        "forms": {},
        "confidence_score": 1.0
      },
      {
        "page_number": 1,
        "text_content": "maintenance manual and\nsubsequently replaced. Once the new\nunit was installed, the input/output\nvoltage, current, and overall\nperformance were tested to confirm\ncompliance with the operational\nstandards outlined in the manual.\"\nKey Personnel involved in Defect Analysis and Resolution\nSl. No. Name Rank/Designation Unit\n01. Sanatan Gridharan Foreman Repair Unit 4\n02. Vishwanath Shrirang HSK 3 Repair Unit 4\n03. S Desai HSK 2 Repair Unit 4",
        "tables": [
          [
            [
              "maintenance manual and\nsubsequently replaced. Once the new\nunit was installed, the input/output\nvoltage, current, and overall\nperformance were tested to confirm\ncompliance with the operational\nstandards outlined in the manual.\"",
              "",
              ""
            ]
          ],
          [
            [
              "Sl. No.",
              "Name",
              "Rank/Designation",
              "Unit"
            ],
            [
              "01.",
              "Sanatan Gridharan",
              "Foreman",
              "Repair Unit 4"
            ],
            [
              "02.",
              "Vishwanath Shrirang",
              "HSK 3",
              "Repair Unit 4"
            ],
            [
              "03.",
              "S Desai",
              "HSK 2",
              "Repair Unit 4"
            ]
          ]
        ],
        "forms": {},
        "confidence_score": 1.0
      }
    ]
  },
  {
    "file_path": "pdfs\\Critical Defects Analysis Reports 003.pdf",
    "total_pages": 2,
    "combined_text": "Critical Defects Analysis Reports\nBasic Information\nName of the Unique Identification\nDepartment System Sub-System OEM\nShip/Establishment Code\nINS Bagh Electrical Radar System Cooling Fan 33333 ABC\nDate Defect Date Defect\nIf defect still outstanding, reasons thereof\nOccurred Resolved\n01/03/2024 05/3/2024 Not Applicable\nDefect Analysis\nBrief Description of the Defect Defect Analysis\nThe magnetron failed to generate the required frequency, necessitating a\nUnusual noise during operation\nreplacement.\nDefect Resolution\nImportant correspondence related to\nSpare parts used to resolve the\nSteps taken to resolve the defect defect analysis and resolution\ndefect\n(OPDEF, STOREDEM, etc,)\nAfter powering down the radar\nsystem and isolating it from its power\nsupply, the cooling fan was\nCooling Fan MAINTREQ 04/24, SUPPLYORD 05/24\ninspected. Following the system's\ndiagnostics, the defective Cooling\nFan was replaced. Post-installation,\nairflow, fan speed, and temperature\nregulation were tested to ensure that\nthe new cooling system met the\nperformance parameters specified in\nthe maintenance documentation.\nKey Personnel involved in Defect Analysis and Resolution\nSl. No. Name Rank/Designation Unit\n01. Arnav Soumen Foreman Repair Unit 6\n02. Ram Vilas HSK 1 Repair Unit 1\n03. S Desai HSK 2 Repair Unit 1",
    "metadata": {
      "Author": "Skand Pujari",
      "Creator": "Microsoft\u00ae Word 2021",
      "CreationDate": "D:20241105210148+05'30'",
      "ModDate": "D:20241105210148+05'30'",
      "Producer": "Microsoft\u00ae Word 2021"
    },
    "error_message": null,
    "pages": [
      {
        "page_number": 0,
        "text_content": "Critical Defects Analysis Reports\nBasic Information\nName of the Unique Identification\nDepartment System Sub-System OEM\nShip/Establishment Code\nINS Bagh Electrical Radar System Cooling Fan 33333 ABC\nDate Defect Date Defect\nIf defect still outstanding, reasons thereof\nOccurred Resolved\n01/03/2024 05/3/2024 Not Applicable\nDefect Analysis\nBrief Description of the Defect Defect Analysis\nThe magnetron failed to generate the required frequency, necessitating a\nUnusual noise during operation\nreplacement.\nDefect Resolution\nImportant correspondence related to\nSpare parts used to resolve the\nSteps taken to resolve the defect defect analysis and resolution\ndefect\n(OPDEF, STOREDEM, etc,)\nAfter powering down the radar\nsystem and isolating it from its power\nsupply, the cooling fan was\nCooling Fan MAINTREQ 04/24, SUPPLYORD 05/24\ninspected. Following the system's\ndiagnostics, the defective Cooling\nFan was replaced. Post-installation,",
        "tables": [
          [
            [
              "Name of the\nShip/Establishment",
              "Department",
              "System",
              "Sub-System",
              "Unique Identification\nCode",
              "OEM"
            ],
            [
              "INS Bagh",
              "Electrical",
              "Radar System",
              "Cooling Fan",
              "33333",
              "ABC"
            ]
          ],
          [
            [
              "Date Defect\nOccurred",
              "Date Defect\nResolved",
              "If defect still outstanding, reasons thereof"
            ],
            [
              "01/03/2024",
              "05/3/2024",
              "Not Applicable"
            ]
          ],
          [
            [
              "Brief Description of the Defect",
              "Defect Analysis"
            ],
            [
              "Unusual noise during operation",
              "The magnetron failed to generate the required frequency, necessitating a\nreplacement."
            ]
          ],
          [
            [
              "Steps taken to resolve the defect",
              "Spare parts used to resolve the\ndefect",
              "Important correspondence related to\ndefect analysis and resolution\n(OPDEF, STOREDEM, etc,)"
            ],
            [
              "After powering down the radar\nsystem and isolating it from its power\nsupply, the cooling fan was\ninspected. Following the system's\ndiagnostics, the defective Cooling\nFan was replaced. Post-installation,",
              "Cooling Fan",
              "MAINTREQ 04/24, SUPPLYORD 05/24"
            ]
          ]
        ],
        "forms": {},
        "confidence_score": 1.0
      },
      {
        "page_number": 1,
        "text_content": "airflow, fan speed, and temperature\nregulation were tested to ensure that\nthe new cooling system met the\nperformance parameters specified in\nthe maintenance documentation.\nKey Personnel involved in Defect Analysis and Resolution\nSl. No. Name Rank/Designation Unit\n01. Arnav Soumen Foreman Repair Unit 6\n02. Ram Vilas HSK 1 Repair Unit 1\n03. S Desai HSK 2 Repair Unit 1",
        "tables": [
          [
            [
              "airflow, fan speed, and temperature\nregulation were tested to ensure that\nthe new cooling system met the\nperformance parameters specified in\nthe maintenance documentation.",
              "",
              ""
            ]
          ],
          [
            [
              "Sl. No.",
              "Name",
              "Rank/Designation",
              "Unit"
            ],
            [
              "01.",
              "Arnav Soumen",
              "Foreman",
              "Repair Unit 6"
            ],
            [
              "02.",
              "Ram Vilas",
              "HSK 1",
              "Repair Unit 1"
            ],
            [
              "03.",
              "S Desai",
              "HSK 2",
              "Repair Unit 1"
            ]
          ]
        ],
        "forms": {},
        "confidence_score": 1.0
      }
    ]
  },
  {
    "file_path": "pdfs\\Critical Defects Analysis Reports 004.pdf",
    "total_pages": 2,
    "combined_text": "Critical Defects Analysis Reports\nBasic Information\nName of the Unique Identification\nDepartment System Sub-System OEM\nShip/Establishment Code\nWaveguide\nINS Bagh Electrical Radar System 44444 ABC\nAssembly\nDate Defect Date Defect\nIf defect still outstanding, reasons thereof\nOccurred Resolved\n01/04/2024 05/4/2024 Applicable\nDefect Analysis\nBrief Description of the Defect Defect Analysis\nThe magnetron was experiencing power fluctuations, which were fixed by\nInconsistent power levels detected\ninstalling a new magnetron.\nDefect Resolution\nImportant correspondence related to\nSpare parts used to resolve the\nSteps taken to resolve the defect defect analysis and resolution\ndefect\n(OPDEF, STOREDEM, etc,)\nThe radar system was powered off,\nand all high-voltage components\nwere discharged. The damaged Waveguide Assembly DEFECTREP 06/24, PARTDEL 07/24\nWaveguide Assembly was removed\nas per the system's manual. After the\nreplacement part was fitted, the\nsystem's transmission path, signal\nintegrity, and standing wave ratio\n(VSWR) were evaluated. The results\nwere within the permissible limits,\nconfirming the waveguide's correct\ninstallation and functionality.\nKey Personnel involved in Defect Analysis and Resolution\nSl. No. Name Rank/Designation Unit\n01. Asija Sahni Senior Foreman Repair Unit 7\n02. Vishwanath Shrirang HSK 3 Repair Unit 7\n03. Ram Vilas HSK 1 Repair Unit 7",
    "metadata": {
      "Author": "Skand Pujari",
      "Creator": "Microsoft\u00ae Word 2021",
      "CreationDate": "D:20241105210155+05'30'",
      "ModDate": "D:20241105210155+05'30'",
      "Producer": "Microsoft\u00ae Word 2021"
    },
    "error_message": null,
    "pages": [
      {
        "page_number": 0,
        "text_content": "Critical Defects Analysis Reports\nBasic Information\nName of the Unique Identification\nDepartment System Sub-System OEM\nShip/Establishment Code\nWaveguide\nINS Bagh Electrical Radar System 44444 ABC\nAssembly\nDate Defect Date Defect\nIf defect still outstanding, reasons thereof\nOccurred Resolved\n01/04/2024 05/4/2024 Applicable\nDefect Analysis\nBrief Description of the Defect Defect Analysis\nThe magnetron was experiencing power fluctuations, which were fixed by\nInconsistent power levels detected\ninstalling a new magnetron.\nDefect Resolution\nImportant correspondence related to\nSpare parts used to resolve the\nSteps taken to resolve the defect defect analysis and resolution\ndefect\n(OPDEF, STOREDEM, etc,)\nThe radar system was powered off,\nand all high-voltage components\nwere discharged. The damaged Waveguide Assembly DEFECTREP 06/24, PARTDEL 07/24\nWaveguide Assembly was removed\nas per the system's manual. After the",
        "tables": [
          [
            [
              "Name of the\nShip/Establishment",
              "Department",
              "System",
              "Sub-System",
              "Unique Identification\nCode",
              "OEM"
            ],
            [
              "INS Bagh",
              "Electrical",
              "Radar System",
              "Waveguide\nAssembly",
              "44444",
              "ABC"
            ]
          ],
          [
            [
              "Date Defect\nOccurred",
              "Date Defect\nResolved",
              "If defect still outstanding, reasons thereof"
            ],
            [
              "01/04/2024",
              "05/4/2024",
              "Applicable"
            ]
          ],
          [
            [
              "Brief Description of the Defect",
              "Defect Analysis"
            ],
            [
              "Inconsistent power levels detected",
              "The magnetron was experiencing power fluctuations, which were fixed by\ninstalling a new magnetron."
            ]
          ],
          [
            [
              "Steps taken to resolve the defect",
              "Spare parts used to resolve the\ndefect",
              "Important correspondence related to\ndefect analysis and resolution\n(OPDEF, STOREDEM, etc,)"
            ],
            [
              "The radar system was powered off,\nand all high-voltage components\nwere discharged. The damaged\nWaveguide Assembly was removed\nas per the system's manual. After the",
              "Waveguide Assembly",
              "DEFECTREP 06/24, PARTDEL 07/24"
            ]
          ]
        ],
        "forms": {},
        "confidence_score": 1.0
      },
      {
        "page_number": 1,
        "text_content": "replacement part was fitted, the\nsystem's transmission path, signal\nintegrity, and standing wave ratio\n(VSWR) were evaluated. The results\nwere within the permissible limits,\nconfirming the waveguide's correct\ninstallation and functionality.\nKey Personnel involved in Defect Analysis and Resolution\nSl. No. Name Rank/Designation Unit\n01. Asija Sahni Senior Foreman Repair Unit 7\n02. Vishwanath Shrirang HSK 3 Repair Unit 7\n03. Ram Vilas HSK 1 Repair Unit 7",
        "tables": [
          [
            [
              "replacement part was fitted, the\nsystem's transmission path, signal\nintegrity, and standing wave ratio\n(VSWR) were evaluated. The results\nwere within the permissible limits,\nconfirming the waveguide's correct\ninstallation and functionality.",
              "",
              ""
            ]
          ],
          [
            [
              "Sl. No.",
              "Name",
              "Rank/Designation",
              "Unit"
            ],
            [
              "01.",
              "Asija Sahni",
              "Senior Foreman",
              "Repair Unit 7"
            ],
            [
              "02.",
              "Vishwanath Shrirang",
              "HSK 3",
              "Repair Unit 7"
            ],
            [
              "03.",
              "Ram Vilas",
              "HSK 1",
              "Repair Unit 7"
            ]
          ]
        ],
        "forms": {},
        "confidence_score": 1.0
      }
    ]
  },
  {
    "file_path": "pdfs\\Critical Defects Analysis Reports 005.pdf",
    "total_pages": 2,
    "combined_text": "Critical Defects Analysis Reports\nBasic Information\nName of the Unique Identification\nDepartment System Sub-System OEM\nShip/Establishment Code\nINS Bagh Electrical Radar System Control Board 55555 ABC\nDate Defect Date Defect\nIf defect still outstanding, reasons thereof\nOccurred Resolved\n01/05/2024 05/5/2024 Not Applicable\nDefect Analysis\nBrief Description of the Defect Defect Analysis\nThe magnetron had a short circuit issue that was resolved through the\nSystem shutdown unexpectedly\nreplacement of the faulty component.\nDefect Resolution\nImportant correspondence related to\nSpare parts used to resolve the\nSteps taken to resolve the defect defect analysis and resolution\ndefect\n(OPDEF, STOREDEM, etc,)\nFollowing the safe power-down of the\nradar system and the discharge of all\ncapacitors, the defective Control\nControl Board SYSTEMCHK 08/24, ORDERCONF 09/24\nBoard was carefully removed. A new\ncontrol board was installed based on\nthe system's technical specifications.\nFunctional checks, including signal\nprocessing, system control\ncommands, and diagnostic feedback,\nwere conducted. The system's\nperformance was confirmed to be\nstable and within the required\noperational limits.\nKey Personnel involved in Defect Analysis and Resolution\nSl. No. Name Rank/Designation Unit\n01. Tika Ram Senior Foreman Repair Unit 1\n02. S Desai HSK 2 Repair Unit 1\n03. Vishwanath Shrirang HSK 3 Repair Unit 1",
    "metadata": {
      "Author": "Skand Pujari",
      "Creator": "Microsoft\u00ae Word 2021",
      "CreationDate": "D:20241105210201+05'30'",
      "ModDate": "D:20241105210201+05'30'",
      "Producer": "Microsoft\u00ae Word 2021"
    },
    "error_message": null,
    "pages": [
      {
        "page_number": 0,
        "text_content": "Critical Defects Analysis Reports\nBasic Information\nName of the Unique Identification\nDepartment System Sub-System OEM\nShip/Establishment Code\nINS Bagh Electrical Radar System Control Board 55555 ABC\nDate Defect Date Defect\nIf defect still outstanding, reasons thereof\nOccurred Resolved\n01/05/2024 05/5/2024 Not Applicable\nDefect Analysis\nBrief Description of the Defect Defect Analysis\nThe magnetron had a short circuit issue that was resolved through the\nSystem shutdown unexpectedly\nreplacement of the faulty component.\nDefect Resolution\nImportant correspondence related to\nSpare parts used to resolve the\nSteps taken to resolve the defect defect analysis and resolution\ndefect\n(OPDEF, STOREDEM, etc,)\nFollowing the safe power-down of the\nradar system and the discharge of all\ncapacitors, the defective Control\nControl Board SYSTEMCHK 08/24, ORDERCONF 09/24\nBoard was carefully removed. A new\ncontrol board was installed based on\nthe system's technical specifications.",
        "tables": [
          [
            [
              "Name of the\nShip/Establishment",
              "Department",
              "System",
              "Sub-System",
              "Unique Identification\nCode",
              "OEM"
            ],
            [
              "INS Bagh",
              "Electrical",
              "Radar System",
              "Control Board",
              "55555",
              "ABC"
            ]
          ],
          [
            [
              "Date Defect\nOccurred",
              "Date Defect\nResolved",
              "If defect still outstanding, reasons thereof"
            ],
            [
              "01/05/2024",
              "05/5/2024",
              "Not Applicable"
            ]
          ],
          [
            [
              "Brief Description of the Defect",
              "Defect Analysis"
            ],
            [
              "System shutdown unexpectedly",
              "The magnetron had a short circuit issue that was resolved through the\nreplacement of the faulty component."
            ]
          ],
          [
            [
              "Steps taken to resolve the defect",
              "Spare parts used to resolve the\ndefect",
              "Important correspondence related to\ndefect analysis and resolution\n(OPDEF, STOREDEM, etc,)"
            ],
            [
              "Following the safe power-down of the\nradar system and the discharge of all\ncapacitors, the defective Control\nBoard was carefully removed. A new\ncontrol board was installed based on\nthe system's technical specifications.",
              "Control Board",
              "SYSTEMCHK 08/24, ORDERCONF 09/24"
            ]
          ]
        ],
        "forms": {},
        "confidence_score": 1.0
      },
      {
        "page_number": 1,
        "text_content": "Functional checks, including signal\nprocessing, system control\ncommands, and diagnostic feedback,\nwere conducted. The system's\nperformance was confirmed to be\nstable and within the required\noperational limits.\nKey Personnel involved in Defect Analysis and Resolution\nSl. No. Name Rank/Designation Unit\n01. Tika Ram Senior Foreman Repair Unit 1\n02. S Desai HSK 2 Repair Unit 1\n03. Vishwanath Shrirang HSK 3 Repair Unit 1",
        "tables": [
          [
            [
              "Functional checks, including signal\nprocessing, system control\ncommands, and diagnostic feedback,\nwere conducted. The system's\nperformance was confirmed to be\nstable and within the required\noperational limits.",
              "",
              ""
            ]
          ],
          [
            [
              "Sl. No.",
              "Name",
              "Rank/Designation",
              "Unit"
            ],
            [
              "01.",
              "Tika Ram",
              "Senior Foreman",
              "Repair Unit 1"
            ],
            [
              "02.",
              "S Desai",
              "HSK 2",
              "Repair Unit 1"
            ],
            [
              "03.",
              "Vishwanath Shrirang",
              "HSK 3",
              "Repair Unit 1"
            ]
          ]
        ],
        "forms": {},
        "confidence_score": 1.0
      }
    ]
  },
  {
    "file_path": "pdfs\\Critical Defects Analysis Reports 006.pdf",
    "total_pages": 2,
    "combined_text": "Critical Defects Analysis Reports\nBasic Information\nName of the Unique Identification\nDepartment System Sub-System OEM\nShip/Establishment Code\nINS Cheetah Electrical Radar System Magnetron 66666 DEF\nDate Defect Date Defect\nIf defect still outstanding, reasons thereof\nOccurred Resolved\n01/06/2024 05/6/2024 Not Applicable\nDefect Analysis\nBrief Description of the Defect Defect Analysis\nMagnetron showed signs of overheating, resolved by replacing with a new\nWeak or intermittent signal output\nunit.\nDefect Resolution\nImportant correspondence related to\nSpare parts used to resolve the\nSteps taken to resolve the defect defect analysis and resolution\ndefect\n(OPDEF, STOREDEM, etc,)\nInput/output voltage, current, and\nperformance were tested to confirm\nMagnetron OPDEF 01/24, STOREDEM 01/24\nthe replacement met system\nrequirements.\nKey Personnel involved in Defect Analysis and Resolution\nSl. No. Name Rank/Designation Unit\n01. Tika Ram Senior Foreman Repair Unit 1\n02. S Desai HSK 2 Repair Unit 1\n03. Vishwanath Shrirang HSK 3 Repair Unit 1",
    "metadata": {
      "Author": "Skand Pujari",
      "Creator": "Microsoft\u00ae Word 2021",
      "CreationDate": "D:20241105210212+05'30'",
      "ModDate": "D:20241105210212+05'30'",
      "Producer": "Microsoft\u00ae Word 2021"
    },
    "error_message": null,
    "pages": [
      {
        "page_number": 0,
        "text_content": "Critical Defects Analysis Reports\nBasic Information\nName of the Unique Identification\nDepartment System Sub-System OEM\nShip/Establishment Code\nINS Cheetah Electrical Radar System Magnetron 66666 DEF\nDate Defect Date Defect\nIf defect still outstanding, reasons thereof\nOccurred Resolved\n01/06/2024 05/6/2024 Not Applicable\nDefect Analysis\nBrief Description of the Defect Defect Analysis\nMagnetron showed signs of overheating, resolved by replacing with a new\nWeak or intermittent signal output\nunit.\nDefect Resolution\nImportant correspondence related to\nSpare parts used to resolve the\nSteps taken to resolve the defect defect analysis and resolution\ndefect\n(OPDEF, STOREDEM, etc,)\nInput/output voltage, current, and\nperformance were tested to confirm\nMagnetron OPDEF 01/24, STOREDEM 01/24\nthe replacement met system\nrequirements.",
        "tables": [
          [
            [
              "Name of the\nShip/Establishment",
              "Department",
              "System",
              "Sub-System",
              "Unique Identification\nCode",
              "OEM"
            ],
            [
              "INS Cheetah",
              "Electrical",
              "Radar System",
              "Magnetron",
              "66666",
              "DEF"
            ]
          ],
          [
            [
              "Date Defect\nOccurred",
              "Date Defect\nResolved",
              "If defect still outstanding, reasons thereof"
            ],
            [
              "01/06/2024",
              "05/6/2024",
              "Not Applicable"
            ]
          ],
          [
            [
              "Brief Description of the Defect",
              "Defect Analysis"
            ],
            [
              "Weak or intermittent signal output",
              "Magnetron showed signs of overheating, resolved by replacing with a new\nunit."
            ]
          ],
          [
            [
              "Steps taken to resolve the defect",
              "Spare parts used to resolve the\ndefect",
              "Important correspondence related to\ndefect analysis and resolution\n(OPDEF, STOREDEM, etc,)"
            ],
            [
              "Input/output voltage, current, and\nperformance were tested to confirm\nthe replacement met system\nrequirements.",
              "Magnetron",
              "OPDEF 01/24, STOREDEM 01/24"
            ]
          ]
        ],
        "forms": {},
        "confidence_score": 1.0
      },
      {
        "page_number": 1,
        "text_content": "Key Personnel involved in Defect Analysis and Resolution\nSl. No. Name Rank/Designation Unit\n01. Tika Ram Senior Foreman Repair Unit 1\n02. S Desai HSK 2 Repair Unit 1\n03. Vishwanath Shrirang HSK 3 Repair Unit 1",
        "tables": [
          [
            [
              "Sl. No.",
              "Name",
              "Rank/Designation",
              "Unit"
            ],
            [
              "01.",
              "Tika Ram",
              "Senior Foreman",
              "Repair Unit 1"
            ],
            [
              "02.",
              "S Desai",
              "HSK 2",
              "Repair Unit 1"
            ],
            [
              "03.",
              "Vishwanath Shrirang",
              "HSK 3",
              "Repair Unit 1"
            ]
          ]
        ],
        "forms": {},
        "confidence_score": 1.0
      }
    ]
  },
  {
    "file_path": "pdfs\\Critical Defects Analysis Reports 007.pdf",
    "total_pages": 2,
    "combined_text": "Critical Defects Analysis Reports\nBasic Information\nName of the Unique Identification\nDepartment System Sub-System OEM\nShip/Establishment Code\nINS Cheetah Electrical Radar System Power Supply Unit 77777 DEF\nDate Defect Date Defect\nIf defect still outstanding, reasons thereof\nOccurred Resolved\n01/03/2024 05/03/24 Applicable\nDefect Analysis\nBrief Description of the Defect Defect Analysis\nWeak signal output Overheating issue in power supply unit; resolved by replacement.\nDefect Resolution\nImportant correspondence related to\nSpare parts used to resolve the\nSteps taken to resolve the defect defect analysis and resolution\ndefect\n(OPDEF, STOREDEM, etc,)\nInput/output voltage, current, and\noverall performance were tested to\nPower Supply Unit REPAIRREQ 02/24, MATREQ 03/24\nensure compliance with operational\nstandards.\nKey Personnel involved in Defect Analysis and Resolution\nSl. No. Name Rank/Designation Unit\n01. Sanatan Gridharan Foreman Repair Unit 4\n02. Vishwanath Shrirang HSK 3 Repair Unit 4\n03. S Desai HSK 2 Repair Unit 4",
    "metadata": {
      "Author": "Skand Pujari",
      "Creator": "Microsoft\u00ae Word 2021",
      "CreationDate": "D:20241105210219+05'30'",
      "ModDate": "D:20241105210219+05'30'",
      "Producer": "Microsoft\u00ae Word 2021"
    },
    "error_message": null,
    "pages": [
      {
        "page_number": 0,
        "text_content": "Critical Defects Analysis Reports\nBasic Information\nName of the Unique Identification\nDepartment System Sub-System OEM\nShip/Establishment Code\nINS Cheetah Electrical Radar System Power Supply Unit 77777 DEF\nDate Defect Date Defect\nIf defect still outstanding, reasons thereof\nOccurred Resolved\n01/03/2024 05/03/24 Applicable\nDefect Analysis\nBrief Description of the Defect Defect Analysis\nWeak signal output Overheating issue in power supply unit; resolved by replacement.\nDefect Resolution\nImportant correspondence related to\nSpare parts used to resolve the\nSteps taken to resolve the defect defect analysis and resolution\ndefect\n(OPDEF, STOREDEM, etc,)\nInput/output voltage, current, and\noverall performance were tested to\nPower Supply Unit REPAIRREQ 02/24, MATREQ 03/24\nensure compliance with operational\nstandards.\nKey Personnel involved in Defect Analysis and Resolution",
        "tables": [
          [
            [
              "Name of the\nShip/Establishment",
              "Department",
              "System",
              "Sub-System",
              "Unique Identification\nCode",
              "OEM"
            ],
            [
              "INS Cheetah",
              "Electrical",
              "Radar System",
              "Power Supply Unit",
              "77777",
              "DEF"
            ]
          ],
          [
            [
              "Date Defect\nOccurred",
              "Date Defect\nResolved",
              "If defect still outstanding, reasons thereof"
            ],
            [
              "01/03/2024",
              "05/03/24",
              "Applicable"
            ]
          ],
          [
            [
              "Brief Description of the Defect",
              "Defect Analysis"
            ],
            [
              "Weak signal output",
              "Overheating issue in power supply unit; resolved by replacement."
            ]
          ],
          [
            [
              "Steps taken to resolve the defect",
              "Spare parts used to resolve the\ndefect",
              "Important correspondence related to\ndefect analysis and resolution\n(OPDEF, STOREDEM, etc,)"
            ],
            [
              "Input/output voltage, current, and\noverall performance were tested to\nensure compliance with operational\nstandards.",
              "Power Supply Unit",
              "REPAIRREQ 02/24, MATREQ 03/24"
            ]
          ]
        ],
        "forms": {},
        "confidence_score": 1.0
      },
      {
        "page_number": 1,
        "text_content": "Sl. No. Name Rank/Designation Unit\n01. Sanatan Gridharan Foreman Repair Unit 4\n02. Vishwanath Shrirang HSK 3 Repair Unit 4\n03. S Desai HSK 2 Repair Unit 4",
        "tables": [
          [
            [
              "Sl. No.",
              "Name",
              "Rank/Designation",
              "Unit"
            ],
            [
              "01.",
              "Sanatan Gridharan",
              "Foreman",
              "Repair Unit 4"
            ],
            [
              "02.",
              "Vishwanath Shrirang",
              "HSK 3",
              "Repair Unit 4"
            ],
            [
              "03.",
              "S Desai",
              "HSK 2",
              "Repair Unit 4"
            ]
          ]
        ],
        "forms": {},
        "confidence_score": 1.0
      }
    ]
  }
]